{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summary_non_py import get_summary_non_py\n",
    "import pathlib\n",
    "from utils import llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"/workspace/code_qa_llm/test_repo/doc/layernorm/layernorm.md\",\n",
    "         \"/workspace/code_qa_llm/test_repo/scripts/run_gpt2_124M.sh\",\n",
    "         ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'name': 'LayerNorm',\n",
       "   'type': 'class',\n",
       "   'summary': 'This class implements the forward and backward passes of Layer Normalization manually using simpler PyTorch operations. It includes static methods to perform the forward pass (normalizing input activations) and the backward pass (computing gradients).',\n",
       "   'path': '/workspace/code_qa_llm/test_repo/doc/layernorm/layernorm.md',\n",
       "   'code': 'import torch\\neps = 1e-5\\n\\nclass LayerNorm:\\n\\n    @staticmethod\\n    def forward(x, w, b):\\n        # x is the input activations, of shape B,T,C\\n        # w are the weights, of shape C\\n        # b are the biases, of shape C\\n        B, T, C = x.size()\\n        # calculate the mean\\n        mean = x.sum(-1, keepdim=True) / C # B,T,1\\n        # calculate the variance\\n        xshift = x - mean # B,T,C\\n        var = (xshift**2).sum(-1, keepdim=True) / C # B,T,1\\n        # calculate the inverse standard deviation: **0.5 is sqrt, **-0.5 is 1/sqrt\\n        rstd = (var + eps) ** -0.5 # B,T,1\\n        # normalize the input activations\\n        norm = xshift * rstd # B,T,C\\n        # scale and shift the normalized activations at the end\\n        out = norm * w + b # B,T,C\\n\\n        # return the output and the cache, of variables needed later during the backward pass\\n        cache = (x, w, mean, rstd)\\n        return out, cache\\n\\n    @staticmethod\\n    def backward(dout, cache):\\n        x, w, mean, rstd = cache\\n        # recompute the norm (save memory at the cost of compute)\\n        norm = (x - mean) * rstd\\n        # gradients for weights, bias\\n        db = dout.sum((0, 1))\\n        dw = (dout * norm).sum((0, 1))\\n        # gradients for input\\n        dnorm = dout * w\\n        dx = dnorm - dnorm.mean(-1, keepdim=True) - norm * (dnorm * norm).mean(-1, keepdim=True)\\n        dx *= rstd\\n        return dx, dw, db',\n",
       "   'dependent_functions': ['torch']},\n",
       "  {'name': 'layernorm_forward',\n",
       "   'type': 'function',\n",
       "   'summary': 'This C function performs the forward pass of Layer Normalization by calculating the mean, variance, and normalized values of the input tensor. It scales and shifts the normalized values using weights and biases, then caches the mean and rstd for potential use in the backward pass.',\n",
       "   'path': '/workspace/code_qa_llm/test_repo/doc/layernorm/layernorm.md',\n",
       "   'code': '#include &lt;stdio.h&gt;\\n#include &lt;stdlib.h&gt;\\n#include &lt;math.h&gt;\\n\\nvoid layernorm_forward(float* out, float* mean, float* rstd,\\n                       float* inp, float* weight, float* bias,\\n                       int B, int T, int C) {\\n    float eps = 1e-5f;\\n    for (int b = 0; b &lt; B; b++) {\\n        for (int t = 0; t &lt; T; t++) {\\n            // seek to the input position inp[b,t,:]\\n            float* x = inp + b * T * C + t * C;\\n            // calculate the mean\\n            float m = 0.0f;\\n            for (int i = 0; i < C; i++) {\\n                m += x[i];\\n            }\\n            m = m/C;\\n            // calculate the variance (without any bias correction)\\n            float v = 0.0f;\\n            for (int i = 0; i < C; i++) {\\n                float xshift = x[i] - m;\\n                v += xshift * xshift;\\n            }\\n            v = v/C;\\n            // calculate the rstd\\n            float s = 1.0f / sqrtf(v + eps);\\n            // seek to the output position in out[b,t,:]\\n            float* out_bt = out + b * T * C + t * C;\\n            for (int i = 0; i < C; i++) {\\n                float n = (s * (x[i] - m)); // normalized output\\n                float o = n * weight[i] + bias[i]; // scale and shift it\\n                out_bt[i] = o; // write\\n            }\\n            // cache the mean and rstd for the backward pass later\\n            mean[b * T + t] = m;\\n            rstd[b * T + t] = s;\\n        }\\n    }\\n}',\n",
       "   'dependent_functions': ['']},\n",
       "  {'name': 'layernorm_backward',\n",
       "   'type': 'function',\n",
       "   'summary': 'This C function performs the backward pass of Layer Normalization. It calculates gradients for input, weights, and biases based on the cached variables (from the forward pass) and dout, the gradient of the output.',\n",
       "   'path': '/workspace/code_qa_llm/test_repo/doc/layernorm/layernorm.md',\n",
       "   'code': 'void layernorm_backward(float* dinp, float* dweight, float* dbias,\\n                        float* dout, float* inp, float* weight, float* mean, float* rstd,\\n                        int B, int T, int C) {\\n    for (int b = 0; b < B; b++) {\\n        for (int t = 0; t < T; t++) {\\n            float* dout_bt = dout + b * T * C + t * C;\\n            float* inp_bt = inp + b * T * C + t * C;\\n            float* dinp_bt = dinp + b * T * C + t * C;\\n            float mean_bt = mean[b * T + t];\\n            float rstd_bt = rstd[b * T + t];\\n\\n            // first: two reduce operations\\n            float dnorm_mean = 0.0f;\\n            float dnorm_norm_mean = 0.0f;\\n            for (int i = 0; i < C; i++) {\\n                float norm_bti = (inp_bt[i] - mean_bt) * rstd_bt;\\n                float dnorm_i = weight[i] * dout_bt[i];\\n                dnorm_mean += dnorm_i;\\n                dnorm_norm_mean += dnorm_i * norm_bti;\\n            }\\n            dnorm_mean = dnorm_mean / C;\\n            dnorm_norm_mean = dnorm_norm_mean / C;\\n\\n            // now iterate again and accumulate all the gradients\\n            for (int i = 0; i < C; i++) {\\n                float norm_bti = (inp_bt[i] - mean_bt) * rstd_bt;\\n                float dnorm_i = weight[i] * dout_bt[i];\\n                // gradient contribution to bias\\n                dbias[i] += dout_bt[i];\\n                // gradient contribution to weight\\n                dweight[i] += norm_bti * dout_bt[i];\\n                // gradient contribution to input\\n                float dval = 0.0f;\\n                dval += dnorm_i; // term 1\\n                dval -= dnorm_mean; // term 2\\n                dval -= norm_bti * dnorm_norm_mean; // term 3\\n                dval *= rstd_bt; // final scale\\n                dinp_bt[i] += dval;\\n            }\\n        }\\n    }\\n}',\n",
       "   'dependent_functions': ['']},\n",
       "  {'name': 'rmsnorm',\n",
       "   'type': 'function',\n",
       "   'summary': 'This function implements RMSNorm in C. It normalizes inputs based on the root mean square instead of standard deviation, does not keep biases, and is simplified compared to LayerNorm.',\n",
       "   'path': '/workspace/code_qa_llm/test_repo/doc/layernorm/layernorm.md',\n",
       "   'code': 'void rmsnorm(float* o, float* x, float* weight, int size) {\\n    // calculate sum of squares\\n    float ss = 0.0f;\\n    for (int j = 0; j < size; j++) {\\n        ss += x[j] * x[j];\\n    }\\n    ss /= size;\\n    ss += 1e-5f;\\n    ss = 1.0f / sqrtf(ss);\\n    // normalize and scale\\n    for (int j = 0; j < size; j++) {\\n        o[j] = weight[j] * (ss * x[j]);\\n    }\\n}',\n",
       "   'dependent_functions': ['']},\n",
       "  {'name': 'tutorial.py',\n",
       "   'type': 'file',\n",
       "   'summary': 'This tutorial demonstrates LayerNorm implementation using PyTorch and manual coding in both Python and C. It involves forward and backward passes, comparisons with PyTorch autograd, and the concept of checkpointing. The C code examples also cover the differences between LayerNorm and a simplified RMSNorm used in inference.',\n",
       "   'path': '/workspace/code_qa_llm/test_repo/doc/layernorm/layernorm.md',\n",
       "   'code': '\\n# layernorm\\n\\nQuick tutorial. Let\\'s look at how LayerNorm is handled, as one example layer in the model. We start with the [PyTorch docs for LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html). LayerNorm of course comes from this original paper by [Ba et al. 2016](https://arxiv.org/abs/1607.06450), and was incorporated into the Transformer in [Vaswani et al.](https://arxiv.org/abs/1706.03762) famous paper Attention is All You Need. [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) picked up the same architecture as the Transformer, but the position of the LayerNorm was famously moved into what is now called the pre-normalization version. That is, the residual path of the Transformer is kept clean, and the LayerNorms are now the first layer of each block of the Transformer. This positively improves training stability.\\n\\nThe first thing to note when looking at [PyTorch LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html) is that you will most likely not be able to find the actual implementation of the equation. That\\'s because it is buried 30 layers deep in the code, behind an inscrutable dynamical dispatcher, in some possibly auto-generated CUDA code (for those who are interested in details, see [layer_norm.cpp](https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/layer_norm.cpp) and  [layer_norm_kernel.cu](https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/cuda/layer_norm_kernel.cu)). This is done because PyTorch really really cares about efficiency, fair enough. For our purposes though, we have to start by first implementing LayerNorm manually using simpler PyTorch operations. This will be a lot less efficient than just forwarding a `LayerNorm` module, but it is algorithmically instructive. So here is the direct implementation of the math of LayerNorm using simpler PyTorch operations:\\n\\n```python\\nimport torch\\neps = 1e-5\\n\\nclass LayerNorm:\\n\\n    @staticmethod\\n    def forward(x, w, b):\\n        # x is the input activations, of shape B,T,C\\n        # w are the weights, of shape C\\n        # b are the biases, of shape C\\n        B, T, C = x.size()\\n        # calculate the mean\\n        mean = x.sum(-1, keepdim=True) / C # B,T,1\\n        # calculate the variance\\n        xshift = x - mean # B,T,C\\n        var = (xshift**2).sum(-1, keepdim=True) / C # B,T,1\\n        # calculate the inverse standard deviation: **0.5 is sqrt, **-0.5 is 1/sqrt\\n        rstd = (var + eps) ** -0.5 # B,T,1\\n        # normalize the input activations\\n        norm = xshift * rstd # B,T,C\\n        # scale and shift the normalized activations at the end\\n        out = norm * w + b # B,T,C\\n\\n        # return the output and the cache, of variables needed later during the backward pass\\n        cache = (x, w, mean, rstd)\\n        return out, cache\\n```\\n\\nThe activation tensors in the residual path of the Transformer during training are 3-dimensional arrays (tensors), of shape `B,T,C`. B is the batch size, T is time, and C is channels. For example, B=8, T=1024, C=768 is one setting you might see, for the smallest (124 million parameter) GPT-2 model.\\n\\nWe can forward this layer with some random numbers:\\n\\n```python\\nB = 2 # some toy numbers here\\nT = 3\\nC = 4\\nx = torch.randn(B, T, C, requires_grad=True)\\nw = torch.randn(C, requires_grad=True)\\nb = torch.randn(C, requires_grad=True)\\nout, cache = LayerNorm.forward(x, w, b)\\n```\\n\\nWhat we get out is the tensor `out`, also of shape `B,T,C`, where each C-dimensional \"fibre\" of activations (as we call them) is normalized and then scaled and at the end also shifted by the weights and biases of this layer. Notice that, importantly, we also return a variable `cache`, which is a tuple of the input activations `x`, the weights `w`, the mean `mean`, and the reciprocal standard deviation `rstd`. These are all variables we need during the backward pass.\\n\\nPyTorch can of course do the backward pass of this layer for us with its Autograd. Let\\'s do that first:\\n\\n```python\\ndout = torch.randn(B, T, C)\\nfakeloss = (out * dout).sum()\\nfakeloss.backward()\\n```\\n\\nYou see here that we created a `fakeloss`, which simply takes a (random) weighted combination of all the outputs of our layernorm. All this is doing is projecting all of the `B,T,C` numbers into a single scalar value (loss), so that we have a single output of our \"computational graph\". Typically this would be the loss of the model, but here we\\'re just doing a fake loss. We then call `backward()` on this scalar, and PyTorch will compute all the gradients for us on all the inputs to this graph - i.e. the input activations `x`, the weights `w`, and the biases `b`. If you don\\'t know too much about autograd, I\\'d encourage you to watch my [micrograd](https://www.youtube.com/watch?v=VMj-3S1tku0) video, where we build a tiny autograd engine. So the magic of PyTorch autograd is that after we call `.backward`, it will populate the `.grad` attribute of all the tensors that have `requires_grad=True` with the gradients of the loss with respect to that tensor. These gradients are telling us the slope of the loss for all of the input numbers in x,w,b. Therefore, the shape of `x.grad`, `w.grad`, and `b.grad` are exactly the same as the shape of `x`, `w`, and `b`.\\n\\nBut we don\\'t want to use PyTorch Autograd. We want to do the backward pass manually. So we take out pen and paper and write out the expression for LayerNorm. The forward pass has the following mathematical form:\\n\\n$\\\\text{LayerNorm}(x) = w \\\\odot \\\\frac{x - \\\\mu}{\\\\sqrt{\\\\sigma^2 + \\\\epsilon}} + b$\\n\\nwhere $\\\\odot$ is elementwise multiplication, $\\\\mu$ is the mean, $\\\\sigma^2$ is the variance, and $\\\\epsilon$ is a small constant to avoid division by zero. Remembering the rules of differentiation from calculus, we now want to derive the gradients. For this part, my video [Becoming a Backprop Ninja](https://www.youtube.com/watch?v=q8SA3rM6ckI) could be very helpful, as I work through (in detail) a similar layer - the Batch Normalization layer. When you work through the differentiation, you\\'ll notice that the expressions simplify analytically and you can move the terms around and simplify the expression somehwat. So you don\\'t have to manually backward every individual line in the forward pass. In particular, we get:\\n\\n```python\\n    @staticmethod\\n    def backward(dout, cache):\\n        x, w, mean, rstd = cache\\n        # recompute the norm (save memory at the cost of compute)\\n        norm = (x - mean) * rstd\\n        # gradients for weights, bias\\n        db = dout.sum((0, 1))\\n        dw = (dout * norm).sum((0, 1))\\n        # gradients for input\\n        dnorm = dout * w\\n        dx = dnorm - dnorm.mean(-1, keepdim=True) - norm * (dnorm * norm).mean(-1, keepdim=True)\\n        dx *= rstd\\n        return dx, dw, db\\n```\\n\\nSo given the gradients on every individual output number stored in `dout`, and the `cache` from the forward pass, we can now backward through this layer into the inputs, to continue the chain rule of the backward pass. So now we can do our own backward pass and see that they match (the errors are tiny):\\n\\n```python\\ndx, dw, db = LayerNorm.backward(dout, cache)\\nprint(\"dx error:\", (x.grad - dx).abs().max().item())\\nprint(\"dw error:\", (w.grad - dw).abs().max().item())\\nprint(\"db error:\", (b.grad - db).abs().max().item())\\n```\\n\\nNotice one more thing. Inside the backward pass we recomputed the variable `norm`. We already calculated this variable in the forward pass but then we threw it away! Couldn\\'t we have made this also be a part of the `cache` and save this recompute? Actually, we very well could and you\\'d of course get the exact same results. The amount of stuff we save into our `cache` is completely up to us. We didn\\'t even have to save `mean` and `rstd` either, and we could have recomputed them in the backward pass. The difference is that `mean` and `rstd` are very small, only of shape `B,T`, where as `norm` is of shape `B,T,C`. So this is simply a tradeoff between memory and compute. By not keeping `norm` in the cache, we are saving memory, but we are trading it off for a bit of compute later in the backward pass. This is very common in all the layers, and you\\'ll see that different implementations of various layers in deep learning frameworks may all have different \"checkpointing settings\". Yes, confusingly enough, this is called checkpointing and has nothing to do with saving the model weights to disk. It\\'s about saving intermediate variables in the forward pass to save compute in the backward pass.\\n\\nOkay so that\\'s the version with PyTorch tensors. Now we have to move this to C and get rid of the Tensor abstraction. Before I give you the full implementation of the forward pass, a brief word on Tensors. What are Tensors? They are 1) a 1D block of memory called Storage that holds the raw data, and 2) a View over that storage that holds its shape. [PyTorch Internals](http://blog.ezyang.com/2019/05/pytorch-internals/) could be helpful here. So for example if we have the 3D tensor:\\n\\n```python\\ntorch.manual_seed(42)\\nB, T, C = 2, 3, 4\\na = torch.randn(B, T, C)\\nprint(a)\\n\\ntensor([[[ 1.9269,  1.4873,  0.9007, -2.1055],\\n         [ 0.6784, -1.2345, -0.0431, -1.6047],\\n         [ 0.3559, -0.6866, -0.4934,  0.2415]],\\n\\n        [[-1.1109,  0.0915, -2.3169, -0.2168],\\n         [-0.3097, -0.3957,  0.8034, -0.6216],\\n         [-0.5920, -0.0631, -0.8286,  0.3309]]])\\n```\\n\\nThis is 2x3x4 Tensor, but the underlying memory of it is just one single 1D array of size 2\\\\*3\\\\*4=24. The View is just a shape over this 1D array. So now when we index into this PyTorch tensor, for example `a[1,2,3]`, PyTorch computes the offset into the 1D array as `1*3*4 + 2*4 + 3 = 23`, and return the value at that offset. The general formula is that if you want to retrieve any element `b,t,c`, you compute the offset into Storage as `b*T*C + t*C + c`. So for example:\\n\\n```python\\nb,t,c = 1,2,3\\nprint(a[b,t,c])\\nprint(a.view(-1)[b*T*C + t*C + c])\\n```\\n\\nBoth of these print 0.3309. So in this way, we know how to access all the individual elements, and how to offset all the pointers. Notice in particular that the channel dimension is the innermost dimension. So as we increase offset by 1, we are traversing the channel dimension. This is important to consider for the memory layout of our C implementation. The equivalent forward pass in C becomes:\\n\\n```c\\n#include <stdio.h>\\n#include <stdlib.h>\\n#include <math.h>\\n\\nvoid layernorm_forward(float* out, float* mean, float* rstd,\\n                       float* inp, float* weight, float* bias,\\n                       int B, int T, int C) {\\n    float eps = 1e-5f;\\n    for (int b = 0; b < B; b++) {\\n        for (int t = 0; t < T; t++) {\\n            // seek to the input position inp[b,t,:]\\n            float* x = inp + b * T * C + t * C;\\n            // calculate the mean\\n            float m = 0.0f;\\n            for (int i = 0; i < C; i++) {\\n                m += x[i];\\n            }\\n            m = m/C;\\n            // calculate the variance (without any bias correction)\\n            float v = 0.0f;\\n            for (int i = 0; i < C; i++) {\\n                float xshift = x[i] - m;\\n                v += xshift * xshift;\\n            }\\n            v = v/C;\\n            // calculate the rstd\\n            float s = 1.0f / sqrtf(v + eps);\\n            // seek to the output position in out[b,t,:]\\n            float* out_bt = out + b * T * C + t * C;\\n            for (int i = 0; i < C; i++) {\\n                float n = (s * (x[i] - m)); // normalized output\\n                float o = n * weight[i] + bias[i]; // scale and shift it\\n                out_bt[i] = o; // write\\n            }\\n            // cache the mean and rstd for the backward pass later\\n            mean[b * T + t] = m;\\n            rstd[b * T + t] = s;\\n        }\\n    }\\n}\\n```\\n\\nYou\\'ll see how I offset the pointer to the `inp[b,t]`, and then you know that the next `C` elements are the channels of that position in (batch, time). And the backward pass:\\n\\n```c\\nvoid layernorm_backward(float* dinp, float* dweight, float* dbias,\\n                        float* dout, float* inp, float* weight, float* mean, float* rstd,\\n                        int B, int T, int C) {\\n    for (int b = 0; b < B; b++) {\\n        for (int t = 0; t < T; t++) {\\n            float* dout_bt = dout + b * T * C + t * C;\\n            float* inp_bt = inp + b * T * C + t * C;\\n            float* dinp_bt = dinp + b * T * C + t * C;\\n            float mean_bt = mean[b * T + t];\\n            float rstd_bt = rstd[b * T + t];\\n\\n            // first: two reduce operations\\n            float dnorm_mean = 0.0f;\\n            float dnorm_norm_mean = 0.0f;\\n            for (int i = 0; i < C; i++) {\\n                float norm_bti = (inp_bt[i] - mean_bt) * rstd_bt;\\n                float dnorm_i = weight[i] * dout_bt[i];\\n                dnorm_mean += dnorm_i;\\n                dnorm_norm_mean += dnorm_i * norm_bti;\\n            }\\n            dnorm_mean = dnorm_mean / C;\\n            dnorm_norm_mean = dnorm_norm_mean / C;\\n\\n            // now iterate again and accumulate all the gradients\\n            for (int i = 0; i < C; i++) {\\n                float norm_bti = (inp_bt[i] - mean_bt) * rstd_bt;\\n                float dnorm_i = weight[i] * dout_bt[i];\\n                // gradient contribution to bias\\n                dbias[i] += dout_bt[i];\\n                // gradient contribution to weight\\n                dweight[i] += norm_bti * dout_bt[i];\\n                // gradient contribution to input\\n                float dval = 0.0f;\\n                dval += dnorm_i; // term 1\\n                dval -= dnorm_mean; // term 2\\n                dval -= norm_bti * dnorm_norm_mean; // term 3\\n                dval *= rstd_bt; // final scale\\n                dinp_bt[i] += dval;\\n            }\\n        }\\n    }\\n}\\n```\\n\\nOne additional detail to note is that we always += into the gradients. We never use = and we never use *=. This is important stylistically because if you have one variable used multiple times in a graph, the backward pass gradients always add up. In this repo this is not important because we don\\'t have exotic branching, but it\\'s proper. So during training we always first do `zero_grad` to set all the gradients to zero, and then we accumulate into them during backward pass.\\n\\nOne more note on differences between training and inference. Some of you may have already seen my earlier project [llama2.c](https://github.com/karpathy/llama2.c), which inferences Llama 2 architecture in pure C. Unlike GPT-2, Llama 2 swaps out LayerNorm for the much simpler RMSNorm. You can see the implementation of the [RMSNorm in llama2.c](https://github.com/karpathy/llama2.c/blob/master/run.c#L182), copy pasting it here:\\n\\n```c\\nvoid rmsnorm(float* o, float* x, float* weight, int size) {\\n    // calculate sum of squares\\n    float ss = 0.0f;\\n    for (int j = 0; j < size; j++) {\\n        ss += x[j] * x[j];\\n    }\\n    ss /= size;\\n    ss += 1e-5f;\\n    ss = 1.0f / sqrtf(ss);\\n    // normalize and scale\\n    for (int j = 0; j < size; j++) {\\n        o[j] = weight[j] * (ss * x[j]);\\n    }\\n}\\n```\\n\\nHow does this differ to our LayerNorm above?\\n\\n- First, algorithmically, you\\'ll notice that RMSNorm does not keep track of or subtract the mean, it only normalizes by the norm. Notice: norm, not standard deviation, because we did not subtract the mean. This is a simplification of the layer that has now become very trendy because it works just as well, if not slightly better. Also, the RMSNorm does not have biases, it only has a weight for scaling after normalization. In general, GPT-2 used way too many biases everywhere and it turns out you can remove these - from all the Linear Layers and from LayerNorms. The network can \"simulate\" biases if it needs them, e.g. by allocating one of the channel dimensions to be constant (data-independent), and then any weight multiplying that constant dimension will effectively work like a bias. This significantly simplies a lot of the code.\\n- Second, the inference code has no batch dimension B, i.e. the batch size is assumed to be 1. You could in principle have batched inference as well, especially if you wish to host an LLM that you expect many simultaneous queries to. But if you\\'re just running an LLM locally, chances are you just want to have a single \"stream\" of generation, so there is no batch size for parallelism that could support multiple streams at once. To keep things simple, llama2.c is not batched, and therefore you won\\'t see any loops that look like `for (int b = 0; b < B; b++)`.\\n- Third, this inference code has no time dimension T within this individual layer. During training, we can loop over time inside each layer and calculate the layernorm at all time steps. But during inference, we have to generate one token at a time, feeding the token predicted at time `t` into the forward pass of the Transformer at the next time step `t+1`. So this is why you don\\'t see any loops that look like `for (int t = 0; t < T; t++)` inside individual layers. This loop over time [does exist](https://github.com/karpathy/llama2.c/blob/master/run.c#L747), but it is on the outside of the Transformer forward pass.\\n- You\\'ll see that we don\\'t keep track of any intermediate calculations, memory, or cache. That\\'s because during inference, there is no `.backward` pass that will follow. We only need to calculate the output, and we don\\'t need to keep any intermediate variables around. As a result, the memory consumption of inference is significantly lower than that of training. We can afford to just discard activations, and only keep memory for the \"activation frontier\". Similarly, there is no need to implement the `backward` function for this RMSNorm anywhere, as there is no backward pass.\\n\\nAs a result of all these difference, training is significantly more complex and involved, both algorithmically and computationally, and that\\'s partly why I started by writing inference (llama2.c) before I implemented training (llm.c, here). Finally, I am attaching two helper files to this same directory that have the complete code. First:\\n\\n```\\npython layernorm.py\\n```\\n\\nTo write out the reference data from PyTorch. Then compile and run the C version:\\n\\n```\\ngcc layernorm.c -o layernorm -lm\\n./layernorm\\n```\\n\\nYou\\'ll see that everything matches ok.\\n\\nThis was just the LayerNorm. We go through the exact same process for all the other layers. Most of the other layers are actually easier than LayerNorm. Hope that helps!\\n',\n",
       "   'dependent_functions': ['']}],\n",
       " [{'name': 'train_gpt2cu',\n",
       "   'type': 'variable',\n",
       "   'summary': 'This is a Makefile command to train the GPT-2 model using CUDA with cuDNN for optimized deep learning training.',\n",
       "   'path': '/workspace/code_qa_llm/test_repo/scripts/run_gpt2_124M.sh',\n",
       "   'code': 'make train_gpt2cu USE_CUDNN=1',\n",
       "   'dependent_functions': ['']},\n",
       "  {'name': 'out_dir',\n",
       "   'type': 'variable',\n",
       "   'summary': 'This variable specifies the output directory where logs of the GPT-2 training with 124M parameters will be stored.',\n",
       "   'path': '/workspace/code_qa_llm/test_repo/scripts/run_gpt2_124M.sh',\n",
       "   'code': 'out_dir=\"log_gpt2_124M\"',\n",
       "   'dependent_functions': ['']},\n",
       "  {'name': 'done_file',\n",
       "   'type': 'variable',\n",
       "   'summary': \"This variable denotes the file that signals the completion of the training process. Its name is based on the output directory specified by the 'out_dir' variable.\",\n",
       "   'path': '/workspace/code_qa_llm/test_repo/scripts/run_gpt2_124M.sh',\n",
       "   'code': 'done_file=\"$out_dir/DONE_00018865\"',\n",
       "   'dependent_functions': ['out_dir']},\n",
       "  {'name': 'training_loop',\n",
       "   'type': 'code block',\n",
       "   'summary': \"This code block implements an infinite loop to train the GPT-2 model. The loop checks if a 'done' file exists to signal the completion of training to terminate the loop. If the training process stalls or crashes, the loop resumes the training process.\",\n",
       "   'path': '/workspace/code_qa_llm/test_repo/scripts/run_gpt2_124M.sh',\n",
       "   'code': 'while true; do\\n\\n    # exit condition is that optimization has finished\\n    if [ -f \"$done_file\" ]; then\\n        echo \"File $done_file exists. Exiting the loop.\"\\n        break\\n    fi\\n\\n    # run python dev/data/fineweb.py --version 10B to prepro data\\n    # run python dev/data/hellaswag.py to prepro hellaswag eval\\n    mpirun -np 8 ./train_gpt2cu \\\\\\n                -i \"dev/data/fineweb10B/fineweb_train_*.bin\" \\\\\\n                -j \"dev/data/fineweb10B/fineweb_val_*.bin\" \\\\\\n                -o $out_dir \\\\\\n                -v 250 -s 20000 -g 144 \\\\\\n                -h 1 \\\\\\n                -b 64 -t 1024 \\\\\\n                -d 524288 \\\\\\n                -r 0 \\\\\\n                -z 1 \\\\\\n                -c 0.1 \\\\\\n                -l 0.0006 \\\\\\n                -q 0.0 \\\\\\n                -u 700 \\\\\\n                -n 5000 \\\\\\n                -y 1 \\\\\\n                -e \"d12\"\\n\\n    sleep 1\\ndone',\n",
       "   'dependent_functions': ['done_file', ' out_dir']},\n",
       "  {'name': 'example.sh',\n",
       "   'type': 'file',\n",
       "   'summary': 'This script is responsible for training a GPT-2 model with 124M parameters on 10 billion tokens using multiple GPU nodes. It includes a looping mechanism to resume training in cases of interruptions, setting various hyperparameters and using MPI for parallel execution.',\n",
       "   'path': '/workspace/code_qa_llm/test_repo/scripts/run_gpt2_124M.sh',\n",
       "   'code': '# GPT-2 (124M) repro on FineWeb\\n# 124M parameter model on 10B tokens\\n# => 6 * 124e6 * 10e9 = 7.44e18 ~= 7e18 capability model\\n# 18,865 steps of 524,288 tokens/step\\n# on 8X A100 80GB SXM ($14/hr) steps in ~300ms/iter\\n# => training time 18,865 * 300ms = 94.3 min ~= $20\\n\\nmake train_gpt2cu USE_CUDNN=1\\nout_dir=\"log_gpt2_124M\"\\ndone_file=\"$out_dir/DONE_00018865\"\\n\\n# in case the training stalls or crashes, loop to resume (-y 1)\\nwhile true; do\\n\\n    # exit condition is that optimization has finished\\n    if [ -f \"$done_file\" ]; then\\n        echo \"File $done_file exists. Exiting the loop.\"\\n        break\\n    fi\\n\\n    # run python dev/data/fineweb.py --version 10B to prepro data\\n    # run python dev/data/hellaswag.py to prepro hellaswag eval\\n    mpirun -np 8 ./train_gpt2cu \\\\\\n                -i \"dev/data/fineweb10B/fineweb_train_*.bin\" \\\\\\n                -j \"dev/data/fineweb10B/fineweb_val_*.bin\" \\\\\\n                -o $out_dir \\\\\\n                -v 250 -s 20000 -g 144 \\\\\\n                -h 1 \\\\\\n                -b 64 -t 1024 \\\\\\n                -d 524288 \\\\\\n                -r 0 \\\\\\n                -z 1 \\\\\\n                -c 0.1 \\\\\\n                -l 0.0006 \\\\\\n                -q 0.0 \\\\\\n                -u 700 \\\\\\n                -n 5000 \\\\\\n                -y 1 \\\\\\n                -e \"d12\"\\n\\n    sleep 1\\ndone\\n',\n",
       "   'dependent_functions': ['']}]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=[]\n",
    "for path in paths:\n",
    "        ans = get_summary_non_py(path)\n",
    "        res.append(ans)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'main',\n",
       "  'type': 'function',\n",
       "  'summary': 'This function serves as the entry point for profiling the GPT-2 training using CUDA. It initializes the multi-GPU configuration, builds the GPT-2 model from a checkpoint, sets the batch size and sequence length, allocates memory for input and output data, overrides the number of layers, and performs a training step including forward pass, zeroing gradients, backward pass, and updating the model. Finally, it synchronizes CUDA devices and frees resources.',\n",
       "  'path': 'test_repo/profile_gpt2.cu',\n",
       "  'code': 'int main(int argc, char *argv[]) {\\n        multi_gpu_config = multi_gpu_config_init(&argc, &argv);\\n        common_start(true, true);\\n\\n        // build the GPT-2 model from a checkpoint\\n        GPT2 model;\\n        gpt2_build_from_checkpoint(&model, \"gpt2_124M_bf16.bin\");\\n\\n        int B = 24; // if program OOMs decrease this number, e.g. all the way down to 4 or etc\\n        int T = 1024; // if even that OOMs move on to this one. keep them nice and powers of 2\\n        printf(\"batch size: %d\\\\n\", B);\\n        printf(\"sequence length: %d\\\\n\", T);\\n\\n        int* x = (int*)mallocCheck(B * T * sizeof(int));\\n        int* y = (int*)mallocCheck(B * T * sizeof(int));\\n        for(int  i = 0; i &lt; B  * T; ++i) {\\n            x[i] = i % model.config.vocab_size;\\n            y[i] = i % model.config.vocab_size;\\n        }\\n\\n        // override number of layers to 1 because all layers repeat the same kernels, only profile once\\n        model.config.num_layers = 1;\\n        set_zero_configs(&multi_gpu_config, 0, model.num_parameters);\\n\\n        // do a training step\\n        gpt2_forward(&model, x, y, B, T);\\n        gpt2_zero_grad(&model);\\n        gpt2_backward(&model, x);\\n        gpt2_update(&model, 1e-4f, 0.9f, 0.999f, 1e-8f, 0.0f, 1.f, 1, &multi_gpu_config);\\n        cudaCheck(cudaDeviceSynchronize()); // finish all CUDA work to get correct precise timings\\n\\n        // free\\n        gpt2_free(&model);\\n        common_free(model);\\n        return 0;\\n    }',\n",
       "  'dependent_functions': 'multi_gpu_config_init, common_start, GPT2, gpt2_build_from_checkpoint, mallocCheck, set_zero_configs, gpt2_forward, gpt2_zero_grad, gpt2_backward, gpt2_update, cudaCheck, gpt2_free, common_free'},\n",
       " {'name': 'profile_gpt2cu.cu',\n",
       "  'type': 'file',\n",
       "  'summary': 'This file is a profiling tool for CUDA kernels in the GPT-2 training loop. It initializes multi-GPU configurations, builds the GPT-2 model from a checkpoint, sets batch size and sequence length, allocates memory, overrides the number of layers to profile only once, performs a training step, synchronizes CUDA devices, and frees resources. It includes instructions for usage with NVIDIA Nsight Compute CLI.',\n",
       "  'code': '/*\\nThis code is a convenience tool for profiling the CUDA kernels in the training\\nloop of train_gpt2.cu. Compile:\\n\\nmake profile_gpt2cu NO_MULTI_GPU=1\\n\\nAnd then e.g. use ncu from NVIDIA. The CLI docs for example:\\nhttps://docs.nvidia.com/nsight-compute/NsightComputeCli/\\n\\nTLDR run like:\\n\\nsudo ncu --set full --import-source yes -o profile -f ./profile_gpt2cu\\n\\nThis:\\n- `--set full` means we\\'ll collect A LOT of metrics. take out for less\\n- `--import-source yes` means we\\'ll get the source code in the profile\\n- `-o profile` writes the results into file profile.ncu-rep\\n- `-f` forces overwrite of the profile.ncu-rep file\\n- `./profile_gpt2cu` is the executable we want to profile\\n\\nThis writes results into profile.ncu-rep output file.\\nYou can open this up in NVIDIA Nsight Compute UI.\\nFor example, I have NVIDIA Nsight Compute installed on my Mac, and I rsync\\nthe profile.ncu-rep from a cloud box to local to pretty view.\\n*/\\n\\n#define TESTING\\n#include \"train_gpt2.cu\"\\n\\nint main(int argc, char *argv[]) {\\n    multi_gpu_config = multi_gpu_config_init(&argc, &argv);\\n    common_start(true, true);\\n\\n    // build the GPT-2 model from a checkpoint\\n    GPT2 model;\\n    gpt2_build_from_checkpoint(&model, \"gpt2_124M_bf16.bin\");\\n\\n    int B = 24; // if program OOMs decrease this number, e.g. all the way down to 4 or etc\\n    int T = 1024; // if even that OOMs move on to this one. keep them nice and powers of 2\\n    printf(\"batch size: %d\\\\n\", B);\\n    printf(\"sequence length: %d\\\\n\", T);\\n\\n    int* x = (int*)mallocCheck(B * T * sizeof(int));\\n    int* y = (int*)mallocCheck(B * T * sizeof(int));\\n    for(int  i = 0; i < B  * T; ++i) {\\n        x[i] = i % model.config.vocab_size;\\n        y[i] = i % model.config.vocab_size;\\n    }\\n\\n    // override number of layers to 1 because all layers repeat the same kernels, only profile once\\n    model.config.num_layers = 1;\\n    set_zero_configs(&multi_gpu_config, 0, model.num_parameters);\\n\\n    // do a training step\\n    gpt2_forward(&model, x, y, B, T);\\n    gpt2_zero_grad(&model);\\n    gpt2_backward(&model, x);\\n    gpt2_update(&model, 1e-4f, 0.9f, 0.999f, 1e-8f, 0.0f, 1.f, 1, &multi_gpu_config);\\n    cudaCheck(cudaDeviceSynchronize()); // finish all CUDA work to get correct precise timings\\n\\n    // free\\n    gpt2_free(&model);\\n    common_free(model);\\n    return 0;\\n}\\n',\n",
       "  'path': 'test_repo/profile_gpt2.cu',\n",
       "  'dependent_functions': ''}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import file_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ = file_read(\"/workspace/code_qa_llm/sos/summaries.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'run.py',\n",
       "  'summary': \"\\nThe provided code initializes and runs a Flask web application. Firstly, it imports the `create_app` function from the `app` module and the `putModelScheduler` from `app.tasks.putModel`. The Flask app is created by calling `create_app()`. A route `/status` is defined which, when accessed via a GET request, returns the response 'Running!'. In the main execution block, the `putModelScheduler` is started, which seems to initiate a scheduler task. Finally, the Flask app runs on host `0.0.0.0` and port `5000` with debugging enabled.\\n\",\n",
       "  'type': 'file',\n",
       "  'code': 'from app import create_app\\nfrom app.tasks.putModel import putModelScheduler\\n\\napp = create_app()\\n\\n@app.route(\\'/status\\', methods=[\\'GET\\'])\\ndef status():\\n    return \\'Running!\\'\\n\\nif __name__ == \"__main__\":\\n    putModelScheduler.start()\\n    app.run(host=\"0.0.0.0\",port=5000, debug=True)',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/run.py'},\n",
       " {'name': 'status',\n",
       "  'summary': \"Defines a function `status` that returns the string 'Running!'\",\n",
       "  'type': 'function',\n",
       "  'code': \"def status():\\n    return 'Running!'\\n\",\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/run.py',\n",
       "  'dependent_functions': []},\n",
       " {'name': 'app.py',\n",
       "  'summary': '\\nThe code sets up and runs a Flask web application. It imports necessary modules and initializes the Flask app object via a factory function `create_app` from the `app` package. The `CORS` (Cross-Origin Resource Sharing) is enabled for the app, allowing resource sharing from different origins. A rate limiter using `flask_limiter` is also applied to the app to handle request rate limiting.\\n\\nA single endpoint `/status` is defined that returns the string \\'Running!\\' when accessed via a GET request, acting as a health check for the service.\\n\\nIn the main execution block, the app runs in debug mode, accessible to any IP address (host set to \"0.0.0.0\"). Additionally, a background task `putModelScheduler` is started, intended for scheduled model updates. An optional task `clearModelScheduler` is also mentioned but is commented out.\\n\\nThe setup ensures the application is ready for deployment with CORS and rate limiting, and includes a mechanism for running background tasks.\\n',\n",
       "  'type': 'file',\n",
       "  'code': 'from app import create_app\\nfrom flask_cors import CORS\\nimport os\\nfrom flask_limiter import Limiter\\nfrom app.tasks.putModel import putModelScheduler\\n# from app.tasks.clearModels import clearModelScheduler\\n\\napp = create_app()\\n\\nCORS(app)\\nlimiter = Limiter(app)\\n\\n@app.route(\\'/status\\', methods=[\\'GET\\'])\\ndef status():\\n    return \\'Running!\\'\\n\\nif __name__ == \"__main__\":\\n    app.run(host=\"0.0.0.0\", debug=True)\\n    putModelScheduler.start()\\n    # clearModelScheduler.start()\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app.py'},\n",
       " {'name': 'status',\n",
       "  'summary': \"This function, status(), returns the string 'Running!'.\",\n",
       "  'type': 'function',\n",
       "  'code': \"def status():\\n    return 'Running!'\\n\",\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app.py',\n",
       "  'dependent_functions': []},\n",
       " {'name': '__init__.py',\n",
       "  'summary': \"\\nThis Python script configures and initializes a Flask application with various features and settings. It imports necessary modules, including `logging.config`, `environ` from `os`, `load_dotenv` from `dotenv`, and components from `Flask`, `flask_cors`, and `redis`. It also imports custom configuration settings from a `config` module and rate limiting utilities from `flask_limiter`.\\n\\nThe `create_app` function initializes the Flask application:\\n1. Retrieves the application environment via the `get_environment` function, which defaults to 'development' if not set.\\n2. Loads environment variables from a `.env` file using `load_dotenv`.\\n3. Configures logging based on the current environment's settings.\\n4. Initializes the Flask app with a specific name and configuration settings.\\n5. Enables Cross-Origin Resource Sharing (CORS) for all API routes.\\n6. Registers additional routes and blueprints: it imports and executes the `setup_blueprints` function from `app.routes`, and registers a health check blueprint with the prefix `/status`.\\n\\nThe `get_environment` function simply returns the value of the environment variable `APPLICATION_ENV`, defaulting to 'development' if it's not set.\\n\",\n",
       "  'type': 'file',\n",
       "  'code': \"import logging.config\\nfrom os import environ\\nfrom dotenv import load_dotenv\\nfrom flask import Flask\\nfrom flask_cors import CORS\\nfrom redis import Redis\\nfrom .config import config as app_config\\nfrom flask_limiter import Limiter\\nfrom flask_limiter.util import get_remote_address\\n\\napp = Flask(__name__)\\n\\ndef create_app():\\n    APPLICATION_ENV = get_environment()\\n    # loading env vars from .env file\\n    load_dotenv()\\n    logging.config.dictConfig(app_config[APPLICATION_ENV].LOGGING)\\n\\n    app = Flask(app_config[APPLICATION_ENV].APP_NAME)\\n    app.config.from_object(app_config[APPLICATION_ENV])\\n\\n    CORS(app, resources={r'/api/*': {'origins': '*'}})\\n\\n    with app.app_context():\\n        from app.routes import setup_blueprints\\n        from .core.health import core as core_blueprint\\n        app.register_blueprint(\\n            core_blueprint,\\n            url_prefix = '/status'\\n        )\\n        setup_blueprints()\\n    return app\\n\\ndef get_environment():\\n    return environ.get('APPLICATION_ENV') or 'development'\\n\",\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/__init__.py'},\n",
       " {'name': 'create_app',\n",
       "  'summary': 'Initializes a Flask application: loads environment variables, configures logging, enables CORS for all API routes, registers blueprints, and returns the Flask app instance.',\n",
       "  'type': 'function',\n",
       "  'code': \"def create_app():\\n    APPLICATION_ENV = get_environment()\\n    # loading env vars from .env file\\n    load_dotenv()\\n    logging.config.dictConfig(app_config[APPLICATION_ENV].LOGGING)\\n\\n    app = Flask(app_config[APPLICATION_ENV].APP_NAME)\\n    app.config.from_object(app_config[APPLICATION_ENV])\\n\\n    CORS(app, resources={r'/api/*': {'origins': '*'}})\\n\\n    with app.app_context():\\n        from app.routes import setup_blueprints\\n        from .core.health import core as core_blueprint\\n        app.register_blueprint(\\n            core_blueprint,\\n            url_prefix = '/status'\\n        )\\n        setup_blueprints()\\n    return app\\n\",\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/__init__.py',\n",
       "  'dependent_functions': ['get_environment',\n",
       "   'load_dotenv',\n",
       "   'Flask',\n",
       "   'CORS',\n",
       "   'setup_blueprints']},\n",
       " {'name': 'get_environment',\n",
       "  'summary': \"Returns the value of the 'APPLICATION_ENV' environment variable; defaults to 'development' if the variable is not set.\",\n",
       "  'type': 'function',\n",
       "  'code': \"def get_environment():\\n    return environ.get('APPLICATION_ENV') or 'development'\\n\",\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/__init__.py',\n",
       "  'dependent_functions': []},\n",
       " {'name': 'routes.py',\n",
       "  'summary': '\\nThe provided code is a Python script designed to be used within a Flask web application. It imports the Flask application instance as `app` and three blueprints: `model_mappings_bp`, `prometheus_bp`, and `tasks_bp`, from their respective modules within the `app.blueprints` package. The script defines a function `setup_blueprints()` that registers these blueprints to the main Flask app with specific URL prefixes. Specifically:\\n- `model_mappings_bp` is registered with the URL prefix \"/server-mappings\".\\n- `prometheus_bp` is registered with the URL prefix \"/prometheus\".\\n- `tasks_bp` is registered with the URL prefix \"/tasks\".\\n\\nThis setup allows the application to modularize different parts of the web app, making it easier to manage and scale.\\n',\n",
       "  'type': 'file',\n",
       "  'code': 'from flask import current_app as app\\nfrom app.blueprints.model_mappings import model_mappings_bp\\nfrom app.blueprints.prometheus import prometheus_bp\\nfrom app.blueprints.tasks import tasks_bp\\n\\n\\ndef setup_blueprints():\\n    app.register_blueprint(model_mappings_bp, url_prefix = \"/server-mappings\")\\n    app.register_blueprint(prometheus_bp, url_prefix = \"/prometheus\")\\n    app.register_blueprint(tasks_bp, url_prefix = \"/tasks\")',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/routes.py'},\n",
       " {'name': 'setup_blueprints',\n",
       "  'summary': 'Registers three blueprints to a Flask app with specific URL prefixes: \"/server-mappings\" for model_mappings_bp, \"/prometheus\" for prometheus_bp, and \"/tasks\" for tasks_bp.',\n",
       "  'type': 'function',\n",
       "  'code': 'def setup_blueprints():\\n    app.register_blueprint(model_mappings_bp, url_prefix = \"/server-mappings\")\\n    app.register_blueprint(prometheus_bp, url_prefix = \"/prometheus\")\\n    app.register_blueprint(tasks_bp, url_prefix = \"/tasks\")',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/routes.py',\n",
       "  'dependent_functions': []},\n",
       " {'name': 'config.py',\n",
       "  'summary': \"\\nThe code snippet initializes and manages configuration settings for different environments (development, staging, and production) in a Python application. It imports necessary modules, sets the base directory, and loads environment variables from a `.env` file using `dotenv`. \\n\\nThe `BaseConfig` class defines several class attributes, which include application settings like `APP_NAME`, API keys, URLs for various services (e.g., Redis, Prometheus, AWS), logging configurations, and endpoint URLs for different functionalities. It also sets up logging handlers and formatters, specifying log file locations and rotation policies.\\n\\nThree additional classes, `Development`, `Staging`, and `Production`, inherit from `BaseConfig` and set environment-specific attributes like `DEBUG` mode and `ENV` (environment name). \\n\\nFinally, a `config` dictionary maps the environment names ('development', 'staging', 'production') to their respective configuration classes.\\n\",\n",
       "  'type': 'file',\n",
       "  'code': 'from os import environ, path\\n\\nfrom dotenv import load_dotenv\\n\\nbasedir = path.abspath(path.join(path.dirname(__file__), \\'..\\'))\\n# loading env vars from .env file\\nload_dotenv()\\n\\n\\nclass BaseConfig(object):\\n    \\'\\'\\' Base config class. \\'\\'\\'\\n\\n    APP_NAME = environ.get(\\'APP_NAME\\') or \\'SOS\\'\\n    ORIGINS = [\\'*\\']\\n    EMAIL_CHARSET = \\'UTF-8\\'\\n    API_KEY = environ.get(\\'API_KEY\\')\\n    BROKER_URL = environ.get(\\'BROKER_URL\\')\\n    RESULT_BACKEND = environ.get(\\'RESULT_BACKEND\\')\\n    REDIS_HOST = environ.get(\"REDIS_HOST\")\\n    REDIS_URL=environ.get(\\'REDIS_URL\\')\\n    REDIS_PORT=environ.get(\\'REDIS_PORT\\')\\n    REDIS_REQUEST_STATUS_DB=environ.get(\\'REDIS_DB\\')\\n    LOG_INFO_FILE = path.join(basedir, \\'log\\', \\'info.log\\')\\n    LOG_CELERY_FILE = path.join(basedir, \\'log\\', \\'celery.log\\')\\n    LOGGING = {\\n        \\'version\\': 1,\\n        \\'disable_existing_loggers\\': False,\\n        \\'formatters\\': {\\n            \\'standard\\': {\\n                \\'format\\': \\'[%(asctime)s] - %(name)s - %(levelname)s - \\'\\n                \\'%(message)s\\',\\n                \\'datefmt\\': \\'%b %d %Y %H:%M:%S\\'\\n            },\\n            \\'simple\\': {\\n                \\'format\\': \\'%(levelname)s - %(message)s\\'\\n            },\\n        },\\n        \\'handlers\\': {\\n            \\'console\\': {\\n                \\'level\\': \\'DEBUG\\',\\n                \\'class\\': \\'logging.StreamHandler\\',\\n                \\'formatter\\': \\'simple\\'\\n            },\\n            \\'log_info_file\\': {\\n                \\'level\\': \\'DEBUG\\',\\n                \\'class\\': \\'logging.handlers.RotatingFileHandler\\',\\n                \\'filename\\': LOG_INFO_FILE,\\n                \\'maxBytes\\': 16777216,  # 16megabytes\\n                \\'formatter\\': \\'standard\\',\\n                \\'backupCount\\': 5\\n            },\\n        },\\n        \\'loggers\\': {\\n            APP_NAME: {\\n                \\'level\\': \\'DEBUG\\',\\n                \\'handlers\\': [\\'log_info_file\\'],\\n            },\\n        },\\n    }\\n    INFERENCE_QUEUE_URL = environ.get(\\'INFERENCE_QUEUE_URL\\')\\n    AWS_ACCESS_KEY_ID = environ.get(\\'AWS_ACCESS_KEY_ID\\')\\n    AWS_SECRET_ACCESS_KEY = environ.get(\\'AWS_SECRET_ACCESS_KEY\\')\\n    REGION_NAME = environ.get(\\'REGION_NAME\\')\\n    PROMETHEUS_URL = environ.get(\\'PROMETHEUS_URL\\')\\n    PROMETHEUS_USERNAME = environ.get(\\'PROMETHEUS_USERNAME\\')\\n    PROMETHEUS_PASSWORD = environ.get(\\'PROMETHEUS_PASSWORD\\')\\n    DOWNLOAD_MODEL_URL = \\'voltaml/download_model\\'\\n    UNLOAD_MODEL_URL = \\'voltaml/unload_model\\'\\n    GET_GPU_MODELS_INFO = \\'voltaml/track\\'\\n    INFERENCE_GPU_URL = \\'voltaml/txt2img\\'\\n    PROXY_INFERENCE_URL = environ.get(\\'PROXY_INFERENCE_URL\\')\\n    SPOT_HUB_STATUS_UPDATE_URL = environ.get(\\'SPOT_HUB_STATUS_UPDATE_URL\\')\\n    SPOT_FINETUNE_STATUS_UPDATE_URL = environ.get(\\'SPOT_FINETUNE_STATUS_UPDATE_URL\\')\\n    DEFAULT_GPU_APP_PORT = \\'5001\\' #Production - 5001, Stage - 5002\\n    SPOT_SECRET_TOKEN = environ.get(\\'SPOT_SECRET_TOKEN\\')\\n    SPOT_GPU_URL = environ.get(\\'SPOT_GPU_URL\\')\\n    SPOT_GPU_ACCESS_TOKEN = environ.get(\\'SPOT_GPU_ACCESS_TOKEN\\')\\n\\n\\n\\nclass Development(BaseConfig):\\n    \\'\\'\\' Development config. \\'\\'\\'\\n\\n    DEBUG = True\\n    ENV = \\'dev\\'\\n\\n\\nclass Staging(BaseConfig):\\n    \\'\\'\\' Staging config. \\'\\'\\'\\n\\n    DEBUG = True\\n    ENV = \\'staging\\'\\n\\n\\nclass Production(BaseConfig):\\n    \\'\\'\\' Production config \\'\\'\\'\\n\\n    DEBUG = False\\n    ENV = \\'production\\'\\n\\n\\nconfig = {\\n    \\'development\\': Development,\\n    \\'staging\\': Staging,\\n    \\'production\\': Production,\\n}',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/config.py'},\n",
       " {'name': 'BaseConfig',\n",
       "  'summary': 'Defines a `BaseConfig` class for application configuration, loading settings from environment variables. It sets up logging to console and rotating file handlers, specifies URLs for various services, defines constants for HTTP endpoints, and includes AWS and Redis connection details.',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/config.py',\n",
       "  'type': 'class',\n",
       "  'code': 'class BaseConfig(object):\\n    \\'\\'\\' Base config class. \\'\\'\\'\\n\\n    APP_NAME = environ.get(\\'APP_NAME\\') or \\'SOS\\'\\n    ORIGINS = [\\'*\\']\\n    EMAIL_CHARSET = \\'UTF-8\\'\\n    API_KEY = environ.get(\\'API_KEY\\')\\n    BROKER_URL = environ.get(\\'BROKER_URL\\')\\n    RESULT_BACKEND = environ.get(\\'RESULT_BACKEND\\')\\n    REDIS_HOST = environ.get(\"REDIS_HOST\")\\n    REDIS_URL=environ.get(\\'REDIS_URL\\')\\n    REDIS_PORT=environ.get(\\'REDIS_PORT\\')\\n    REDIS_REQUEST_STATUS_DB=environ.get(\\'REDIS_DB\\')\\n    LOG_INFO_FILE = path.join(basedir, \\'log\\', \\'info.log\\')\\n    LOG_CELERY_FILE = path.join(basedir, \\'log\\', \\'celery.log\\')\\n    LOGGING = {\\n        \\'version\\': 1,\\n        \\'disable_existing_loggers\\': False,\\n        \\'formatters\\': {\\n            \\'standard\\': {\\n                \\'format\\': \\'[%(asctime)s] - %(name)s - %(levelname)s - \\'\\n                \\'%(message)s\\',\\n                \\'datefmt\\': \\'%b %d %Y %H:%M:%S\\'\\n            },\\n            \\'simple\\': {\\n                \\'format\\': \\'%(levelname)s - %(message)s\\'\\n            },\\n        },\\n        \\'handlers\\': {\\n            \\'console\\': {\\n                \\'level\\': \\'DEBUG\\',\\n                \\'class\\': \\'logging.StreamHandler\\',\\n                \\'formatter\\': \\'simple\\'\\n            },\\n            \\'log_info_file\\': {\\n                \\'level\\': \\'DEBUG\\',\\n                \\'class\\': \\'logging.handlers.RotatingFileHandler\\',\\n                \\'filename\\': LOG_INFO_FILE,\\n                \\'maxBytes\\': 16777216,  # 16megabytes\\n                \\'formatter\\': \\'standard\\',\\n                \\'backupCount\\': 5\\n            },\\n        },\\n        \\'loggers\\': {\\n            APP_NAME: {\\n                \\'level\\': \\'DEBUG\\',\\n                \\'handlers\\': [\\'log_info_file\\'],\\n            },\\n        },\\n    }\\n    INFERENCE_QUEUE_URL = environ.get(\\'INFERENCE_QUEUE_URL\\')\\n    AWS_ACCESS_KEY_ID = environ.get(\\'AWS_ACCESS_KEY_ID\\')\\n    AWS_SECRET_ACCESS_KEY = environ.get(\\'AWS_SECRET_ACCESS_KEY\\')\\n    REGION_NAME = environ.get(\\'REGION_NAME\\')\\n    PROMETHEUS_URL = environ.get(\\'PROMETHEUS_URL\\')\\n    PROMETHEUS_USERNAME = environ.get(\\'PROMETHEUS_USERNAME\\')\\n    PROMETHEUS_PASSWORD = environ.get(\\'PROMETHEUS_PASSWORD\\')\\n    DOWNLOAD_MODEL_URL = \\'voltaml/download_model\\'\\n    UNLOAD_MODEL_URL = \\'voltaml/unload_model\\'\\n    GET_GPU_MODELS_INFO = \\'voltaml/track\\'\\n    INFERENCE_GPU_URL = \\'voltaml/txt2img\\'\\n    PROXY_INFERENCE_URL = environ.get(\\'PROXY_INFERENCE_URL\\')\\n    SPOT_HUB_STATUS_UPDATE_URL = environ.get(\\'SPOT_HUB_STATUS_UPDATE_URL\\')\\n    SPOT_FINETUNE_STATUS_UPDATE_URL = environ.get(\\'SPOT_FINETUNE_STATUS_UPDATE_URL\\')\\n    DEFAULT_GPU_APP_PORT = \\'5001\\' #Production - 5001, Stage - 5002\\n    SPOT_SECRET_TOKEN = environ.get(\\'SPOT_SECRET_TOKEN\\')\\n    SPOT_GPU_URL = environ.get(\\'SPOT_GPU_URL\\')\\n    SPOT_GPU_ACCESS_TOKEN = environ.get(\\'SPOT_GPU_ACCESS_TOKEN\\')\\n',\n",
       "  'class_funcs': {}},\n",
       " {'name': 'Development',\n",
       "  'summary': \"Defines a Development class inheriting from BaseConfig with DEBUG set to True and ENV set to 'dev'.\",\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/config.py',\n",
       "  'type': 'class',\n",
       "  'code': \"class Development(BaseConfig):\\n    ''' Development config. '''\\n\\n    DEBUG = True\\n    ENV = 'dev'\\n\",\n",
       "  'class_funcs': {}},\n",
       " {'name': 'Staging',\n",
       "  'summary': \"Defines a Staging configuration class that inherits from BaseConfig, setting DEBUG to True and ENV to 'staging'.\",\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/config.py',\n",
       "  'type': 'class',\n",
       "  'code': \"class Staging(BaseConfig):\\n    ''' Staging config. '''\\n\\n    DEBUG = True\\n    ENV = 'staging'\\n\",\n",
       "  'class_funcs': {}},\n",
       " {'name': 'Production',\n",
       "  'summary': \"\\nDefines a `Production` class inheriting from `BaseConfig` with production-specific settings: `DEBUG` set to `False` and `ENV` set to `'production'`.\\n\",\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/config.py',\n",
       "  'type': 'class',\n",
       "  'code': \"class Production(BaseConfig):\\n    ''' Production config '''\\n\\n    DEBUG = False\\n    ENV = 'production'\\n\",\n",
       "  'class_funcs': {}},\n",
       " {'name': 'security.py',\n",
       "  'summary': \"\\nThe provided code is a set of Flask API helper functions and decorators for handling token-based authentication. The code imports various necessary modules and performs the following key functions:\\n\\n1. **get_access_token**: Extracts an 'authorization' token from request headers, query parameters, or the request body.\\n2. **get_api_token**: Similar to `get_access_token`, but looks for an 'x-api-key'.\\n3. **authenticate_user**: Authenticates a user by sending a request to an external authentication service ('REFINE_AUTH_URL') using either a JWT token or API key, and returns user details if authentication is successful.\\n4. **jwt_or_api_token_required (decorator)**: Ensures either an 'authorization' token or 'x-api-key' is present in the request headers, and validates that having both is not allowed.\\n5. **jwt_token_required (decorator)**: Ensures an 'authorization' token is present in the request headers and validates it.\\n6. **get_user_id**: Retrieves a user ID by authenticating the provided token, using cached information if available.\\n7. **is_admin_user**: Checks if the user, authenticated via token or API key, is an admin user by comparing user ID with an environment variable ('WHITELIST_USER_ID').\\n\\nThe code makes extensive use of caching utilities for tokens and user details using Redis-related functions (`get_key_value_hash`, `set_key_value_hash`, and `set_key_expiry`). It ensures secure and validated access to resources by employing decorators and utility functions.\\n\",\n",
       "  'type': 'file',\n",
       "  'code': 'from os.path import join, dirname\\nfrom os import environ\\nfrom flask import Blueprint, current_app, request, abort\\nimport requests\\nfrom functools import wraps\\nimport jwt\\nfrom http import HTTPStatus\\nfrom datetime import datetime, timedelta\\nfrom app.utils.redis_utils import get_key_value_hash, set_key_value_hash, set_key_expiry\\n\\n\\ndef get_access_token():\\n    \"\"\"\\n    Looks for token in Headers, Query string and Post body\\n    :return: token extracted from request headers\\n    \"\"\"\\n    auth_header = request.headers.get(\"authorization\")\\n    if auth_header and \"Bearer\" in auth_header:\\n        _, token = auth_header.split(\" \", 1)\\n        return token\\n\\n    if request.args.get(\"token\"):\\n        return request.args.get(\"token\")\\n\\n    return request.get_json().get(\"token\")\\n\\n\\ndef get_api_token():\\n    \"\"\"\\n    Looks for api token in Headers,  Query string and Post body\\n    :return: token extracted from request headers\\n    \"\"\"\\n    token = request.headers.get(\"x-api-key\")\\n    if token:\\n        return token\\n\\n    if request.args.get(\"token\"):\\n        return request.args.get(\"token\")\\n\\n    return request.get_json().get(\"token\")\\n\\n\\ndef authenticate_user():\\n    \"\"\"\\n    Authenticate user from Segmind\\'s Refine backend.\\n    :return: user details\\n    \"\"\"\\n    if request.headers.get(\"authorization\"):\\n        jwt_token = get_access_token()\\n        headers = {\"authorization\": f\"Bearer {jwt_token}\"}\\n\\n    elif request.headers.get(\"x-api-key\"):\\n        api_token = get_api_token()\\n        headers = {\"x-api-key\": f\"{api_token}\"}\\n\\n    response = requests.get(url=environ.get(\\'REFINE_AUTH_URL\\'), headers=headers)\\n\\n    if not response.status_code == 200:\\n        return None\\n\\n    if \"uuid\" not in response.json():\\n        return None\\n\\n    if \"email\" not in response.json():\\n        return None\\n\\n    if request.headers.get(\"x-api-key\"):\\n        if \"expiry\" not in response.json():\\n            return None\\n\\n    user_details = response.json()\\n    if \\'credits\\' in user_details:\\n        user_details[\"credits\"] = round(float(user_details[\"credits\"]))\\n\\n    user_details.pop(\\'cluster\\', None)\\n    user_details.pop(\\'role_in_org\\', None)\\n    user_details.pop(\\'organization\\', None)\\n\\n    return user_details\\n\\n\\ndef jwt_or_api_token_required(func):\\n    @wraps(func)\\n    def decorated_function(*args, **kwargs):\\n        auth_headers = request.headers.get(\\'Authorization\\', \\'\\').split()\\n        api_key = request.headers.get(\\'x-api-key\\')\\n\\n        if api_key and auth_headers:\\n            return (\\n                {\"error\": \"Both Authorizationn and x-api-key can not be present\"},\\n                HTTPStatus.UNAUTHORIZED,\\n            )\\n\\n        if auth_headers:\\n            if len(auth_headers) != 2:\\n                return (\\n                {\"error\": \"Invalid Authorization header\"},\\n                HTTPStatus.UNAUTHORIZED,\\n            )\\n\\n            token_type, token = auth_headers\\n\\n            if token_type != \\'Bearer\\':\\n                return (\\n                {\"error\": \"Authorization header must start with Bearer\"},\\n                HTTPStatus.UNAUTHORIZED,\\n            )\\n\\n        elif api_key:\\n            x_api_key = request.headers[\\'x-api-key\\']\\n            if not x_api_key:\\n                abort(401, \\'x-api-key header missing\\')\\n\\n        else:\\n            return (\\n                {\"error\": \"Missing Authorization or x-api-key in header\"},\\n                HTTPStatus.UNAUTHORIZED,\\n            )\\n\\n        return func(*args, **kwargs)\\n\\n    return decorated_function\\n\\ndef jwt_token_required(func):\\n    @wraps(func)\\n    def decorated_function(*args, **kwargs):\\n        auth_headers = request.headers.get(\\'Authorization\\', \\'\\').split()\\n        if auth_headers:\\n            if len(auth_headers) != 2:\\n                return (\\n                {\"error\": \"Invalid Authorization header\"},\\n                HTTPStatus.UNAUTHORIZED,\\n            )\\n            token_type, token = auth_headers\\n            if token_type != \\'Bearer\\':\\n                return (\\n                {\"error\": \"Authorization header must start with Bearer\"},\\n                HTTPStatus.UNAUTHORIZED,\\n            )\\n        else:\\n            return (\\n                {\"error\": \"Missing Authorization in request header\"},\\n                HTTPStatus.UNAUTHORIZED,\\n            )\\n        return func(*args, **kwargs)\\n\\n    return decorated_function\\n\\n\\ndef get_user_id():\\n    if request.headers.get(\"authorization\"):\\n        api_token = get_access_token()\\n\\n    elif request.headers.get(\"x-api-key\"):\\n        api_token = get_api_token()\\n\\n    else:\\n        abort(401, \"Missing Authorization or x-api-key in header\")\\n\\n    if api_token:\\n        user = get_key_value_hash(api_token)\\n        if not user:\\n            user_details = authenticate_user()\\n            if user_details is not None:\\n                #set_key_value(api_token, user_details)\\n                return user_details.get(\\'uuid\\')\\n            else:\\n                abort(401, \"Unknown user\")\\n        else:\\n            return user[\\'uuid\\']\\n    else:\\n        abort(401, \"Missing Authorization or x-api-key in header\")\\n\\n\\ndef is_admin_user(request):\\n    admin_user_id = environ.get(\"WHITELIST_USER_ID\")\\n    if request.headers.get(\"authorization\"):\\n        token = get_access_token()\\n        user = get_key_value_hash(token)\\n        if not user:\\n            user = authenticate_user()\\n            if user is not None:\\n                set_key_value_hash(token, user)\\n                decoded_token = jwt.decode(token, options={\"verify_signature\": False})\\n                access_token_expiry = decoded_token.get(\"exp\")\\n                expiry = timedelta.total_seconds(datetime.utcfromtimestamp(access_token_expiry)-datetime.utcnow())\\n                set_key_expiry(token, int(expiry))\\n            else:\\n                abort(401)\\n    elif request.headers.get(\"x-api-key\"):\\n        user = get_key_value_hash(request.headers.get(\"x-api-key\"))\\n        if not user:\\n            user = authenticate_user()\\n            if user is not None:\\n                set_key_value_hash(request.headers.get(\"x-api-key\"), user)\\n                expiry_str = user[\\'expiry\\']\\n                expiry = datetime.strptime(expiry_str, \\'%Y-%m-%d %H:%M:%S.%f\\')\\n                expiry_seconds = int(expiry.timestamp())\\n                expiry_token = timedelta.total_seconds(datetime.utcfromtimestamp(expiry_seconds)-datetime.utcnow())\\n                set_key_expiry(request.headers.get(\"x-api-key\"), int(expiry_token))\\n            else:\\n                abort(401)\\n    user_id = user[\"uuid\"]\\n    if user_id in admin_user_id:\\n        return True\\n    else:\\n        return False',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/utils/security.py'},\n",
       " {'name': 'get_access_token',\n",
       "  'summary': 'Extracts access token from HTTP request headers, query parameters, or JSON body in a Flask application.',\n",
       "  'type': 'function',\n",
       "  'code': 'def get_access_token():\\n    \"\"\"\\n    Looks for token in Headers, Query string and Post body\\n    :return: token extracted from request headers\\n    \"\"\"\\n    auth_header = request.headers.get(\"authorization\")\\n    if auth_header and \"Bearer\" in auth_header:\\n        _, token = auth_header.split(\" \", 1)\\n        return token\\n\\n    if request.args.get(\"token\"):\\n        return request.args.get(\"token\")\\n\\n    return request.get_json().get(\"token\")\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/utils/security.py',\n",
       "  'dependent_functions': []},\n",
       " {'name': 'get_api_token',\n",
       "  'summary': 'Extracts API token from request headers, query string, or POST body in that order; returns the token if found.',\n",
       "  'type': 'function',\n",
       "  'code': 'def get_api_token():\\n    \"\"\"\\n    Looks for api token in Headers,  Query string and Post body\\n    :return: token extracted from request headers\\n    \"\"\"\\n    token = request.headers.get(\"x-api-key\")\\n    if token:\\n        return token\\n\\n    if request.args.get(\"token\"):\\n        return request.args.get(\"token\")\\n\\n    return request.get_json().get(\"token\")\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/utils/security.py',\n",
       "  'dependent_functions': []},\n",
       " {'name': 'authenticate_user',\n",
       "  'summary': \"Authenticates user using JWT or API key from headers, fetches user details from the Segmind's Refine backend, validates response, removes unnecessary fields, and returns user details.\",\n",
       "  'type': 'function',\n",
       "  'code': 'def authenticate_user():\\n    \"\"\"\\n    Authenticate user from Segmind\\'s Refine backend.\\n    :return: user details\\n    \"\"\"\\n    if request.headers.get(\"authorization\"):\\n        jwt_token = get_access_token()\\n        headers = {\"authorization\": f\"Bearer {jwt_token}\"}\\n\\n    elif request.headers.get(\"x-api-key\"):\\n        api_token = get_api_token()\\n        headers = {\"x-api-key\": f\"{api_token}\"}\\n\\n    response = requests.get(url=environ.get(\\'REFINE_AUTH_URL\\'), headers=headers)\\n\\n    if not response.status_code == 200:\\n        return None\\n\\n    if \"uuid\" not in response.json():\\n        return None\\n\\n    if \"email\" not in response.json():\\n        return None\\n\\n    if request.headers.get(\"x-api-key\"):\\n        if \"expiry\" not in response.json():\\n            return None\\n\\n    user_details = response.json()\\n    if \\'credits\\' in user_details:\\n        user_details[\"credits\"] = round(float(user_details[\"credits\"]))\\n\\n    user_details.pop(\\'cluster\\', None)\\n    user_details.pop(\\'role_in_org\\', None)\\n    user_details.pop(\\'organization\\', None)\\n\\n    return user_details\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/utils/security.py',\n",
       "  'dependent_functions': ['get_access_token',\n",
       "   'round',\n",
       "   'get_api_token',\n",
       "   'float']},\n",
       " {'name': 'jwt_or_api_token_required',\n",
       "  'summary': \"Decorator that ensures an HTTP request contains either a JWT (Bearer token) in the Authorization header or an 'x-api-key' header, but not both, and returns appropriate error messages for violations.\",\n",
       "  'type': 'function',\n",
       "  'code': 'def jwt_or_api_token_required(func):\\n    @wraps(func)\\n    def decorated_function(*args, **kwargs):\\n        auth_headers = request.headers.get(\\'Authorization\\', \\'\\').split()\\n        api_key = request.headers.get(\\'x-api-key\\')\\n\\n        if api_key and auth_headers:\\n            return (\\n                {\"error\": \"Both Authorizationn and x-api-key can not be present\"},\\n                HTTPStatus.UNAUTHORIZED,\\n            )\\n\\n        if auth_headers:\\n            if len(auth_headers) != 2:\\n                return (\\n                {\"error\": \"Invalid Authorization header\"},\\n                HTTPStatus.UNAUTHORIZED,\\n            )\\n\\n            token_type, token = auth_headers\\n\\n            if token_type != \\'Bearer\\':\\n                return (\\n                {\"error\": \"Authorization header must start with Bearer\"},\\n                HTTPStatus.UNAUTHORIZED,\\n            )\\n\\n        elif api_key:\\n            x_api_key = request.headers[\\'x-api-key\\']\\n            if not x_api_key:\\n                abort(401, \\'x-api-key header missing\\')\\n\\n        else:\\n            return (\\n                {\"error\": \"Missing Authorization or x-api-key in header\"},\\n                HTTPStatus.UNAUTHORIZED,\\n            )\\n\\n        return func(*args, **kwargs)\\n\\n    return decorated_function\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/utils/security.py',\n",
       "  'dependent_functions': ['wraps', 'func', 'len', 'abort']},\n",
       " {'name': 'jwt_token_required',\n",
       "  'summary': \"Decorator function 'jwt_token_required' validates the presence and format of a 'Bearer' JWT token in the request's 'Authorization' header before executing the wrapped function. Returns error messages and HTTP 401 status on validation failures.\",\n",
       "  'type': 'function',\n",
       "  'code': 'def jwt_token_required(func):\\n    @wraps(func)\\n    def decorated_function(*args, **kwargs):\\n        auth_headers = request.headers.get(\\'Authorization\\', \\'\\').split()\\n        if auth_headers:\\n            if len(auth_headers) != 2:\\n                return (\\n                {\"error\": \"Invalid Authorization header\"},\\n                HTTPStatus.UNAUTHORIZED,\\n            )\\n            token_type, token = auth_headers\\n            if token_type != \\'Bearer\\':\\n                return (\\n                {\"error\": \"Authorization header must start with Bearer\"},\\n                HTTPStatus.UNAUTHORIZED,\\n            )\\n        else:\\n            return (\\n                {\"error\": \"Missing Authorization in request header\"},\\n                HTTPStatus.UNAUTHORIZED,\\n            )\\n        return func(*args, **kwargs)\\n\\n    return decorated_function\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/utils/security.py',\n",
       "  'dependent_functions': ['wraps', 'func', 'len']},\n",
       " {'name': 'get_user_id',\n",
       "  'summary': '\\nThe function `get_user_id()` retrieves a user ID by first checking for \"authorization\" or \"x-api-key\" in the request headers, then obtaining the corresponding token. It validates the token, retrieves user details, and returns the user\\'s UUID. Fails with a 401 error if token or user details are invalid or missing.\\n',\n",
       "  'type': 'function',\n",
       "  'code': 'def get_user_id():\\n    if request.headers.get(\"authorization\"):\\n        api_token = get_access_token()\\n\\n    elif request.headers.get(\"x-api-key\"):\\n        api_token = get_api_token()\\n\\n    else:\\n        abort(401, \"Missing Authorization or x-api-key in header\")\\n\\n    if api_token:\\n        user = get_key_value_hash(api_token)\\n        if not user:\\n            user_details = authenticate_user()\\n            if user_details is not None:\\n                #set_key_value(api_token, user_details)\\n                return user_details.get(\\'uuid\\')\\n            else:\\n                abort(401, \"Unknown user\")\\n        else:\\n            return user[\\'uuid\\']\\n    else:\\n        abort(401, \"Missing Authorization or x-api-key in header\")\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/utils/security.py',\n",
       "  'dependent_functions': ['get_access_token',\n",
       "   'get_key_value_hash',\n",
       "   'abort',\n",
       "   'get_api_token',\n",
       "   'abort',\n",
       "   'authenticate_user',\n",
       "   'abort']},\n",
       " {'name': 'is_admin_user',\n",
       "  'summary': 'Checks if the user is an admin by validating authorization headers or x-api-key, decoding tokens, verifying stored user information, and comparing user_id with whitelisted admin IDs.',\n",
       "  'type': 'function',\n",
       "  'code': 'def is_admin_user(request):\\n    admin_user_id = environ.get(\"WHITELIST_USER_ID\")\\n    if request.headers.get(\"authorization\"):\\n        token = get_access_token()\\n        user = get_key_value_hash(token)\\n        if not user:\\n            user = authenticate_user()\\n            if user is not None:\\n                set_key_value_hash(token, user)\\n                decoded_token = jwt.decode(token, options={\"verify_signature\": False})\\n                access_token_expiry = decoded_token.get(\"exp\")\\n                expiry = timedelta.total_seconds(datetime.utcfromtimestamp(access_token_expiry)-datetime.utcnow())\\n                set_key_expiry(token, int(expiry))\\n            else:\\n                abort(401)\\n    elif request.headers.get(\"x-api-key\"):\\n        user = get_key_value_hash(request.headers.get(\"x-api-key\"))\\n        if not user:\\n            user = authenticate_user()\\n            if user is not None:\\n                set_key_value_hash(request.headers.get(\"x-api-key\"), user)\\n                expiry_str = user[\\'expiry\\']\\n                expiry = datetime.strptime(expiry_str, \\'%Y-%m-%d %H:%M:%S.%f\\')\\n                expiry_seconds = int(expiry.timestamp())\\n                expiry_token = timedelta.total_seconds(datetime.utcfromtimestamp(expiry_seconds)-datetime.utcnow())\\n                set_key_expiry(request.headers.get(\"x-api-key\"), int(expiry_token))\\n            else:\\n                abort(401)\\n    user_id = user[\"uuid\"]\\n    if user_id in admin_user_id:\\n        return True\\n    else:\\n        return False',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/utils/security.py',\n",
       "  'dependent_functions': ['get_access_token',\n",
       "   'get_key_value_hash',\n",
       "   'authenticate_user',\n",
       "   'get_key_value_hash',\n",
       "   'set_key_value_hash',\n",
       "   'set_key_expiry',\n",
       "   'abort',\n",
       "   'authenticate_user',\n",
       "   'int',\n",
       "   'set_key_value_hash',\n",
       "   'int',\n",
       "   'set_key_expiry',\n",
       "   'abort',\n",
       "   'int']},\n",
       " {'name': 'decorated_function',\n",
       "  'summary': \"Validates request headers by checking for either 'Authorization' (must be 'Bearer' type) or 'x-api-key'. Returns various error messages with HTTP 401 status if conditions are not met. If validations pass, it invokes the `func(*args, **kwargs)`.\",\n",
       "  'type': 'function',\n",
       "  'code': '    def decorated_function(*args, **kwargs):\\n        auth_headers = request.headers.get(\\'Authorization\\', \\'\\').split()\\n        api_key = request.headers.get(\\'x-api-key\\')\\n\\n        if api_key and auth_headers:\\n            return (\\n                {\"error\": \"Both Authorizationn and x-api-key can not be present\"},\\n                HTTPStatus.UNAUTHORIZED,\\n            )\\n\\n        if auth_headers:\\n            if len(auth_headers) != 2:\\n                return (\\n                {\"error\": \"Invalid Authorization header\"},\\n                HTTPStatus.UNAUTHORIZED,\\n            )\\n\\n            token_type, token = auth_headers\\n\\n            if token_type != \\'Bearer\\':\\n                return (\\n                {\"error\": \"Authorization header must start with Bearer\"},\\n                HTTPStatus.UNAUTHORIZED,\\n            )\\n\\n        elif api_key:\\n            x_api_key = request.headers[\\'x-api-key\\']\\n            if not x_api_key:\\n                abort(401, \\'x-api-key header missing\\')\\n\\n        else:\\n            return (\\n                {\"error\": \"Missing Authorization or x-api-key in header\"},\\n                HTTPStatus.UNAUTHORIZED,\\n            )\\n\\n        return func(*args, **kwargs)\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/utils/security.py',\n",
       "  'dependent_functions': ['wraps', 'func', 'len', 'abort']},\n",
       " {'name': 'decorated_function',\n",
       "  'summary': \"Checks 'Authorization' header for a valid 'Bearer' token; returns error with HTTP 401 if invalid or missing, otherwise calls `func` with the given arguments.\",\n",
       "  'type': 'function',\n",
       "  'code': '    def decorated_function(*args, **kwargs):\\n        auth_headers = request.headers.get(\\'Authorization\\', \\'\\').split()\\n        if auth_headers:\\n            if len(auth_headers) != 2:\\n                return (\\n                {\"error\": \"Invalid Authorization header\"},\\n                HTTPStatus.UNAUTHORIZED,\\n            )\\n            token_type, token = auth_headers\\n            if token_type != \\'Bearer\\':\\n                return (\\n                {\"error\": \"Authorization header must start with Bearer\"},\\n                HTTPStatus.UNAUTHORIZED,\\n            )\\n        else:\\n            return (\\n                {\"error\": \"Missing Authorization in request header\"},\\n                HTTPStatus.UNAUTHORIZED,\\n            )\\n        return func(*args, **kwargs)\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/utils/security.py',\n",
       "  'dependent_functions': ['wraps', 'func', 'len']},\n",
       " {'name': 'queue_utils.py',\n",
       "  'summary': \"\\nThe provided code sets up a script to communicate with an Amazon SQS (Simple Queue Service) queue using Boto3, the AWS SDK for Python. Key configurations such as `INFERENCE_QUEUE_URL`, `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, and `REGION_NAME` are imported from a `BaseConfig` class in the `app.config` module.\\n\\n1. **Initialization**:\\n    - The SQS client is created using the provided AWS credentials and region.\\n\\n2. **Functions**:\\n    - `receive_message(sqs=sqs, queue_url=queue_url)`: \\n        - This function receives a message from the specified SQS queue.\\n        - The `receive_message` method of the SQS client is called with parameters to retrieve one message, including its attributes and setting a visibility timeout.\\n        - If a message is received, it is returned; otherwise, `None` is returned.\\n    - `delete_message(message, sqs=sqs, queue_url=queue_url)`:\\n        - This function deletes a message from the SQS queue.\\n        - It uses the message's receipt handle to delete the specific message via the `delete_message` method of the SQS client.\\n\\nThese two functions could be used to implement message processing from an SQS queue, handling both retrieval and deletion of messages.\\n\",\n",
       "  'type': 'file',\n",
       "  'code': \"import boto3\\nfrom app.config import BaseConfig\\n\\nconfig = BaseConfig()\\n\\nINFERENCE_QUEUE_URL = config.INFERENCE_QUEUE_URL\\nAWS_ACCESS_KEY_ID = config.AWS_ACCESS_KEY_ID\\nAWS_SECRET_ACCESS_KEY = config.AWS_SECRET_ACCESS_KEY\\nREGION_NAME = config.REGION_NAME\\n\\n# Create SQS client\\nsqs = boto3.client('sqs',\\n                    aws_access_key_id=AWS_ACCESS_KEY_ID, \\n                    aws_secret_access_key=AWS_SECRET_ACCESS_KEY, \\n                    region_name=REGION_NAME)\\n\\nqueue_url = INFERENCE_QUEUE_URL\\n\\n\\ndef receive_message(sqs=sqs,queue_url=queue_url):\\n\\n    # Receive message from SQS queue\\n    response = sqs.receive_message(\\n        QueueUrl=queue_url,\\n        AttributeNames=[\\n            'SentTimestamp'\\n        ],\\n        MaxNumberOfMessages=1,\\n        MessageAttributeNames=[\\n            'All'\\n        ],\\n        VisibilityTimeout=300,\\n        WaitTimeSeconds=0\\n    )\\n\\n    # print(response)\\n    if 'Messages' in response and len(response['Messages']) > 0:\\n        message = response['Messages'][0]\\n        return message\\n    else:\\n        return\\n\\ndef delete_message(message,sqs=sqs,queue_url=queue_url):\\n    receipt_handle = message['ReceiptHandle']\\n\\n    # Delete received message from queue\\n    sqs.delete_message(\\n        QueueUrl=queue_url,\\n        ReceiptHandle=receipt_handle\\n    )\\n           \\n\\n\",\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/utils/queue_utils.py'},\n",
       " {'name': 'receive_message',\n",
       "  'summary': 'Receives a single message from an SQS queue, retrieves and returns the message if available, otherwise returns None.',\n",
       "  'type': 'function',\n",
       "  'code': \"def receive_message(sqs=sqs,queue_url=queue_url):\\n\\n    # Receive message from SQS queue\\n    response = sqs.receive_message(\\n        QueueUrl=queue_url,\\n        AttributeNames=[\\n            'SentTimestamp'\\n        ],\\n        MaxNumberOfMessages=1,\\n        MessageAttributeNames=[\\n            'All'\\n        ],\\n        VisibilityTimeout=300,\\n        WaitTimeSeconds=0\\n    )\\n\\n    # print(response)\\n    if 'Messages' in response and len(response['Messages']) > 0:\\n        message = response['Messages'][0]\\n        return message\\n    else:\\n        return\\n\",\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/utils/queue_utils.py',\n",
       "  'dependent_functions': ['len']},\n",
       " {'name': 'delete_message',\n",
       "  'summary': \"This function deletes a specified message from an SQS queue using the message's ReceiptHandle, the `sqs` client, and the provided `queue_url`.\",\n",
       "  'type': 'function',\n",
       "  'code': \"def delete_message(message,sqs=sqs,queue_url=queue_url):\\n    receipt_handle = message['ReceiptHandle']\\n\\n    # Delete received message from queue\\n    sqs.delete_message(\\n        QueueUrl=queue_url,\\n        ReceiptHandle=receipt_handle\\n    )\\n\",\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/utils/queue_utils.py',\n",
       "  'dependent_functions': []},\n",
       " {'name': 'prometheus_utils.py',\n",
       "  'summary': \"\\nThe code establishes a connection to a Prometheus server using credentials retrieved from a configuration file (`BaseConfig`). It uses the `PrometheusConnect` class from the `prometheus_api_client` package and constructs a Basic Authentication header using base64-encoded credentials. The `query_range` function is defined to perform a custom query to the Prometheus server over a specified time range. This function takes four parameters: `query` (the PromQL query), `start_time`, `end_time`, and `step` (the query resolution step width). If the query is defined, the function uses the Prometheus client's `custom_query_range` method to fetch and return the data; otherwise, it returns an error message indicating that the query is not defined. This setup allows for querying time series data from Prometheus with specified parameters.\\n\",\n",
       "  'type': 'file',\n",
       "  'code': 'from app.config import BaseConfig\\nfrom prometheus_api_client import PrometheusConnect\\nimport datetime\\nimport base64\\n\\nconfig = BaseConfig()\\n\\n# Prometheus server URL\\nprometheus_url = config.PROMETHEUS_URL\\n\\n# Prometheus server username and password\\nusername = config.PROMETHEUS_USERNAME\\npassword = config.PROMETHEUS_PASSWORD\\n\\ncredentials = f\"{username}:{password}\"\\nencode_credentials_bytes = credentials.encode(\\'ascii\\')\\nbase64_bytes = base64.b64encode(encode_credentials_bytes)\\ncredentials_base64_string = base64_bytes.decode(\\'ascii\\')\\n\\n# Create a Prometheus client\\nprometheus = PrometheusConnect(url=prometheus_url, headers={\\n                               \\'Authorization\\': f\\'Basic {credentials_base64_string}\\'})\\n\\nprint(\"Connected to Prometheus server.\")\\n\\n\\ndef query_range(query, start_time, end_time, step):\\n    if query:\\n        data = prometheus.custom_query_range(\\n            query=query, start_time=start_time, end_time=end_time, step=step)\\n        return data\\n    else:\\n        return \\'Query is not defined\\'\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/utils/prometheus_utils.py'},\n",
       " {'name': 'query_range',\n",
       "  'summary': \"Function `query_range` executes a Prometheus custom query within a specified time range and step interval, returning the data if a query is defined; otherwise, it returns an error message 'Query is not defined'.\",\n",
       "  'type': 'function',\n",
       "  'code': \"def query_range(query, start_time, end_time, step):\\n    if query:\\n        data = prometheus.custom_query_range(\\n            query=query, start_time=start_time, end_time=end_time, step=step)\\n        return data\\n    else:\\n        return 'Query is not defined'\\n\",\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/utils/prometheus_utils.py',\n",
       "  'dependent_functions': []},\n",
       " {'name': 'string_utils.py',\n",
       "  'summary': \"\\nThe provided code includes utility functions and a function to extract the hostname from a given URL. It uses the `urlparse` function from the `urllib.parse` module to parse URLs.\\n\\n1. **isBlank(myString)**: A utility function that returns `True` if the input string `myString` is either empty, `None`, or contains only whitespace characters. It uses `not` combined with the `strip` method to determine this.\\n\\n2. **isNotBlank(myString)**: A utility function that returns `True` if the input string `myString` is non-empty and contains non-whitespace characters. It returns the boolean value of the combined checks.\\n\\n3. **extractHostname(url)**: A function that parses a URL and extracts its hostname.\\n   - It first parses the URL using `urlparse(url)`.\\n   - If a hostname is found (`parsed_url.hostname`), it checks if the hostname represents an IP address by removing dots and checking if the remaining characters are all digits.\\n   - If not an IP address, it splits the hostname into parts using the dot (`.`) separator.\\n   - If the first part of the hostname (assumed as a subdomain) contains a hyphen (`-`), it removes the segment after the hyphen to return the initial segment.\\n   - The function returns a tuple containing the extracted hostname (or subdomain) and a boolean indicating whether it's an IP address.\\n\",\n",
       "  'type': 'file',\n",
       "  'code': \"from urllib.parse import urlparse\\n\\ndef isBlank (myString):\\n    return not (myString and myString.strip())\\n\\ndef isNotBlank (myString):\\n    return bool(myString and myString.strip())\\n\\ndef extractHostname(url):\\n    parsed_url = urlparse(url)\\n    if parsed_url.hostname:\\n        # Check if the hostname is an IP address\\n        is_ip = False\\n        if parsed_url.hostname.replace('.', '').isdigit():\\n            is_ip = True\\n            return parsed_url.hostname, is_ip\\n        else:\\n            # Extracting the subdomain or domain\\n            parts = parsed_url.hostname.split('.')\\n            if len(parts) >= 2:\\n                hostname = parts[0]\\n                # Remove '-port' if it exists\\n                if '-' in hostname:\\n                    hostname = hostname.split('-')[0]\\n                return hostname, is_ip\\n    return None, None\",\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/utils/string_utils.py'},\n",
       " {'name': 'isBlank',\n",
       "  'summary': '\\nThis function `isBlank` checks if a given string `myString` is empty or contains only whitespace characters. It returns `True` if the string is empty or just whitespace, and `False` otherwise.\\n',\n",
       "  'type': 'function',\n",
       "  'code': 'def isBlank (myString):\\n    return not (myString and myString.strip())\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/utils/string_utils.py',\n",
       "  'dependent_functions': []},\n",
       " {'name': 'isNotBlank',\n",
       "  'summary': 'This function, `isNotBlank`, checks if a given string is not empty or only whitespace. It returns `True` if the string contains non-whitespace characters, and `False` otherwise.',\n",
       "  'type': 'function',\n",
       "  'code': 'def isNotBlank (myString):\\n    return bool(myString and myString.strip())\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/utils/string_utils.py',\n",
       "  'dependent_functions': ['bool']},\n",
       " {'name': 'extractHostname',\n",
       "  'summary': \"\\nThis function `extractHostname(url)` parses a URL to extract the hostname. It determines if the hostname is an IP address, returning it directly if true. Otherwise, it splits the hostname to identify the subdomain or domain and removes any '-port' if appended. Returns a tuple `(hostname, is_ip)`.\\n\",\n",
       "  'type': 'function',\n",
       "  'code': \"def extractHostname(url):\\n    parsed_url = urlparse(url)\\n    if parsed_url.hostname:\\n        # Check if the hostname is an IP address\\n        is_ip = False\\n        if parsed_url.hostname.replace('.', '').isdigit():\\n            is_ip = True\\n            return parsed_url.hostname, is_ip\\n        else:\\n            # Extracting the subdomain or domain\\n            parts = parsed_url.hostname.split('.')\\n            if len(parts) >= 2:\\n                hostname = parts[0]\\n                # Remove '-port' if it exists\\n                if '-' in hostname:\\n                    hostname = hostname.split('-')[0]\\n                return hostname, is_ip\\n    return None, None\",\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/utils/string_utils.py',\n",
       "  'dependent_functions': ['urlparse', 'len']},\n",
       " {'name': 'redis_utils.py',\n",
       "  'summary': \"\\nThe code establishes a connection to a Redis database using the `redis` and `StrictRedis` libraries based on configurations from `BaseConfig`. It defines several functions to interact with Redis:\\n\\n1. `set_mapping_value_list(key, value)`: Appends a value to the list stored at the specified key.\\n2. `get_values_list(key)`: Retrieves all values from the list stored at the specified key.\\n3. `get_key_value_hash(key)`: Retrieves and decodes a hash stored at the specified key.\\n4. `set_key_value_hash(key, value)`: Sets multiple fields in a hash stored at the specified key.\\n5. `remove_values_list(key, value, count)`: Removes elements matching the value from the list stored at the specified key.\\n6. `set_key_expiry(key, expiry)`: Sets an expiry time for the specified key.\\n7. `get_mapping_keyname(key)`: Returns a formatted string for a mapping key.\\n8. `get_all_mappings_keys()`: Retrieves all keys matching the pattern 'mappings/*'.\\n9. `delete_key(key)`: Deletes the specified key.\\n\\nThe Redis connection uses `url` from the config for the host and defaults to port 6379 and database 0.\\n\",\n",
       "  'type': 'file',\n",
       "  'code': 'import redis\\nfrom app.config import BaseConfig\\nfrom redis import StrictRedis\\nfrom flask import current_app as app\\n\\nconfig = BaseConfig()\\n\\nurl = config.REDIS_URL\\n\\nr = redis.Redis(host = url , port = 6379, db = 0)\\n\\ndef set_mapping_value_list(key,value):\\n    r.rpush(key,value)\\n\\ndef get_values_list(key):\\n    redis_dict= r.lrange(key, 0, -1)\\n    return redis_dict\\n\\ndef get_key_value_hash(key):\\n    redis_dict = r.hgetall(key)\\n    decoded_dict = {key.decode() : value.decode() for key, value in redis_dict.items()}\\n    return decoded_dict\\n\\ndef set_key_value_hash(key, value):\\n    r.hmset(key, value)\\n\\ndef remove_values_list(key, value, count):\\n    r.lrem(key,count,value)\\n\\ndef set_key_expiry(key, expiry):\\n    r.expire(key, expiry)\\n\\ndef get_mapping_keyname(key):\\n    return f\"mappings/{key}\"\\n\\ndef get_all_mappings_keys():\\n    return r.keys(pattern = \\'mappings/*\\')\\n\\ndef delete_key(key):\\n    r.delete(key)\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/utils/redis_utils.py'},\n",
       " {'name': 'set_mapping_value_list',\n",
       "  'summary': 'Appends a value to a list stored at the given key in a Redis database using the rpush command.',\n",
       "  'type': 'function',\n",
       "  'code': 'def set_mapping_value_list(key,value):\\n    r.rpush(key,value)\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/utils/redis_utils.py',\n",
       "  'dependent_functions': []},\n",
       " {'name': 'get_values_list',\n",
       "  'summary': 'Fetches a list of all values associated with a Redis list key using the `lrange` method, returning elements from start to end of the list.',\n",
       "  'type': 'function',\n",
       "  'code': 'def get_values_list(key):\\n    redis_dict= r.lrange(key, 0, -1)\\n    return redis_dict\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/utils/redis_utils.py',\n",
       "  'dependent_functions': []},\n",
       " {'name': 'get_key_value_hash',\n",
       "  'summary': \"Fetches all key-value pairs from a Redis hash specified by 'key', decodes them from bytes to string, and returns a decoded dictionary.\",\n",
       "  'type': 'function',\n",
       "  'code': 'def get_key_value_hash(key):\\n    redis_dict = r.hgetall(key)\\n    decoded_dict = {key.decode() : value.decode() for key, value in redis_dict.items()}\\n    return decoded_dict\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/utils/redis_utils.py',\n",
       "  'dependent_functions': []},\n",
       " {'name': 'set_key_value_hash',\n",
       "  'summary': 'Sets multiple key-value pairs in a Redis hash using the `hmset` method.',\n",
       "  'type': 'function',\n",
       "  'code': 'def set_key_value_hash(key, value):\\n    r.hmset(key, value)\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/utils/redis_utils.py',\n",
       "  'dependent_functions': []},\n",
       " {'name': 'remove_values_list',\n",
       "  'summary': \"This Python function uses Redis' LREM command to remove a specified number of occurrences (`count`) of a given value (`value`) from a Redis list identified by `key`.\",\n",
       "  'type': 'function',\n",
       "  'code': 'def remove_values_list(key, value, count):\\n    r.lrem(key,count,value)\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/utils/redis_utils.py',\n",
       "  'dependent_functions': []},\n",
       " {'name': 'set_key_expiry',\n",
       "  'summary': \"Sets an expiration time on a given key using Redis's `expire` method.\",\n",
       "  'type': 'function',\n",
       "  'code': 'def set_key_expiry(key, expiry):\\n    r.expire(key, expiry)\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/utils/redis_utils.py',\n",
       "  'dependent_functions': []},\n",
       " {'name': 'get_mapping_keyname',\n",
       "  'summary': 'Defines a function `get_mapping_keyname` that returns a formatted string in the form \"mappings/{key}\" given an input `key`.',\n",
       "  'type': 'function',\n",
       "  'code': 'def get_mapping_keyname(key):\\n    return f\"mappings/{key}\"\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/utils/redis_utils.py',\n",
       "  'dependent_functions': []},\n",
       " {'name': 'get_all_mappings_keys',\n",
       "  'summary': \"Fetches all keys from a Redis database that match the pattern 'mappings/*'\",\n",
       "  'type': 'function',\n",
       "  'code': \"def get_all_mappings_keys():\\n    return r.keys(pattern = 'mappings/*')\\n\",\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/utils/redis_utils.py',\n",
       "  'dependent_functions': []},\n",
       " {'name': 'delete_key',\n",
       "  'summary': \"Deletes a specified key from a Redis database using the 'r' Redis client instance.\",\n",
       "  'type': 'function',\n",
       "  'code': 'def delete_key(key):\\n    r.delete(key)\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/utils/redis_utils.py',\n",
       "  'dependent_functions': []},\n",
       " {'name': 'common.py',\n",
       "  'summary': \"\\nThe code imports the necessary modules and functions, namely `requests`, `extractHostname` from `app.utils.string_utils`, and `BaseConfig` from `app.config`. It initializes a `config` object using `BaseConfig`. The function `get_instance_gpu(instance)` splits an input string `instance` using `:` into an IP address and port, strips any leading/trailing whitespace, and maps specific port numbers (`8002`, `8005`, `8008`, `8011`) to their corresponding GPU IDs (`0`, `1`, `2`, `3`). It returns a list containing the IP address and the GPU ID. The second function `get_gpu_status(server_url)` uses `extractHostname` to determine if the `server_url` is an IP address or not. It constructs a URL using `config.SPOT_GPU_URL`, appending either a GPU IP address or GPU ID query parameter based on the hostname's type. It sets up a `payload` and `headers` dictionary containing a secret access token from the configuration, makes a GET request to the constructed URL using the `requests` module, and returns the HTTP status code from the response.\\n\",\n",
       "  'type': 'file',\n",
       "  'code': 'import requests\\nfrom app.utils.string_utils import extractHostname\\nfrom app.config import BaseConfig\\n\\nconfig = BaseConfig()\\n\\ndef get_instance_gpu(instance):\\n    instance_split = instance.split(\\':\\')\\n    ip_address = instance_split[0].strip()\\n    port = instance_split[1].strip()\\n    gpu_id = 0\\n    if port == \\'8002\\':\\n        gpu_id = \"0\"    \\n    if port == \\'8005\\':\\n        gpu_id = \"1\" \\n    if port == \\'8008\\':\\n        gpu_id = \"2\"    \\n    if port == \\'8011\\':\\n        gpu_id = \"3\" \\n    return [ip_address, gpu_id]\\n\\ndef get_gpu_status(server_url):\\n    hostname, is_ip = extractHostname(server_url)\\n    url = config.SPOT_GPU_URL\\n    if is_ip:\\n        url = f\\'{url}?gpu_ip_address={hostname}\\'\\n    else:\\n        url = f\\'{url}?gpu_id={hostname}\\'\\n    payload = {}\\n    headers = { \\'secret\\': config.SPOT_GPU_ACCESS_TOKEN }\\n    response = requests.request(\"GET\", url, headers=headers, data=payload)\\n    return response.status_code',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/utils/common.py'},\n",
       " {'name': 'get_instance_gpu',\n",
       "  'summary': 'Parses an instance string in the format \"ip:port\", extracts the IP address and port, assigns a GPU ID based on the port (8002:0, 8005:1, 8008:2, 8011:3), and returns them as a list [ip_address, gpu_id].',\n",
       "  'type': 'function',\n",
       "  'code': 'def get_instance_gpu(instance):\\n    instance_split = instance.split(\\':\\')\\n    ip_address = instance_split[0].strip()\\n    port = instance_split[1].strip()\\n    gpu_id = 0\\n    if port == \\'8002\\':\\n        gpu_id = \"0\"    \\n    if port == \\'8005\\':\\n        gpu_id = \"1\" \\n    if port == \\'8008\\':\\n        gpu_id = \"2\"    \\n    if port == \\'8011\\':\\n        gpu_id = \"3\" \\n    return [ip_address, gpu_id]\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/utils/common.py',\n",
       "  'dependent_functions': []},\n",
       " {'name': 'get_gpu_status',\n",
       "  'summary': 'Defines `get_gpu_status(server_url)` to fetch GPU status from a given server URL, constructing the request URL based on whether the hostname is an IP address. It sends a GET request with an authorization header and returns the HTTP status code of the response.',\n",
       "  'type': 'function',\n",
       "  'code': 'def get_gpu_status(server_url):\\n    hostname, is_ip = extractHostname(server_url)\\n    url = config.SPOT_GPU_URL\\n    if is_ip:\\n        url = f\\'{url}?gpu_ip_address={hostname}\\'\\n    else:\\n        url = f\\'{url}?gpu_id={hostname}\\'\\n    payload = {}\\n    headers = { \\'secret\\': config.SPOT_GPU_ACCESS_TOKEN }\\n    response = requests.request(\"GET\", url, headers=headers, data=payload)\\n    return response.status_code',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/utils/common.py',\n",
       "  'dependent_functions': ['extractHostname']},\n",
       " {'name': '__init__.py',\n",
       "  'summary': 'Sure, please provide the code you would like me to summarize.',\n",
       "  'type': 'file',\n",
       "  'code': '',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/tasks/__init__.py'},\n",
       " {'name': 'putModel.py',\n",
       "  'summary': '\\nThe provided code imports various utilities and configuration data needed for managing GPU resource allocation and model deployment. The script defines a VRAM threshold and initializes a configuration instance. The `allocate_gpu` function is responsible for determining the GPU servers that have available VRAM above the specified threshold. However, the code that dynamically queries GPU metrics from Prometheus is commented out, and hard-coded GPU server data is used instead. The function returns a list of eligible GPU servers.\\n\\nThe `putModelOnGPU` function takes a message, parses its JSON content, determines the best GPU server(s) by calling `allocate_gpu`, and attempts to send a model loading request to the selected server(s). If successful, it updates the request status in a Redis store.\\n\\nThe `_spotStatusUpdate` function updates the status of the model deployment by sending a payload to a specified endpoint, differentiating between \"hubmodel\" or \"finetune\" request types.\\n\\nThe `putModelTask` function periodically checks for new messages, processes requests to load models onto GPUs, and updates status information accordingly. \\n\\nThe script uses the `apscheduler` library to schedule the `putModelTask` function to run at intervals of 20 seconds. The scheduling is initiated at the end of the script.\\n\\n',\n",
       "  'type': 'file',\n",
       "  'code': 'from app.config import BaseConfig\\nfrom app.utils.queue_utils import receive_message, delete_message\\nfrom app.utils.prometheus_utils import query_range\\nfrom app.utils.redis_utils import get_values_list, set_mapping_value_list, get_mapping_keyname, r\\nfrom app.utils.common import get_instance_gpu\\nimport requests\\nimport json\\nimport time\\nfrom apscheduler.schedulers.background import BackgroundScheduler\\nimport datetime\\n\\nconfig = BaseConfig()\\n\\n# Define your thresholds\\nvram_threshold = 1  # Example: 10 GB VRAM\\n\\n\\ndef allocate_gpu():\\n    # Prometheus is commented out - Hard coded GPU values\\n\\n    # # Query Prometheus for the metrics you need (replace with actual metric names)\\n    \\n    # gpu_available_vram_query = \\'(nv_gpu_memory_total_bytes{job=\"prometheus\"}/(1024*1024*1024)) - (nv_gpu_memory_used_bytes{job=\"prometheus\"}/(1024*1024*1024))\\'\\n    # start_time = datetime.datetime.now()-datetime.timedelta(minutes=1)\\n    # end_time = datetime.datetime.now()\\n    # step = \"1m\"\\n    # data = query_range(gpu_available_vram_query, start_time, end_time, step)  # Across the systems\\n    # gpu_servers = []\\n    # print(data)\\n    # for item in data:\\n    #     gpu_uuid = item[\"metric\"][\"gpu_uuid\"]\\n    #     instance = get_instance_gpu(item[\"metric\"][\"instance\"])\\n    #     values = item[\"values\"]\\n    #     vram = 0\\n    #     if(len(values) == 1):\\n    #         vram = float(values[0][1])\\n    #     else:\\n    #         vram = float(values[len(values) - 1][1])\\n    #     model_list = []\\n    #     keyname = f\"model_cold_mappings/{instance[0]}:{instance[1]}\"\\n    #     if r.exists(keyname):\\n    #         model_list = get_values_list(keyname)\\n    #     if vram > vram_threshold:\\n    #         gpu_servers.append({\\n    #             \"gpu_uuid\": gpu_uuid,\\n    #             \"instance_ip_address\": instance[0],\\n    #             \"instance_gpu_id\": instance[1],\\n    #             \"available_vram\": vram,\\n    #             \"models_allocated\": len(model_list)\\n    #         })\\n\\n    # # Select the best server (you can define your own criteria)\\n    # best_servers = sorted(\\n    #     gpu_servers, key=lambda x: (-x[\"available_vram\"], x[\"models_allocated\"]))\\n    \\n    # Prometheus is commented out code ends here - Hard coded GPU values\\n\\n    # Current Production Configuration - GCP GPUs\\n    best_servers = [{\\n        \\'gpu_uuid\\': \\'GPU-d4a9fc5e-1f35-608b-633f-9acd903dfe71\\', \\n        \\'instance_ip_address\\': \\'35.229.88.57\\', \\n        \\'instance_gpu_id\\': \\'0\\', \\n        \\'available_vram\\': 29.6181640625, \\n        \\'models_allocated\\': 25 \\n        },\\n        {\\n        \\'gpu_uuid\\': \\'GPU-d4a9fc5e-1f35-608b-633f-9acd903dfe72\\', \\n        \\'instance_ip_address\\': \\'35.229.88.57\\', \\n        \\'instance_gpu_id\\': \\'1\\', \\n        \\'available_vram\\': 29.6181640625, \\n        \\'models_allocated\\': 25 \\n        }]\\n    \\n    if best_servers:\\n        if len(best_servers) > 1:\\n            print(f\"Identified the servers, {best_servers[0]}, {best_servers[1]}\")\\n        else :\\n            print(f\"Identified the servers, {best_servers[0]}\")\\n        \\n\\n    return best_servers\\n\\n\\ndef putModelOnGPU(message):\\n    servers = allocate_gpu()\\n    print(servers)\\n    messageBody = json.loads(message[\\'Body\\'])\\n    try:\\n        for x in range(len(servers)):\\n            messageBody[\\'instance_gpu_id\\'] = servers[x][\\'instance_gpu_id\\']\\n            messageBody[\\'instance_ip_address\\'] = servers[x][\\'instance_ip_address\\']\\n            payload = json.dumps(messageBody)\\n            headers = {\\n                \\'Content-Type\\': \\'application/json\\'\\n            }\\n            url = f\"http://{servers[x][\\'instance_ip_address\\']}:{config.DEFAULT_GPU_APP_PORT}/{config.DOWNLOAD_MODEL_URL}\"\\n            \\n            response = requests.request(\\n                    \"POST\", url, headers=headers, data=payload)\\n            if response.status_code == 200:\\n                keyname = f\"model_request_status/{messageBody[\\'request_id\\']}\"\\n                r.set(keyname, \\'DOWNLOADING\\')\\n                # status = _registerEndpoint(messageBody[\\'short_code\\'], messageBody[\\'instance_ip_address\\'], messageBody[\\'instance_gpu_id\\'])\\n                # if status == 200:\\n                #     print(x, \\' - Endpoint registered successfully\\')\\n                #     keyname = f\"model_cold_mappings/{messageBody[\\'instance_ip_address\\']}:{messageBody[\\'instance_gpu_id\\']}\"\\n                #     set_mapping_value_list(keyname, messageBody[\\'model_name\\'])\\n                # else:\\n                #     raise Exception(\\'Register Endpoint unsuccessful\\')\\n            else:\\n                raise Exception(\\'Downloading model unsuccessful\\')\\n        return 200\\n    except Exception as e:\\n        print(e)\\n        return 400\\n    \\n\\n# def _registerEndpoint(model_name, instance_ip_address, instance_gpu_id):\\n#     server_endpoint = f\"http://{instance_ip_address}:{config.DEFAULT_GPU_APP_PORT}/{config.INFERENCE_GPU_URL}/{model_name}?gpu={instance_gpu_id}\"\\n#     if model_name and server_endpoint:\\n#         model_name_key = get_mapping_keyname(model_name)\\n#         r.lpush(model_name_key, server_endpoint)\\n#         return 200   \\n#     else:\\n#         return 400\\n\\ndef _spotStatusUpdate(message, status, request_type):\\n    try:\\n        payload = {       \\n            \"status\": status,\\n            \"request_id\": message[\\'request_id\\'],\\n            \"version\": \"v1\"\\n        }\\n        # if(status == \"AVAILABLE\"):\\n        #     payload[\\'segmind_model_path\\'] = f\"{config.PROXY_INFERENCE_URL}/{message[\\'short_code\\']}\"\\n        headers = {\\n            \\'Content-Type\\': \\'application/json\\',\\n            \\'secret\\': config.SPOT_SECRET_TOKEN\\n        }\\n        if request_type.lower() == \\'hubmodel\\':\\n            url = config.SPOT_HUB_STATUS_UPDATE_URL\\n        elif request_type.lower() == \\'finetune\\':\\n            url = config.SPOT_FINETUNE_STATUS_UPDATE_URL\\n        \\n        response = requests.request(\\n                \"POST\", url, headers=headers, data=json.dumps(payload))\\n        print(f\"Response - {response.text}\")\\n        return 200\\n    except Exception as e:\\n        print(e)\\n        return 400\\n\\n\\n# Main loop to periodically fetch, process, and delete messages\\ndef putModelTask():\\n    print(\\'Task Started\\')\\n    print(\\'Checking for messages\\')\\n    message = receive_message()\\n    if message:\\n        messageBody = json.loads(message[\\'Body\\'])\\n        print(f\"messageBody - {messageBody}\")\\n        request_type = messageBody[\\'request_type\\']\\n        print(f\"request_type - {request_type}\")\\n        keyname = f\"model_request_status/{messageBody[\\'request_id\\']}\"\\n        if r.exists(keyname):\\n            keyvalue = r.get(keyname)\\n            if (keyvalue == \\'AVAILABLE\\' or keyvalue == \\'DOWNLOAD_SUCCESS\\' or keyvalue == \\'DOWNLOADING\\'):\\n                    print(f\"Request {messageBody[\\'model_name\\']} under process\")\\n                    return\\n        _spotStatusUpdate(messageBody, \"DEPLOYING\", request_type)\\n        print(f\"Deploying Model {messageBody[\\'model_name\\']}\")\\n        status = putModelOnGPU(message)\\n        if status == 200:\\n            # _spotStatusUpdate(messageBody, \"AVAILABLE\", request_type)\\n            # delete_message(message)\\n            print(f\"Model Allocated {messageBody[\\'model_name\\']}\")\\n            modelKey = f\"track/{messageBody[\\'request_id\\']}\"\\n            r.set(modelKey, json.dumps(message))\\n            print(f\"{modelKey} message succesfully set in Redis\")\\n        else :\\n            _spotStatusUpdate(messageBody, \"FAILED\", request_type)\\n            print(f\"Model failed to load on GPU {messageBody[\\'model_name\\']}\")\\n\\n\\nputModelScheduler = BackgroundScheduler()\\nputModelScheduler.add_job(putModelTask, \\'interval\\', seconds=20)  # Adjust the schedule as needed\\nputModelScheduler.start()\\n\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/tasks/putModel.py'},\n",
       " {'name': 'allocate_gpu',\n",
       "  'summary': '\\nThe `allocate_gpu()` function assigns GPUs to models using pre-defined hardcoded GPU values. It includes commented-out code for querying Prometheus to dynamically get GPU metrics such as available VRAM, which is currently not used. The function ranks servers based on available VRAM and models allocated, returning the best servers for GPU allocation.\\n',\n",
       "  'type': 'function',\n",
       "  'code': 'def allocate_gpu():\\n    # Prometheus is commented out - Hard coded GPU values\\n\\n    # # Query Prometheus for the metrics you need (replace with actual metric names)\\n    \\n    # gpu_available_vram_query = \\'(nv_gpu_memory_total_bytes{job=\"prometheus\"}/(1024*1024*1024)) - (nv_gpu_memory_used_bytes{job=\"prometheus\"}/(1024*1024*1024))\\'\\n    # start_time = datetime.datetime.now()-datetime.timedelta(minutes=1)\\n    # end_time = datetime.datetime.now()\\n    # step = \"1m\"\\n    # data = query_range(gpu_available_vram_query, start_time, end_time, step)  # Across the systems\\n    # gpu_servers = []\\n    # print(data)\\n    # for item in data:\\n    #     gpu_uuid = item[\"metric\"][\"gpu_uuid\"]\\n    #     instance = get_instance_gpu(item[\"metric\"][\"instance\"])\\n    #     values = item[\"values\"]\\n    #     vram = 0\\n    #     if(len(values) == 1):\\n    #         vram = float(values[0][1])\\n    #     else:\\n    #         vram = float(values[len(values) - 1][1])\\n    #     model_list = []\\n    #     keyname = f\"model_cold_mappings/{instance[0]}:{instance[1]}\"\\n    #     if r.exists(keyname):\\n    #         model_list = get_values_list(keyname)\\n    #     if vram > vram_threshold:\\n    #         gpu_servers.append({\\n    #             \"gpu_uuid\": gpu_uuid,\\n    #             \"instance_ip_address\": instance[0],\\n    #             \"instance_gpu_id\": instance[1],\\n    #             \"available_vram\": vram,\\n    #             \"models_allocated\": len(model_list)\\n    #         })\\n\\n    # # Select the best server (you can define your own criteria)\\n    # best_servers = sorted(\\n    #     gpu_servers, key=lambda x: (-x[\"available_vram\"], x[\"models_allocated\"]))\\n    \\n    # Prometheus is commented out code ends here - Hard coded GPU values\\n\\n    # Current Production Configuration - GCP GPUs\\n    best_servers = [{\\n        \\'gpu_uuid\\': \\'GPU-d4a9fc5e-1f35-608b-633f-9acd903dfe71\\', \\n        \\'instance_ip_address\\': \\'35.229.88.57\\', \\n        \\'instance_gpu_id\\': \\'0\\', \\n        \\'available_vram\\': 29.6181640625, \\n        \\'models_allocated\\': 25 \\n        },\\n        {\\n        \\'gpu_uuid\\': \\'GPU-d4a9fc5e-1f35-608b-633f-9acd903dfe72\\', \\n        \\'instance_ip_address\\': \\'35.229.88.57\\', \\n        \\'instance_gpu_id\\': \\'1\\', \\n        \\'available_vram\\': 29.6181640625, \\n        \\'models_allocated\\': 25 \\n        }]\\n    \\n    if best_servers:\\n        if len(best_servers) > 1:\\n            print(f\"Identified the servers, {best_servers[0]}, {best_servers[1]}\")\\n        else :\\n            print(f\"Identified the servers, {best_servers[0]}\")\\n        \\n\\n    return best_servers\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/tasks/putModel.py',\n",
       "  'dependent_functions': ['len', 'print', 'print']},\n",
       " {'name': 'putModelOnGPU',\n",
       "  'summary': '\\nAllocates GPU servers and sends a model download request to each server using HTTP POST. Updates the model request status in Redis if the response status is 200. Returns 200 on success, 400 on failure.\\n',\n",
       "  'type': 'function',\n",
       "  'code': 'def putModelOnGPU(message):\\n    servers = allocate_gpu()\\n    print(servers)\\n    messageBody = json.loads(message[\\'Body\\'])\\n    try:\\n        for x in range(len(servers)):\\n            messageBody[\\'instance_gpu_id\\'] = servers[x][\\'instance_gpu_id\\']\\n            messageBody[\\'instance_ip_address\\'] = servers[x][\\'instance_ip_address\\']\\n            payload = json.dumps(messageBody)\\n            headers = {\\n                \\'Content-Type\\': \\'application/json\\'\\n            }\\n            url = f\"http://{servers[x][\\'instance_ip_address\\']}:{config.DEFAULT_GPU_APP_PORT}/{config.DOWNLOAD_MODEL_URL}\"\\n            \\n            response = requests.request(\\n                    \"POST\", url, headers=headers, data=payload)\\n            if response.status_code == 200:\\n                keyname = f\"model_request_status/{messageBody[\\'request_id\\']}\"\\n                r.set(keyname, \\'DOWNLOADING\\')\\n                # status = _registerEndpoint(messageBody[\\'short_code\\'], messageBody[\\'instance_ip_address\\'], messageBody[\\'instance_gpu_id\\'])\\n                # if status == 200:\\n                #     print(x, \\' - Endpoint registered successfully\\')\\n                #     keyname = f\"model_cold_mappings/{messageBody[\\'instance_ip_address\\']}:{messageBody[\\'instance_gpu_id\\']}\"\\n                #     set_mapping_value_list(keyname, messageBody[\\'model_name\\'])\\n                # else:\\n                #     raise Exception(\\'Register Endpoint unsuccessful\\')\\n            else:\\n                raise Exception(\\'Downloading model unsuccessful\\')\\n        return 200\\n    except Exception as e:\\n        print(e)\\n        return 400\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/tasks/putModel.py',\n",
       "  'dependent_functions': ['allocate_gpu',\n",
       "   'print',\n",
       "   'range',\n",
       "   'len',\n",
       "   'print',\n",
       "   'Exception']},\n",
       " {'name': '_spotStatusUpdate',\n",
       "  'summary': 'Function `_spotStatusUpdate` updates the status of a process by sending a POST request with relevant payload and headers to different URLs based on `request_type`. It handles exceptions and prints the response or error, returning status codes 200 or 400 accordingly.',\n",
       "  'type': 'function',\n",
       "  'code': 'def _spotStatusUpdate(message, status, request_type):\\n    try:\\n        payload = {       \\n            \"status\": status,\\n            \"request_id\": message[\\'request_id\\'],\\n            \"version\": \"v1\"\\n        }\\n        # if(status == \"AVAILABLE\"):\\n        #     payload[\\'segmind_model_path\\'] = f\"{config.PROXY_INFERENCE_URL}/{message[\\'short_code\\']}\"\\n        headers = {\\n            \\'Content-Type\\': \\'application/json\\',\\n            \\'secret\\': config.SPOT_SECRET_TOKEN\\n        }\\n        if request_type.lower() == \\'hubmodel\\':\\n            url = config.SPOT_HUB_STATUS_UPDATE_URL\\n        elif request_type.lower() == \\'finetune\\':\\n            url = config.SPOT_FINETUNE_STATUS_UPDATE_URL\\n        \\n        response = requests.request(\\n                \"POST\", url, headers=headers, data=json.dumps(payload))\\n        print(f\"Response - {response.text}\")\\n        return 200\\n    except Exception as e:\\n        print(e)\\n        return 400\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/tasks/putModel.py',\n",
       "  'dependent_functions': ['print', 'print']},\n",
       " {'name': 'putModelTask',\n",
       "  'summary': '\\nThe function `putModelTask` handles a model deployment task by checking a queue for messages, updating the model deployment status in Redis, and deploying the model on a GPU. It processes request states and updates Redis keys based on the deployment outcome.\\n',\n",
       "  'type': 'function',\n",
       "  'code': 'def putModelTask():\\n    print(\\'Task Started\\')\\n    print(\\'Checking for messages\\')\\n    message = receive_message()\\n    if message:\\n        messageBody = json.loads(message[\\'Body\\'])\\n        print(f\"messageBody - {messageBody}\")\\n        request_type = messageBody[\\'request_type\\']\\n        print(f\"request_type - {request_type}\")\\n        keyname = f\"model_request_status/{messageBody[\\'request_id\\']}\"\\n        if r.exists(keyname):\\n            keyvalue = r.get(keyname)\\n            if (keyvalue == \\'AVAILABLE\\' or keyvalue == \\'DOWNLOAD_SUCCESS\\' or keyvalue == \\'DOWNLOADING\\'):\\n                    print(f\"Request {messageBody[\\'model_name\\']} under process\")\\n                    return\\n        _spotStatusUpdate(messageBody, \"DEPLOYING\", request_type)\\n        print(f\"Deploying Model {messageBody[\\'model_name\\']}\")\\n        status = putModelOnGPU(message)\\n        if status == 200:\\n            # _spotStatusUpdate(messageBody, \"AVAILABLE\", request_type)\\n            # delete_message(message)\\n            print(f\"Model Allocated {messageBody[\\'model_name\\']}\")\\n            modelKey = f\"track/{messageBody[\\'request_id\\']}\"\\n            r.set(modelKey, json.dumps(message))\\n            print(f\"{modelKey} message succesfully set in Redis\")\\n        else :\\n            _spotStatusUpdate(messageBody, \"FAILED\", request_type)\\n            print(f\"Model failed to load on GPU {messageBody[\\'model_name\\']}\")\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/tasks/putModel.py',\n",
       "  'dependent_functions': ['print',\n",
       "   'print',\n",
       "   'receive_message',\n",
       "   'print',\n",
       "   'print',\n",
       "   '_spotStatusUpdate',\n",
       "   'print',\n",
       "   'putModelOnGPU',\n",
       "   'print',\n",
       "   'print',\n",
       "   '_spotStatusUpdate',\n",
       "   'print',\n",
       "   'print']},\n",
       " {'name': 'clearModels.py',\n",
       "  'summary': '\\nThe script performs routine maintenance tasks on machine learning model instances using Redis and Prometheus data. The key components include:\\n\\n1. **Configuration and Imports**: Imports required modules and configuration (e.g., `BaseConfig`, Redis, Prometheus, and schedulers).\\n2. **fetchRediskeys()**:\\n    - Scans Redis for all keys, filtering out specific patterns.\\n    - Processes each key to gather details like user ID, model name, and status.\\n    - Tracks inference request details per model.\\n    - Uses `ThreadPoolExecutor` for concurrent processing of keys.\\n3. **identifyCleanUpModels()**:\\n    - Queries Prometheus for machine learning model inference rates.\\n    - Correlates this data with Redis to identify models that are not being used.\\n    - Fetches detailed GPU model information from instances and records models to be cleaned if they have low inference rates.\\n4. **cleanUpModels()**:\\n    - Uses the list of models identified for cleanup.\\n    - Sends HTTP POST requests to instances to unload models no longer in use.\\n    - Catches errors and returns an appropriate status code.\\n5. **clearModelTask()**:\\n    - Calls `cleanUpModels()` and prints success or failure message.\\n6. **Scheduler**:\\n    - Uses `BackgroundScheduler` to run `clearModelTask` every 5 minutes for periodic cleanup.\\n\\nThe script is designed for scalable, periodic model management across distributed GPU instances.\\n',\n",
       "  'type': 'file',\n",
       "  'code': 'from app.config import BaseConfig\\nfrom app.utils.prometheus_utils import query_range\\nfrom app.utils.redis_utils import get_key_value_hash, r\\nfrom app.utils.common import get_instance_gpu\\nfrom concurrent.futures import ThreadPoolExecutor\\nimport requests\\nimport json\\nimport time\\nimport datetime\\nfrom apscheduler.schedulers.background import BackgroundScheduler\\n\\nconfig = BaseConfig()\\n\\ndef fetchRediskeys():\\n    print(\\'Start the celery task\\')\\n\\n    cursor = 0\\n    keys = []\\n    while True:\\n        cursor, partial_keys = r.scan(cursor, count=10000)\\n        keys.extend(partial_keys)\\n        if cursor == 0:\\n            break\\n\\n    failed_keys = r.smembers(\\'failed_keys\\')\\n\\n    inference_request_details = {}\\n\\n    def process_key(key):\\n        key = key.decode(\\'utf-8\\')\\n\\n        if key.startswith((\\'eyJ\\', \\'LIMITER\\', \\'SG\\', \\'unacked\\', \\'unacked_index\\', \\'mappings\\', \\'request_model_track\\')) or \\'celery\\' in key or \\'failed_keys\\' in key:\\n            return\\n\\n        if key in failed_keys:\\n            return\\n\\n        if \\'/\\' not in key:\\n            value = get_key_value_hash(key)\\n        else:\\n            parts = key.split(\\'/\\')\\n            if parts[0] == \\'request\\':\\n                user_id, model_name = parts[1], parts[2]\\n                values = r.hgetall(key)\\n                if values:\\n                    try:\\n                        status = values.get(b\\'status\\').decode()\\n                        start_time = values.get(b\\'start_time\\').decode()\\n                        triton_server_url = values.get(\\n                            b\\'triton_server_url\\', b\\'\\').decode()\\n\\n                    except Exception as error:\\n                        print(f\"An error occurred: {str(error)}\")\\n\\n                    time_now = datetime.now()\\n                    if start_time >= (time_now - datetime.timedelta(minutes=6)):\\n                        if model_name in inference_request_details:\\n                            inference_request_details[model_name] = inference_request_details[model_name] + 1\\n                        else:\\n                            inference_request_details[model_name] = 1\\n                    return\\n\\n    with ThreadPoolExecutor() as executor:\\n        executor.map(process_key, keys)\\n\\n    return inference_request_details\\n\\n\\ndef identifyCleanUpModels():\\n    # Query Prometheus for the metrics you need (replace with actual metric names)\\n    gpu_hotness_model_query = \\'rate(nv_inference_request_success[5m])\\'\\n    start_time = datetime.datetime.now()-datetime.timedelta(minutes=1)\\n    end_time = datetime.datetime.now()\\n    step = \"1m\"\\n    data = query_range(gpu_hotness_model_query, start_time, end_time, step)  # Across the systems\\n    models = []\\n    inference_request_details = fetchRediskeys()\\n    gpu_track = {}\\n    for item in data:\\n        instance = get_instance_gpu(item[\"metric\"][\"instance\"])\\n        model = item[\"metric\"][\"model\"]\\n        values = item[\"values\"]\\n        inferenceRate1 = float(values[0][1])\\n        inferenceRate2 = float(values[1][1])\\n        instanceID = f\"{instance[0]}:{instance[1]}\"\\n        body = {}\\n        if instanceID in gpu_track:\\n            body = gpu_track[instanceID]\\n        else:\\n            url = f\"http://{instance[0]}:{config.DEFAULT_GPU_APP_PORT}/{config.GET_GPU_MODELS_INFO}?gpu={instance[1]}\"\\n            payload = {}\\n            headers = {}\\n            response = requests.request(\"GET\", url, headers=headers, data=payload)\\n            body = response.json()\\n            gpu_track[instanceID] = body\\n        if inferenceRate2 <= 0:\\n            if body[model] not in inference_request_details:\\n                try:\\n                    if(body[model] != \\'\\' and body[model] != \\'none\\'):\\n                        models.append({\\n                            \"model_name\": body[model],\\n                            \"instance_ip_address\": instance[0],\\n                            \"instance_gpu_id\": instance[1]\\n                        })\\n                except Exception as e:\\n                    print(e)\\n\\n    # Deploy your application to the best server (replace with actual deployment code)\\n    print(f\"Identified the models for cleaning up\")\\n\\n    return models\\n\\n\\ndef cleanUpModels():\\n    models = identifyCleanUpModels()\\n    print(models)\\n    try:\\n        for model in models:\\n            payload = json.dumps({\\n                \"model_name\": model[\\'model_name\\']\\n            })\\n            headers = {\\n                \\'Content-Type\\': \\'application/json\\'\\n            }\\n            url = f\"http://{model[\\'instance_ip_address\\']}:{config.DEFAULT_GPU_APP_PORT}/{config.UNLOAD_MODEL_URL}?gpu={model[\\'instance_gpu_id\\']}\"\\n\\n            response = requests.request(\\n                \"POST\", url, headers=headers, data=payload)\\n        return 200\\n    except Exception as e:\\n        print(e)\\n        return 400\\n\\n\\n# Main loop to periodically fetch, process, and delete messages\\ndef clearModelTask():\\n    status = cleanUpModels()\\n    if status == 200:\\n        print(\\'Models Cleaned Up successfully !\\')\\n    else:\\n        print(\\'Models Clean up failed\\')\\n\\n\\nclearModelScheduler = BackgroundScheduler()\\nclearModelScheduler.add_job(clearModelTask, \\'interval\\', minutes=5)  # Adjust the schedule as needed\\n# clearModelScheduler.start()\\n\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/tasks/clearModels.py'},\n",
       " {'name': 'fetchRediskeys',\n",
       "  'summary': '\\nFetches all Redis keys using the SCAN command, filters out specific keys, processes remaining keys to extract details, and counts inference requests for various models if they meet specific conditions. Results are returned in a dictionary of model names mapped to their request count. ThreadPoolExecutor is used for concurrent processing.\\n',\n",
       "  'type': 'function',\n",
       "  'code': 'def fetchRediskeys():\\n    print(\\'Start the celery task\\')\\n\\n    cursor = 0\\n    keys = []\\n    while True:\\n        cursor, partial_keys = r.scan(cursor, count=10000)\\n        keys.extend(partial_keys)\\n        if cursor == 0:\\n            break\\n\\n    failed_keys = r.smembers(\\'failed_keys\\')\\n\\n    inference_request_details = {}\\n\\n    def process_key(key):\\n        key = key.decode(\\'utf-8\\')\\n\\n        if key.startswith((\\'eyJ\\', \\'LIMITER\\', \\'SG\\', \\'unacked\\', \\'unacked_index\\', \\'mappings\\', \\'request_model_track\\')) or \\'celery\\' in key or \\'failed_keys\\' in key:\\n            return\\n\\n        if key in failed_keys:\\n            return\\n\\n        if \\'/\\' not in key:\\n            value = get_key_value_hash(key)\\n        else:\\n            parts = key.split(\\'/\\')\\n            if parts[0] == \\'request\\':\\n                user_id, model_name = parts[1], parts[2]\\n                values = r.hgetall(key)\\n                if values:\\n                    try:\\n                        status = values.get(b\\'status\\').decode()\\n                        start_time = values.get(b\\'start_time\\').decode()\\n                        triton_server_url = values.get(\\n                            b\\'triton_server_url\\', b\\'\\').decode()\\n\\n                    except Exception as error:\\n                        print(f\"An error occurred: {str(error)}\")\\n\\n                    time_now = datetime.now()\\n                    if start_time >= (time_now - datetime.timedelta(minutes=6)):\\n                        if model_name in inference_request_details:\\n                            inference_request_details[model_name] = inference_request_details[model_name] + 1\\n                        else:\\n                            inference_request_details[model_name] = 1\\n                    return\\n\\n    with ThreadPoolExecutor() as executor:\\n        executor.map(process_key, keys)\\n\\n    return inference_request_details\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/tasks/clearModels.py',\n",
       "  'dependent_functions': ['print',\n",
       "   'ThreadPoolExecutor',\n",
       "   'get_key_value_hash',\n",
       "   'print',\n",
       "   'str']},\n",
       " {'name': 'identifyCleanUpModels',\n",
       "  'summary': '\\nThe function `identifyCleanUpModels` queries Prometheus for GPU inference metrics, processes these metrics to identify specific models with no recent inference requests, checks these models against Redis data, and returns a list of models along with their instance details for possible cleanup.\\n',\n",
       "  'type': 'function',\n",
       "  'code': 'def identifyCleanUpModels():\\n    # Query Prometheus for the metrics you need (replace with actual metric names)\\n    gpu_hotness_model_query = \\'rate(nv_inference_request_success[5m])\\'\\n    start_time = datetime.datetime.now()-datetime.timedelta(minutes=1)\\n    end_time = datetime.datetime.now()\\n    step = \"1m\"\\n    data = query_range(gpu_hotness_model_query, start_time, end_time, step)  # Across the systems\\n    models = []\\n    inference_request_details = fetchRediskeys()\\n    gpu_track = {}\\n    for item in data:\\n        instance = get_instance_gpu(item[\"metric\"][\"instance\"])\\n        model = item[\"metric\"][\"model\"]\\n        values = item[\"values\"]\\n        inferenceRate1 = float(values[0][1])\\n        inferenceRate2 = float(values[1][1])\\n        instanceID = f\"{instance[0]}:{instance[1]}\"\\n        body = {}\\n        if instanceID in gpu_track:\\n            body = gpu_track[instanceID]\\n        else:\\n            url = f\"http://{instance[0]}:{config.DEFAULT_GPU_APP_PORT}/{config.GET_GPU_MODELS_INFO}?gpu={instance[1]}\"\\n            payload = {}\\n            headers = {}\\n            response = requests.request(\"GET\", url, headers=headers, data=payload)\\n            body = response.json()\\n            gpu_track[instanceID] = body\\n        if inferenceRate2 <= 0:\\n            if body[model] not in inference_request_details:\\n                try:\\n                    if(body[model] != \\'\\' and body[model] != \\'none\\'):\\n                        models.append({\\n                            \"model_name\": body[model],\\n                            \"instance_ip_address\": instance[0],\\n                            \"instance_gpu_id\": instance[1]\\n                        })\\n                except Exception as e:\\n                    print(e)\\n\\n    # Deploy your application to the best server (replace with actual deployment code)\\n    print(f\"Identified the models for cleaning up\")\\n\\n    return models\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/tasks/clearModels.py',\n",
       "  'dependent_functions': ['query_range',\n",
       "   'fetchRediskeys',\n",
       "   'print',\n",
       "   'get_instance_gpu',\n",
       "   'float',\n",
       "   'float',\n",
       "   'print']},\n",
       " {'name': 'cleanUpModels',\n",
       "  'summary': '\\nThe `cleanUpModels` function fetches a list of models, prints it, and sends a POST request to unload each model using their respective IP address and GPU ID. It constructs the payload with the model name, sets JSON headers, forms the URL, and returns 200 on success or 400 on failure.\\n',\n",
       "  'type': 'function',\n",
       "  'code': 'def cleanUpModels():\\n    models = identifyCleanUpModels()\\n    print(models)\\n    try:\\n        for model in models:\\n            payload = json.dumps({\\n                \"model_name\": model[\\'model_name\\']\\n            })\\n            headers = {\\n                \\'Content-Type\\': \\'application/json\\'\\n            }\\n            url = f\"http://{model[\\'instance_ip_address\\']}:{config.DEFAULT_GPU_APP_PORT}/{config.UNLOAD_MODEL_URL}?gpu={model[\\'instance_gpu_id\\']}\"\\n\\n            response = requests.request(\\n                \"POST\", url, headers=headers, data=payload)\\n        return 200\\n    except Exception as e:\\n        print(e)\\n        return 400\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/tasks/clearModels.py',\n",
       "  'dependent_functions': ['identifyCleanUpModels', 'print', 'print']},\n",
       " {'name': 'clearModelTask',\n",
       "  'summary': 'Defines clearModelTask() function that calls cleanUpModels(), checks if the returned status is 200, and prints a success or failure message accordingly.',\n",
       "  'type': 'function',\n",
       "  'code': \"def clearModelTask():\\n    status = cleanUpModels()\\n    if status == 200:\\n        print('Models Cleaned Up successfully !')\\n    else:\\n        print('Models Clean up failed')\\n\",\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/tasks/clearModels.py',\n",
       "  'dependent_functions': ['cleanUpModels', 'print', 'print']},\n",
       " {'name': 'process_key',\n",
       "  'summary': \"\\nThis function processes a given key by decoding it and checking it against specific conditions to determine its relevance. If certain prefixes or substrings exist, or if the key is found in a 'failed_keys' list, processing stops. For keys not containing a '/', a hash value is retrieved. For keys with a '/', it extracts 'user_id' and 'model_name' and updates a dictionary based on conditions by fetching and decoding values from a Redis-like data source.\\n\",\n",
       "  'type': 'function',\n",
       "  'code': '    def process_key(key):\\n        key = key.decode(\\'utf-8\\')\\n\\n        if key.startswith((\\'eyJ\\', \\'LIMITER\\', \\'SG\\', \\'unacked\\', \\'unacked_index\\', \\'mappings\\', \\'request_model_track\\')) or \\'celery\\' in key or \\'failed_keys\\' in key:\\n            return\\n\\n        if key in failed_keys:\\n            return\\n\\n        if \\'/\\' not in key:\\n            value = get_key_value_hash(key)\\n        else:\\n            parts = key.split(\\'/\\')\\n            if parts[0] == \\'request\\':\\n                user_id, model_name = parts[1], parts[2]\\n                values = r.hgetall(key)\\n                if values:\\n                    try:\\n                        status = values.get(b\\'status\\').decode()\\n                        start_time = values.get(b\\'start_time\\').decode()\\n                        triton_server_url = values.get(\\n                            b\\'triton_server_url\\', b\\'\\').decode()\\n\\n                    except Exception as error:\\n                        print(f\"An error occurred: {str(error)}\")\\n\\n                    time_now = datetime.now()\\n                    if start_time >= (time_now - datetime.timedelta(minutes=6)):\\n                        if model_name in inference_request_details:\\n                            inference_request_details[model_name] = inference_request_details[model_name] + 1\\n                        else:\\n                            inference_request_details[model_name] = 1\\n                    return\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/tasks/clearModels.py',\n",
       "  'dependent_functions': ['get_key_value_hash', 'print', 'str']},\n",
       " {'name': 'health.py',\n",
       "  'summary': '\\nThe code defines a Flask Blueprint `core` with a single route `/Redis-health-check` that handles GET requests. This endpoint performs a health check on a Redis instance using the `redis_utils` module\\'s Redis client `r`. In the `health_check` function, it tries to set, get, and expire a key (`health-check`) in Redis to ensure Redis service is operational. If these operations succeed, Redis is considered available, and the health status is set to \"healthy\" with an HTTP 200 status code. If any exception occurs during these operations, Redis is considered unavailable, and the health status is set to \"unhealthy\" with an HTTP 400 status code. The response is returned as a JSON object indicating the health status.\\n',\n",
       "  'type': 'file',\n",
       "  'code': 'from flask import Blueprint, jsonify\\nfrom app.utils.redis_utils import r\\n\\ncore = Blueprint(\\'core\\', __name__)\\n\\n@core.route(\\'/Redis-health-check\\', methods=[\\'GET\\'])\\ndef health_check():\\n    redis_available = True\\n    health_status = True\\n    try:\\n        health_check_key = \\'health-check\\'\\n        r.set(health_check_key, 1)\\n        r.get(health_check_key)\\n        r.expire(health_check_key, 1)\\n    except:\\n        redis_available = False\\n\\n    if redis_available and health_status:\\n        response = jsonify(health=\"healthy\")\\n        response.status_code = 200\\n    else:\\n        response = jsonify(health=\"unhealthy\")\\n        response.status_code = 400\\n\\n    return response',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/core/health.py'},\n",
       " {'name': 'health_check',\n",
       "  'summary': 'Performs a health check by attempting to set, get, and expire a key in Redis. If successful, it returns a JSON response with status \"healthy\" and HTTP 200. If Redis operations fail, it returns \"unhealthy\" with HTTP 400.',\n",
       "  'type': 'function',\n",
       "  'code': 'def health_check():\\n    redis_available = True\\n    health_status = True\\n    try:\\n        health_check_key = \\'health-check\\'\\n        r.set(health_check_key, 1)\\n        r.get(health_check_key)\\n        r.expire(health_check_key, 1)\\n    except:\\n        redis_available = False\\n\\n    if redis_available and health_status:\\n        response = jsonify(health=\"healthy\")\\n        response.status_code = 200\\n    else:\\n        response = jsonify(health=\"unhealthy\")\\n        response.status_code = 400\\n\\n    return response',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/core/health.py',\n",
       "  'dependent_functions': ['jsonify', 'jsonify']},\n",
       " {'name': 'tasks.py',\n",
       "  'summary': \"\\nThe code is an implementation of a Flask RESTful API for managing the status of machine learning models. It involves several utility functions and authentication methods. The main class, `UpdateModelStatus`, contains a `post` method which processes incoming JSON data from HTTP POST requests to update the status of a model. Key functionalities include:\\n\\n1. Extracting information like `model_name`, `model_status`, `request_id`, `instance_ip_address`, `instance_gpu_id`, and `request_type` from the incoming JSON request data.\\n2. Checking if required parameters are present and validating the status of the model.\\n3. Registering the endpoint of the model if the status is 'DOWNLOAD_SUCCESS'.\\n4. Updating the status to 'AVAILABLE' if registration is successful.\\n5. Logging messages and deleting them from a message queue upon successful status update and endpoint registration.\\n6. Handling errors by updating the status to 'FAILED' and logging details.\\n7. Using helper functions `_registerEndpoint` to register the model endpoint with Redis and `_spotStatusUpdate` to send status updates to a specified URL.\\n\\nThe code includes necessary imports, configurations, and utility functions for interacting with Redis, message queues, and external APIs.\\n\",\n",
       "  'type': 'file',\n",
       "  'code': 'from flask_restful import Resource, abort, reqparse\\nfrom flask import request ,  current_app as app\\nfrom flask import request, Blueprint, current_app, jsonify, abort\\nfrom os import environ\\nfrom app.utils.string_utils import isBlank, isNotBlank\\nfrom app.utils.redis_utils import get_mapping_keyname,set_mapping_value_list, get_key_value_hash, r\\nfrom app.utils.queue_utils import receive_message, delete_message\\nfrom app.utils.security import jwt_or_api_token_required, jwt_token_required, get_access_token, get_user_id\\nimport jwt\\nfrom urllib.parse import urlsplit\\nimport requests\\nimport json\\nfrom app.config import BaseConfig\\n\\nconfig = BaseConfig()\\n\\nclass UpdateModelStatus(Resource):\\n    def __init__(self):\\n        self.parser = reqparse.RequestParser()\\n\\n    # @jwt_or_api_token_required\\n    def post(self):\\n            request_data = request.get_json()\\n            print(f\"Request Data {request_data}\")\\n            model_name = request_data.get(\\'short_code\\') if request_data.get(\\'short_code\\') else request_data.get(\\'model_name\\')\\n            model_status = request_data.get(\\'model_status\\')\\n            request_id = request_data.get(\\'request_id\\')\\n            instance_ip_address = request_data.get(\\'instance_ip_address\\')\\n            instance_gpu_id = request_data.get(\\'instance_gpu_id\\') if request_data.get(\\'instance_gpu_id\\') else 0\\n            request_type = request_data.get(\\'request_type\\')\\n            try:\\n                if request_id and model_name and model_status and model_status == \\'DOWNLOAD_SUCCESS\\':\\n                    requestKeyname = f\"model_request_status/{request_id}\"\\n                    r.set(requestKeyname, model_status)\\n                    status = _registerEndpoint(model_name, instance_ip_address, instance_gpu_id)\\n                    if status == 200:\\n                        print(f\\'{instance_ip_address}:{instance_gpu_id} Endpoint registered successfully\\')\\n                        spotStatus = _spotStatusUpdate(request_id, model_name, \\'AVAILABLE\\', request_type)\\n                        if spotStatus == 200:\\n                            keyname = f\"model_cold_mappings/{instance_ip_address}:{instance_gpu_id}\"\\n                            set_mapping_value_list(keyname, model_name)\\n                            r.set(requestKeyname, \\'AVAILABLE\\')\\n                            modelKey = f\"track/{request_id}\"\\n                            message = r.get(modelKey)\\n                            print(f\"message {message}\")\\n                            delete_message(json.loads(message))\\n                            print(\\'Message deleted successfully from queue\\')\\n                            return jsonify({\\n                                \"success\": True,\\n                                \"status\":\"Model available\"\\n                            })\\n                        else:\\n                            _spotStatusUpdate(request_id, model_name, \\'FAILED\\', request_type)\\n                            abort(500, \"Spot Status Update Failed\")\\n                    else:\\n                        _spotStatusUpdate(request_id, model_name, \\'FAILED\\', request_type)\\n                        abort(500, \"Endpoint Registration Failed\")\\n                else:\\n                    raise Exception(\\'Missing parameters\\')\\n            except Exception as e:\\n                print(\\'Task Update Route\\',e)\\n                if request_id and model_name:\\n                    keyname = f\"model_request_status/{request_id}\"\\n                    r.set(keyname, model_status)\\n                    _spotStatusUpdate(request_id, model_name, \\'FAILED\\', request_type)\\n                abort(400, \"Bad Request\")\\n\\ndef _registerEndpoint(model_name, instance_ip_address, instance_gpu_id):\\n    server_endpoint = f\"http://{instance_ip_address}:{config.DEFAULT_GPU_APP_PORT}/{config.INFERENCE_GPU_URL}/{model_name}?gpu={instance_gpu_id}\"\\n    if model_name and server_endpoint:\\n        model_name_key = get_mapping_keyname(model_name)\\n        r.lpush(model_name_key, server_endpoint)\\n        return 200   \\n    else:\\n        return 400\\n\\ndef _spotStatusUpdate(request_id, model_name, status, request_type):\\n    try:\\n        payload = {       \\n            \"status\": status,\\n            \"request_id\": request_id,\\n            \"version\": \"v1\"\\n        }\\n        if(status == \"AVAILABLE\"):\\n            payload[\\'segmind_model_path\\'] = f\"{config.PROXY_INFERENCE_URL}/{model_name}\"\\n        headers = {\\n            \\'Content-Type\\': \\'application/json\\',\\n            \\'secret\\': config.SPOT_SECRET_TOKEN\\n        }\\n        print(f\"request_type {request_type}\")\\n        if request_type.lower() == \\'hubmodel\\':\\n            url = config.SPOT_HUB_STATUS_UPDATE_URL\\n        elif request_type.lower() == \\'finetune\\':\\n            url = config.SPOT_FINETUNE_STATUS_UPDATE_URL\\n        print(f\"{url} and {payload}\")\\n        response = requests.request(\\n                \"POST\", url, headers=headers, data=json.dumps(payload))\\n        return 200\\n    except Exception as e:\\n        print(f\"Spot Update {e}\")\\n        return 400\\n\\n\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/controllers/tasks/tasks.py'},\n",
       " {'name': 'UpdateModelStatus',\n",
       "  'summary': '\\nThe `UpdateModelStatus` class contains a RESTful API resource for updating the status of a model. It uses Flask-RESTful and parses JSON requests to retrieve model details and update Redis storage and Spot status. It handles successful endpoint registration and failure cases with detailed responses and logging.\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/controllers/tasks/tasks.py',\n",
       "  'type': 'class',\n",
       "  'code': 'class UpdateModelStatus(Resource):\\n    def __init__(self):\\n        self.parser = reqparse.RequestParser()\\n\\n    # @jwt_or_api_token_required\\n    def post(self):\\n            request_data = request.get_json()\\n            print(f\"Request Data {request_data}\")\\n            model_name = request_data.get(\\'short_code\\') if request_data.get(\\'short_code\\') else request_data.get(\\'model_name\\')\\n            model_status = request_data.get(\\'model_status\\')\\n            request_id = request_data.get(\\'request_id\\')\\n            instance_ip_address = request_data.get(\\'instance_ip_address\\')\\n            instance_gpu_id = request_data.get(\\'instance_gpu_id\\') if request_data.get(\\'instance_gpu_id\\') else 0\\n            request_type = request_data.get(\\'request_type\\')\\n            try:\\n                if request_id and model_name and model_status and model_status == \\'DOWNLOAD_SUCCESS\\':\\n                    requestKeyname = f\"model_request_status/{request_id}\"\\n                    r.set(requestKeyname, model_status)\\n                    status = _registerEndpoint(model_name, instance_ip_address, instance_gpu_id)\\n                    if status == 200:\\n                        print(f\\'{instance_ip_address}:{instance_gpu_id} Endpoint registered successfully\\')\\n                        spotStatus = _spotStatusUpdate(request_id, model_name, \\'AVAILABLE\\', request_type)\\n                        if spotStatus == 200:\\n                            keyname = f\"model_cold_mappings/{instance_ip_address}:{instance_gpu_id}\"\\n                            set_mapping_value_list(keyname, model_name)\\n                            r.set(requestKeyname, \\'AVAILABLE\\')\\n                            modelKey = f\"track/{request_id}\"\\n                            message = r.get(modelKey)\\n                            print(f\"message {message}\")\\n                            delete_message(json.loads(message))\\n                            print(\\'Message deleted successfully from queue\\')\\n                            return jsonify({\\n                                \"success\": True,\\n                                \"status\":\"Model available\"\\n                            })\\n                        else:\\n                            _spotStatusUpdate(request_id, model_name, \\'FAILED\\', request_type)\\n                            abort(500, \"Spot Status Update Failed\")\\n                    else:\\n                        _spotStatusUpdate(request_id, model_name, \\'FAILED\\', request_type)\\n                        abort(500, \"Endpoint Registration Failed\")\\n                else:\\n                    raise Exception(\\'Missing parameters\\')\\n            except Exception as e:\\n                print(\\'Task Update Route\\',e)\\n                if request_id and model_name:\\n                    keyname = f\"model_request_status/{request_id}\"\\n                    r.set(keyname, model_status)\\n                    _spotStatusUpdate(request_id, model_name, \\'FAILED\\', request_type)\\n                abort(400, \"Bad Request\")\\n',\n",
       "  'class_funcs': {'__init__': {'function_name': '__init__',\n",
       "    'summary': '',\n",
       "    'code': '    def __init__(self):\\n        self.parser = reqparse.RequestParser()\\n',\n",
       "    'dependent_functions': []},\n",
       "   'post': {'function_name': 'post',\n",
       "    'summary': '',\n",
       "    'code': '    def post(self):\\n            request_data = request.get_json()\\n            print(f\"Request Data {request_data}\")\\n            model_name = request_data.get(\\'short_code\\') if request_data.get(\\'short_code\\') else request_data.get(\\'model_name\\')\\n            model_status = request_data.get(\\'model_status\\')\\n            request_id = request_data.get(\\'request_id\\')\\n            instance_ip_address = request_data.get(\\'instance_ip_address\\')\\n            instance_gpu_id = request_data.get(\\'instance_gpu_id\\') if request_data.get(\\'instance_gpu_id\\') else 0\\n            request_type = request_data.get(\\'request_type\\')\\n            try:\\n                if request_id and model_name and model_status and model_status == \\'DOWNLOAD_SUCCESS\\':\\n                    requestKeyname = f\"model_request_status/{request_id}\"\\n                    r.set(requestKeyname, model_status)\\n                    status = _registerEndpoint(model_name, instance_ip_address, instance_gpu_id)\\n                    if status == 200:\\n                        print(f\\'{instance_ip_address}:{instance_gpu_id} Endpoint registered successfully\\')\\n                        spotStatus = _spotStatusUpdate(request_id, model_name, \\'AVAILABLE\\', request_type)\\n                        if spotStatus == 200:\\n                            keyname = f\"model_cold_mappings/{instance_ip_address}:{instance_gpu_id}\"\\n                            set_mapping_value_list(keyname, model_name)\\n                            r.set(requestKeyname, \\'AVAILABLE\\')\\n                            modelKey = f\"track/{request_id}\"\\n                            message = r.get(modelKey)\\n                            print(f\"message {message}\")\\n                            delete_message(json.loads(message))\\n                            print(\\'Message deleted successfully from queue\\')\\n                            return jsonify({\\n                                \"success\": True,\\n                                \"status\":\"Model available\"\\n                            })\\n                        else:\\n                            _spotStatusUpdate(request_id, model_name, \\'FAILED\\', request_type)\\n                            abort(500, \"Spot Status Update Failed\")\\n                    else:\\n                        _spotStatusUpdate(request_id, model_name, \\'FAILED\\', request_type)\\n                        abort(500, \"Endpoint Registration Failed\")\\n                else:\\n                    raise Exception(\\'Missing parameters\\')\\n            except Exception as e:\\n                print(\\'Task Update Route\\',e)\\n                if request_id and model_name:\\n                    keyname = f\"model_request_status/{request_id}\"\\n                    r.set(keyname, model_status)\\n                    _spotStatusUpdate(request_id, model_name, \\'FAILED\\', request_type)\\n                abort(400, \"Bad Request\")\\n',\n",
       "    'dependent_functions': ['print',\n",
       "     '_registerEndpoint',\n",
       "     'Exception',\n",
       "     'print',\n",
       "     'abort',\n",
       "     'print',\n",
       "     '_spotStatusUpdate',\n",
       "     '_spotStatusUpdate',\n",
       "     'abort',\n",
       "     '_spotStatusUpdate',\n",
       "     'set_mapping_value_list',\n",
       "     'print',\n",
       "     'delete_message',\n",
       "     'print',\n",
       "     'jsonify',\n",
       "     '_spotStatusUpdate',\n",
       "     'abort']}}},\n",
       " {'name': '_registerEndpoint',\n",
       "  'summary': 'Registers an endpoint URL for a given model with IP address and GPU ID, storing it in a Redis list if successful, and returns HTTP status code 200 or 400 based on input validity.',\n",
       "  'type': 'function',\n",
       "  'code': 'def _registerEndpoint(model_name, instance_ip_address, instance_gpu_id):\\n    server_endpoint = f\"http://{instance_ip_address}:{config.DEFAULT_GPU_APP_PORT}/{config.INFERENCE_GPU_URL}/{model_name}?gpu={instance_gpu_id}\"\\n    if model_name and server_endpoint:\\n        model_name_key = get_mapping_keyname(model_name)\\n        r.lpush(model_name_key, server_endpoint)\\n        return 200   \\n    else:\\n        return 400\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/controllers/tasks/tasks.py',\n",
       "  'dependent_functions': ['get_mapping_keyname']},\n",
       " {'name': '_spotStatusUpdate',\n",
       "  'summary': '\\nThe function `_spotStatusUpdate` updates the status of a request by sending a POST request with a payload containing the request ID, status, and version. If the status is \"AVAILABLE\", it includes a model path. The URL for the request depends on `request_type` being either \\'hubmodel\\' or \\'finetune\\'. It returns 200 on success and 400 on failure.\\n',\n",
       "  'type': 'function',\n",
       "  'code': 'def _spotStatusUpdate(request_id, model_name, status, request_type):\\n    try:\\n        payload = {       \\n            \"status\": status,\\n            \"request_id\": request_id,\\n            \"version\": \"v1\"\\n        }\\n        if(status == \"AVAILABLE\"):\\n            payload[\\'segmind_model_path\\'] = f\"{config.PROXY_INFERENCE_URL}/{model_name}\"\\n        headers = {\\n            \\'Content-Type\\': \\'application/json\\',\\n            \\'secret\\': config.SPOT_SECRET_TOKEN\\n        }\\n        print(f\"request_type {request_type}\")\\n        if request_type.lower() == \\'hubmodel\\':\\n            url = config.SPOT_HUB_STATUS_UPDATE_URL\\n        elif request_type.lower() == \\'finetune\\':\\n            url = config.SPOT_FINETUNE_STATUS_UPDATE_URL\\n        print(f\"{url} and {payload}\")\\n        response = requests.request(\\n                \"POST\", url, headers=headers, data=json.dumps(payload))\\n        return 200\\n    except Exception as e:\\n        print(f\"Spot Update {e}\")\\n        return 400\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/controllers/tasks/tasks.py',\n",
       "  'dependent_functions': ['print', 'print', 'print']},\n",
       " {'name': 'prometheus.py',\n",
       "  'summary': '\\nThe code implements a Flask RESTful resource named `GetServerStatus` that provides server status information. It uses several Flask and utility imports for handling HTTP requests, authentication, and Redis operations. \\n\\n1. **Setup**: A request parser `self.parser` is initialized in the `__init__` method.\\n2. **Authentication**: The `get` method is decorated with `@jwt_or_api_token_required` to ensure that only authenticated users can access it.\\n3. **Authorization**: Within the `get` method, `is_admin_user(request)` checks if the requesting user has admin privileges; if not, the method aborts with a 401 error.\\n4. **Data Gathering**: It retrieves all keys from Redis using `get_all_mappings_keys()` and initializes an empty dictionary `data`.\\n5. **Server Status Check**: For each key, it fetches values using `get_values_list(key_name)` and parses the URL to obtain the base address. For each base address, it checks if the address is already in the dictionary. If not, it creates a dictionary with the default port, server status (obtained via `GetPrometheusServerStatus`), and an empty list for models. It then adds the key to the Models list for the corresponding base address.\\n6. **Returning Data**: The constructed `data` dictionary is returned, listing the status and models for each server.\\n7. **Status Helper Function**: `GetPrometheusServerStatus` constructs a metrics URL for the server and checks the status by sending a request. It returns \"Up\" if the server responds with status code 200; otherwise, it returns \"Down\".\\n',\n",
       "  'type': 'file',\n",
       "  'code': 'from flask_restful import Resource, abort, reqparse\\nfrom flask import request ,  current_app as app\\nfrom flask import request, Blueprint, current_app, jsonify, abort\\nfrom app.utils.redis_utils import get_values_list, get_all_mappings_keys, r\\nfrom app.utils.security import jwt_or_api_token_required , is_admin_user\\nimport jwt\\nfrom urllib.parse import urlsplit\\nimport requests\\n\\n\\nclass GetServerStatus(Resource):\\n    def __init__(self):\\n        self.parser = reqparse.RequestParser()\\n    \\n    @jwt_or_api_token_required\\n    def get(self):\\n        if is_admin_user(request):\\n            mappings_key_names = get_all_mappings_keys()\\n            data = {}\\n            for key_name in mappings_key_names:\\n                for val in get_values_list(key_name):\\n                    url_parts = urlsplit(val.decode(\\'utf-8\\'))\\n                    base_addr = url_parts.netloc\\n                    base_addr_split = base_addr.split(\\':\\')\\n                    if base_addr_split[0] not in data:\\n                        stats= {\\n                            \"Port\" : \"8002\",\\n                            \"Status\" : GetPrometheusServerStatus(base_addr_split[0]),\\n                            \"Models\" : []\\n                        }\\n                            \\n                        if val.decode(\\'utf-8\\').__contains__(base_addr):\\n                            stats[\"Models\"].append(key_name.decode(\\'utf-8\\').replace(\\'mappings/\\',\\'\\'))\\n                        data[base_addr_split[0]] = stats\\n                    else:\\n                        data[base_addr_split[0]][\"Models\"].append(key_name.decode(\\'utf-8\\').replace(\\'mappings/\\',\\'\\'))\\n            return data\\n        else:\\n            abort(401,\"Elevated access privileges required.\")\\n\\ndef GetPrometheusServerStatus(serveraddress):\\n    address= f\"http://{serveraddress}:8002/metrics\"\\n    try:\\n        reply = requests.get(address).status_code\\n        if reply == 200:\\n            return \"Up\"\\n        else:\\n            return \"Down\"\\n    except (requests.exceptions.HTTPError, requests.exceptions.ConnectionError):\\n        return \"Down\"',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/controllers/admins/prometheus.py'},\n",
       " {'name': 'GetServerStatus',\n",
       "  'summary': 'Class `GetServerStatus` retrieves server status data for admin users, compiling mappings and Prometheus server status for different base addresses, and returns the structured information. Inherits from `Resource`, uses `reqparse` for request parsing, requires JWT/API token authentication, and returns 401 for non-admin users.',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/controllers/admins/prometheus.py',\n",
       "  'type': 'class',\n",
       "  'code': 'class GetServerStatus(Resource):\\n    def __init__(self):\\n        self.parser = reqparse.RequestParser()\\n    \\n    @jwt_or_api_token_required\\n    def get(self):\\n        if is_admin_user(request):\\n            mappings_key_names = get_all_mappings_keys()\\n            data = {}\\n            for key_name in mappings_key_names:\\n                for val in get_values_list(key_name):\\n                    url_parts = urlsplit(val.decode(\\'utf-8\\'))\\n                    base_addr = url_parts.netloc\\n                    base_addr_split = base_addr.split(\\':\\')\\n                    if base_addr_split[0] not in data:\\n                        stats= {\\n                            \"Port\" : \"8002\",\\n                            \"Status\" : GetPrometheusServerStatus(base_addr_split[0]),\\n                            \"Models\" : []\\n                        }\\n                            \\n                        if val.decode(\\'utf-8\\').__contains__(base_addr):\\n                            stats[\"Models\"].append(key_name.decode(\\'utf-8\\').replace(\\'mappings/\\',\\'\\'))\\n                        data[base_addr_split[0]] = stats\\n                    else:\\n                        data[base_addr_split[0]][\"Models\"].append(key_name.decode(\\'utf-8\\').replace(\\'mappings/\\',\\'\\'))\\n            return data\\n        else:\\n            abort(401,\"Elevated access privileges required.\")\\n',\n",
       "  'class_funcs': {'__init__': {'function_name': '__init__',\n",
       "    'summary': '',\n",
       "    'code': '    def __init__(self):\\n        self.parser = reqparse.RequestParser()\\n',\n",
       "    'dependent_functions': []},\n",
       "   'get': {'function_name': 'get',\n",
       "    'summary': '',\n",
       "    'code': '    def get(self):\\n        if is_admin_user(request):\\n            mappings_key_names = get_all_mappings_keys()\\n            data = {}\\n            for key_name in mappings_key_names:\\n                for val in get_values_list(key_name):\\n                    url_parts = urlsplit(val.decode(\\'utf-8\\'))\\n                    base_addr = url_parts.netloc\\n                    base_addr_split = base_addr.split(\\':\\')\\n                    if base_addr_split[0] not in data:\\n                        stats= {\\n                            \"Port\" : \"8002\",\\n                            \"Status\" : GetPrometheusServerStatus(base_addr_split[0]),\\n                            \"Models\" : []\\n                        }\\n                            \\n                        if val.decode(\\'utf-8\\').__contains__(base_addr):\\n                            stats[\"Models\"].append(key_name.decode(\\'utf-8\\').replace(\\'mappings/\\',\\'\\'))\\n                        data[base_addr_split[0]] = stats\\n                    else:\\n                        data[base_addr_split[0]][\"Models\"].append(key_name.decode(\\'utf-8\\').replace(\\'mappings/\\',\\'\\'))\\n            return data\\n        else:\\n            abort(401,\"Elevated access privileges required.\")\\n',\n",
       "    'dependent_functions': ['is_admin_user',\n",
       "     'get_all_mappings_keys',\n",
       "     'abort',\n",
       "     'get_values_list',\n",
       "     'urlsplit',\n",
       "     'GetPrometheusServerStatus']}}},\n",
       " {'name': 'GetPrometheusServerStatus',\n",
       "  'summary': 'Checks Prometheus server status by sending an HTTP GET request to \"http://{serveraddress}:8002/metrics\"; returns \"Up\" if status code is 200, otherwise \"Down\"; handles HTTP and connection errors by returning \"Down\".',\n",
       "  'type': 'function',\n",
       "  'code': 'def GetPrometheusServerStatus(serveraddress):\\n    address= f\"http://{serveraddress}:8002/metrics\"\\n    try:\\n        reply = requests.get(address).status_code\\n        if reply == 200:\\n            return \"Up\"\\n        else:\\n            return \"Down\"\\n    except (requests.exceptions.HTTPError, requests.exceptions.ConnectionError):\\n        return \"Down\"',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/controllers/admins/prometheus.py',\n",
       "  'dependent_functions': []},\n",
       " {'name': 'mappings.py',\n",
       "  'summary': '\\nThe provided code defines a Flask RESTful API for managing model-to-server endpoint mappings using Redis as a cache. The system includes five resources: `RegisterModelMappings`, `DeRegisterModelMappings`, `UpdateModelMappings`, `GetAllModelMappings`, and `GetAllServerMappings`.\\n\\n1. **RegisterModelMappings**:\\n   - Uses `POST` to register model names with server endpoints in Redis.\\n   - Checks GPU status of endpoints and supports redundancy management by adding endpoints multiple times.\\n   - Requires admin user authentication.\\n\\n2. **DeRegisterModelMappings**:\\n   - Uses `POST` to remove server endpoints from the model mappings in Redis.\\n   - Can delete entire mappings or specific endpoints, with support for conditional deletion counts.\\n   - Requires admin user authentication.\\n\\n3. **UpdateModelMappings**:\\n   - Uses `POST` to update server endpoints for a given model in Redis.\\n   - Verifies GPU status of new endpoints before updating.\\n   - Requires admin user authentication.\\n\\n4. **GetAllModelMappings**:\\n   - Uses `GET` to fetch all model-to-endpoint mappings from Redis.\\n   - Returns mappings in a structured format.\\n   - Requires admin user authentication.\\n\\n5. **GetAllServerMappings**:\\n   - Uses `GET` to obtain all server mappings, grouping endpoints by base address and port.\\n   - Also provides the total number of servers.\\n   - Requires admin user authentication.\\n\\nThe code handles errors with detailed messages and ensures elevated privileges for each operation.\\n',\n",
       "  'type': 'file',\n",
       "  'code': 'from flask_restful import Resource, abort, reqparse\\nfrom flask import request ,  current_app as app\\nfrom flask import request, Blueprint, current_app, jsonify, abort\\nfrom os import environ\\nfrom app.utils.string_utils import isBlank, isNotBlank\\nfrom app.utils.redis_utils import get_values_list, set_mapping_value_list, get_mapping_keyname, remove_values_list, delete_key, get_key_value_hash, set_key_value_hash, set_key_expiry, get_all_mappings_keys, r\\nfrom app.utils.security import authenticate_user, jwt_or_api_token_required, jwt_token_required, get_access_token, get_user_id, is_admin_user\\nfrom app.utils.common import get_gpu_status\\nimport jwt\\nfrom urllib.parse import urlsplit\\n\\nclass RegisterModelMappings(Resource):\\n    def __init__(self):\\n        self.parser = reqparse.RequestParser()\\n\\n    @jwt_or_api_token_required\\n    def post(self):\\n        rsp_string = \\'\\'\\n        if is_admin_user(request):\\n            request_data = request.get_json()\\n            model_name = request_data.get(\\'model_name\\')\\n            server_endpoint = request_data.get(\\'server_endpoint\\')\\n            redundancy_count = request_data.get(\\'redundancy_count\\')\\n            if model_name and server_endpoint:\\n                for endpoint in server_endpoint:\\n                    gpu_status = get_gpu_status(endpoint)\\n                    if gpu_status != 200:\\n                        abort(400, f\\'{endpoint} GPU information is missing ! Please add the GPU information to DB\\')\\n                model_name_key = get_mapping_keyname(model_name)\\n                if r.exists(model_name_key):\\n                    endpoint_list = r.lrange(model_name_key, 0, -1)\\n                    if redundancy_count and int(redundancy_count):\\n                        for endpoint in server_endpoint:\\n                            for _ in range(redundancy_count):\\n                                r.lpush(model_name_key, endpoint)\\n                                rsp_string = f\"Server-endpoints updated successfully!\"\\n                    else:\\n                        for endpoint in server_endpoint:\\n                            r.lpush(model_name_key, endpoint)\\n                            rsp_string = f\"Server-endpoints updated successfully!\"\\n                            continue\\n                    return jsonify(rsp_string)\\n                else:\\n                    if redundancy_count and int(redundancy_count):\\n                        for endpoint in server_endpoint:\\n                            for _ in range(redundancy_count):\\n                                r.lpush(model_name_key, endpoint)\\n                                rsp_string = \"Model-name & Server-endpoints added in Redis cache successfully!\"\\n                    else:\\n                        for endpoint in server_endpoint:\\n                            r.lpush(model_name_key, endpoint)\\n                            rsp_string = \"Model-name & Server-endpoints added in Redis cache successfully!\"\\n                    return jsonify(rsp_string)   \\n            else:\\n                abort(400, \\'Model-name & Server-endpoint missing in request params!\\')\\n        else:\\n            abort(401,\"Elevated access privileges required.\")\\n\\n\\nclass DeRegisterModelMappings(Resource):\\n    def __init__(self):\\n        self.parser = reqparse.RequestParser()\\n\\n    @jwt_or_api_token_required\\n    def post(self):\\n        rsp_string = \\'\\'\\n        if is_admin_user(request):\\n            request_data = request.get_json()\\n            model_name = request_data.get(\\'model_name\\')\\n            server_endpoint = request_data.get(\\'server_endpoint\\')\\n            is_delete_key_required = request_data.get(\\'is_delete_key_required\\')\\n            deletion_count = request_data.get(\\'deletion_count\\')\\n\\n            if model_name:\\n                model_name_key = get_mapping_keyname(model_name)\\n                if r.exists(model_name_key):\\n                    if is_delete_key_required is True:\\n                        delete_key(model_name_key)\\n                        rsp_string = \"Model mappings successfully removed from Redis cache.\"\\n                        return jsonify(rsp_string)\\n                    else:\\n                        endpoint_list = r.lrange(model_name_key, 0, -1)\\n                        rsp_string = \\'\\'\\n                        if deletion_count is None:\\n                            for endpoint in server_endpoint:\\n                                if endpoint_list and (endpoint.encode() in endpoint_list):\\n                                    endpoint_count = endpoint_list.count(endpoint.encode())\\n                                    remove_values_list(model_name_key, endpoint, endpoint_count)\\n                                    rsp_string += f\"Server-endpoint: {endpoint} successfully removed from Redis cache.\"\\n                                    endpoint_list = r.lrange(model_name_key, 0, -1)\\n                                else:\\n                                    rsp_string += f\"Server-endpoint: {endpoint} not found in Redis cache.\"\\n                                    continue\\n                        else:\\n                            for endpoint in server_endpoint:\\n                                if endpoint_list and (endpoint.encode() in endpoint_list):\\n                                    remove_values_list(model_name_key, endpoint, int(deletion_count))\\n                                    rsp_string += f\"Server-endpoint: {endpoint} successfully removed from Redis cache.\"\\n                                    endpoint_list = r.lrange(model_name_key, 0, -1)\\n                                else:\\n                                    rsp_string += f\"Server-endpoint: {endpoint} not found in Redis cache.\"\\n                                    continue\\n                        return jsonify(rsp_string)\\n                else:\\n                    rsp_string = \"Server-endpoint not found in Redis cache.\"\\n                    return jsonify(rsp_string)\\n            else:\\n                abort(400, \\'Mandatory parameters missing in the request.\\')\\n        else:\\n            abort(401,\"Elevated access privileges required.\")\\n \\n\\nclass UpdateModelMappings(Resource):\\n    def __init__(self):\\n        self.parser = reqparse.RequestParser()\\n\\n    @jwt_or_api_token_required\\n    def post(self):\\n        rsp_string = \\'\\'\\n        if is_admin_user(request):\\n            request_data = request.get_json()\\n            model_name = request_data.get(\\'model_name\\')\\n            old_server_endpoint = request_data.get(\\'old_server_endpoint\\')\\n            new_server_endpoint = request_data.get(\\'new_server_endpoint\\')\\n\\n            if model_name and old_server_endpoint and new_server_endpoint:\\n                gpu_status = get_gpu_status(new_server_endpoint)\\n                if gpu_status != 200:\\n                    abort(400, f\\'{new_server_endpoint} GPU information is missing ! Please add the GPU information to DB\\')\\n                model_name_key = get_mapping_keyname(model_name)\\n                if r.exists(model_name_key):\\n                    endpoint_list = r.lrange(model_name_key, 0, -1)\\n                    if endpoint_list:\\n                        endpoint_count = endpoint_list.count(old_server_endpoint.encode())\\n                        if old_server_endpoint.encode() in endpoint_list:\\n                            for _ in range(int(endpoint_count)):\\n                                old_mapping_index = endpoint_list.index(old_server_endpoint.encode())\\n                                r.lset(model_name_key, old_mapping_index, new_server_endpoint)\\n                                rsp_string = f\"Server-endpoint successfully updated with newer value in Redis cache.\"\\n                                endpoint_list = r.lrange(model_name_key, 0, -1)\\n                        else:\\n                            rsp_string = f\"Endpoint: {old_server_endpoint} not found for Model-name: {model_name}\"\\n                        return jsonify(rsp_string)\\n                    else:\\n                        rsp_string = \"Model-name does not have any registered server endpoints in Redis cache.\"\\n                        return jsonify(rsp_string)\\n                else:\\n                    rsp_string = \"Model-name not found in Redis cache.\"\\n                    return jsonify(rsp_string)\\n            else:\\n                abort(400, \\'Mandatory parameters missing in request.\\')\\n        else:\\n            abort(401,\"Elevated access privileges required.\")\\n\\n\\nclass GetAllModelMappings(Resource):\\n    def __init__(self):\\n        self.parser = reqparse.RequestParser()\\n\\n    @jwt_or_api_token_required\\n    def get(self):\\n        if is_admin_user(request):\\n            mappings_key_names = get_all_mappings_keys()\\n            data = {}\\n\\n            for key_name in mappings_key_names:\\n                str_key_name = key_name.decode(\\'utf-8\\')\\n                data[str_key_name] = []\\n                for val in get_values_list(key_name):\\n                    data[str_key_name].append(val.decode(\\'utf-8\\'))\\n\\n            return data\\n        else:\\n            abort(401,\"Elevated access privileges required.\")\\n\\n\\nclass GetAllServerMappings(Resource):\\n    def __init__(self):\\n        self.parser = reqparse.RequestParser()\\n\\n    @jwt_or_api_token_required\\n    def get(self):\\n        if is_admin_user(request):\\n            mappings_key_names = get_all_mappings_keys()\\n            data = {}\\n\\n            for key_name in mappings_key_names:\\n                for val in get_values_list(key_name):\\n                    url_parts = urlsplit(val.decode(\\'utf-8\\'))\\n                    base_addr = url_parts.scheme + \"://\" + url_parts.netloc\\n                    base_addr = base_addr.replace(\\'http://\\',\\'\\')\\n                    base_addr_split = base_addr.split(\\':\\')\\n                    if base_addr_split[0] not in data:\\n                        data[base_addr_split[0]] = {}\\n                    if base_addr_split[1] not in data[base_addr_split[0]]:\\n                        data[base_addr_split[0]][base_addr_split[1]] = []\\n                    if val.decode(\\'utf-8\\').__contains__(base_addr):\\n                        data[base_addr_split[0]][base_addr_split[1]].append(key_name.decode(\\'utf-8\\').replace(\\'mappings/\\',\\'\\'))\\n            data[\\'totalNoOfServers\\'] = len(data)\\n            return data\\n        else:\\n            abort(401,\"Elevated access privileges required.\")',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/controllers/admins/mappings.py'},\n",
       " {'name': 'RegisterModelMappings',\n",
       "  'summary': 'Handles the POST request to register model mappings. Checks that the user is an admin and validates the request data. It updates or adds model names and server endpoints in Redis cache, with support for redundancy count. Aborts on insufficient data or missing GPU information.',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/controllers/admins/mappings.py',\n",
       "  'type': 'class',\n",
       "  'code': 'class RegisterModelMappings(Resource):\\n    def __init__(self):\\n        self.parser = reqparse.RequestParser()\\n\\n    @jwt_or_api_token_required\\n    def post(self):\\n        rsp_string = \\'\\'\\n        if is_admin_user(request):\\n            request_data = request.get_json()\\n            model_name = request_data.get(\\'model_name\\')\\n            server_endpoint = request_data.get(\\'server_endpoint\\')\\n            redundancy_count = request_data.get(\\'redundancy_count\\')\\n            if model_name and server_endpoint:\\n                for endpoint in server_endpoint:\\n                    gpu_status = get_gpu_status(endpoint)\\n                    if gpu_status != 200:\\n                        abort(400, f\\'{endpoint} GPU information is missing ! Please add the GPU information to DB\\')\\n                model_name_key = get_mapping_keyname(model_name)\\n                if r.exists(model_name_key):\\n                    endpoint_list = r.lrange(model_name_key, 0, -1)\\n                    if redundancy_count and int(redundancy_count):\\n                        for endpoint in server_endpoint:\\n                            for _ in range(redundancy_count):\\n                                r.lpush(model_name_key, endpoint)\\n                                rsp_string = f\"Server-endpoints updated successfully!\"\\n                    else:\\n                        for endpoint in server_endpoint:\\n                            r.lpush(model_name_key, endpoint)\\n                            rsp_string = f\"Server-endpoints updated successfully!\"\\n                            continue\\n                    return jsonify(rsp_string)\\n                else:\\n                    if redundancy_count and int(redundancy_count):\\n                        for endpoint in server_endpoint:\\n                            for _ in range(redundancy_count):\\n                                r.lpush(model_name_key, endpoint)\\n                                rsp_string = \"Model-name & Server-endpoints added in Redis cache successfully!\"\\n                    else:\\n                        for endpoint in server_endpoint:\\n                            r.lpush(model_name_key, endpoint)\\n                            rsp_string = \"Model-name & Server-endpoints added in Redis cache successfully!\"\\n                    return jsonify(rsp_string)   \\n            else:\\n                abort(400, \\'Model-name & Server-endpoint missing in request params!\\')\\n        else:\\n            abort(401,\"Elevated access privileges required.\")\\n',\n",
       "  'class_funcs': {'__init__': {'function_name': '__init__',\n",
       "    'summary': '',\n",
       "    'code': '    def __init__(self):\\n        self.parser = reqparse.RequestParser()\\n',\n",
       "    'dependent_functions': []},\n",
       "   'post': {'function_name': 'post',\n",
       "    'summary': '',\n",
       "    'code': '    def post(self):\\n        rsp_string = \\'\\'\\n        if is_admin_user(request):\\n            request_data = request.get_json()\\n            model_name = request_data.get(\\'model_name\\')\\n            server_endpoint = request_data.get(\\'server_endpoint\\')\\n            redundancy_count = request_data.get(\\'redundancy_count\\')\\n            if model_name and server_endpoint:\\n                for endpoint in server_endpoint:\\n                    gpu_status = get_gpu_status(endpoint)\\n                    if gpu_status != 200:\\n                        abort(400, f\\'{endpoint} GPU information is missing ! Please add the GPU information to DB\\')\\n                model_name_key = get_mapping_keyname(model_name)\\n                if r.exists(model_name_key):\\n                    endpoint_list = r.lrange(model_name_key, 0, -1)\\n                    if redundancy_count and int(redundancy_count):\\n                        for endpoint in server_endpoint:\\n                            for _ in range(redundancy_count):\\n                                r.lpush(model_name_key, endpoint)\\n                                rsp_string = f\"Server-endpoints updated successfully!\"\\n                    else:\\n                        for endpoint in server_endpoint:\\n                            r.lpush(model_name_key, endpoint)\\n                            rsp_string = f\"Server-endpoints updated successfully!\"\\n                            continue\\n                    return jsonify(rsp_string)\\n                else:\\n                    if redundancy_count and int(redundancy_count):\\n                        for endpoint in server_endpoint:\\n                            for _ in range(redundancy_count):\\n                                r.lpush(model_name_key, endpoint)\\n                                rsp_string = \"Model-name & Server-endpoints added in Redis cache successfully!\"\\n                    else:\\n                        for endpoint in server_endpoint:\\n                            r.lpush(model_name_key, endpoint)\\n                            rsp_string = \"Model-name & Server-endpoints added in Redis cache successfully!\"\\n                    return jsonify(rsp_string)   \\n            else:\\n                abort(400, \\'Model-name & Server-endpoint missing in request params!\\')\\n        else:\\n            abort(401,\"Elevated access privileges required.\")\\n',\n",
       "    'dependent_functions': ['is_admin_user',\n",
       "     'abort',\n",
       "     'get_mapping_keyname',\n",
       "     'abort',\n",
       "     'get_gpu_status',\n",
       "     'jsonify',\n",
       "     'jsonify',\n",
       "     'abort',\n",
       "     'int',\n",
       "     'int',\n",
       "     'range',\n",
       "     'range']}}},\n",
       " {'name': 'DeRegisterModelMappings',\n",
       "  'summary': 'A RESTful API resource `DeRegisterModelMappings` with a `post` method to deregister model mappings from a Redis cache. The method checks for admin access, validates input parameters, and either deletes the Redis key or removes specified endpoints, depending on input. Responses are returned as JSON messages.',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/controllers/admins/mappings.py',\n",
       "  'type': 'class',\n",
       "  'code': 'class DeRegisterModelMappings(Resource):\\n    def __init__(self):\\n        self.parser = reqparse.RequestParser()\\n\\n    @jwt_or_api_token_required\\n    def post(self):\\n        rsp_string = \\'\\'\\n        if is_admin_user(request):\\n            request_data = request.get_json()\\n            model_name = request_data.get(\\'model_name\\')\\n            server_endpoint = request_data.get(\\'server_endpoint\\')\\n            is_delete_key_required = request_data.get(\\'is_delete_key_required\\')\\n            deletion_count = request_data.get(\\'deletion_count\\')\\n\\n            if model_name:\\n                model_name_key = get_mapping_keyname(model_name)\\n                if r.exists(model_name_key):\\n                    if is_delete_key_required is True:\\n                        delete_key(model_name_key)\\n                        rsp_string = \"Model mappings successfully removed from Redis cache.\"\\n                        return jsonify(rsp_string)\\n                    else:\\n                        endpoint_list = r.lrange(model_name_key, 0, -1)\\n                        rsp_string = \\'\\'\\n                        if deletion_count is None:\\n                            for endpoint in server_endpoint:\\n                                if endpoint_list and (endpoint.encode() in endpoint_list):\\n                                    endpoint_count = endpoint_list.count(endpoint.encode())\\n                                    remove_values_list(model_name_key, endpoint, endpoint_count)\\n                                    rsp_string += f\"Server-endpoint: {endpoint} successfully removed from Redis cache.\"\\n                                    endpoint_list = r.lrange(model_name_key, 0, -1)\\n                                else:\\n                                    rsp_string += f\"Server-endpoint: {endpoint} not found in Redis cache.\"\\n                                    continue\\n                        else:\\n                            for endpoint in server_endpoint:\\n                                if endpoint_list and (endpoint.encode() in endpoint_list):\\n                                    remove_values_list(model_name_key, endpoint, int(deletion_count))\\n                                    rsp_string += f\"Server-endpoint: {endpoint} successfully removed from Redis cache.\"\\n                                    endpoint_list = r.lrange(model_name_key, 0, -1)\\n                                else:\\n                                    rsp_string += f\"Server-endpoint: {endpoint} not found in Redis cache.\"\\n                                    continue\\n                        return jsonify(rsp_string)\\n                else:\\n                    rsp_string = \"Server-endpoint not found in Redis cache.\"\\n                    return jsonify(rsp_string)\\n            else:\\n                abort(400, \\'Mandatory parameters missing in the request.\\')\\n        else:\\n            abort(401,\"Elevated access privileges required.\")\\n',\n",
       "  'class_funcs': {'__init__': {'function_name': '__init__',\n",
       "    'summary': '',\n",
       "    'code': '    def __init__(self):\\n        self.parser = reqparse.RequestParser()\\n',\n",
       "    'dependent_functions': []},\n",
       "   'post': {'function_name': 'post',\n",
       "    'summary': '',\n",
       "    'code': '    def post(self):\\n        rsp_string = \\'\\'\\n        if is_admin_user(request):\\n            request_data = request.get_json()\\n            model_name = request_data.get(\\'model_name\\')\\n            server_endpoint = request_data.get(\\'server_endpoint\\')\\n            is_delete_key_required = request_data.get(\\'is_delete_key_required\\')\\n            deletion_count = request_data.get(\\'deletion_count\\')\\n\\n            if model_name:\\n                model_name_key = get_mapping_keyname(model_name)\\n                if r.exists(model_name_key):\\n                    if is_delete_key_required is True:\\n                        delete_key(model_name_key)\\n                        rsp_string = \"Model mappings successfully removed from Redis cache.\"\\n                        return jsonify(rsp_string)\\n                    else:\\n                        endpoint_list = r.lrange(model_name_key, 0, -1)\\n                        rsp_string = \\'\\'\\n                        if deletion_count is None:\\n                            for endpoint in server_endpoint:\\n                                if endpoint_list and (endpoint.encode() in endpoint_list):\\n                                    endpoint_count = endpoint_list.count(endpoint.encode())\\n                                    remove_values_list(model_name_key, endpoint, endpoint_count)\\n                                    rsp_string += f\"Server-endpoint: {endpoint} successfully removed from Redis cache.\"\\n                                    endpoint_list = r.lrange(model_name_key, 0, -1)\\n                                else:\\n                                    rsp_string += f\"Server-endpoint: {endpoint} not found in Redis cache.\"\\n                                    continue\\n                        else:\\n                            for endpoint in server_endpoint:\\n                                if endpoint_list and (endpoint.encode() in endpoint_list):\\n                                    remove_values_list(model_name_key, endpoint, int(deletion_count))\\n                                    rsp_string += f\"Server-endpoint: {endpoint} successfully removed from Redis cache.\"\\n                                    endpoint_list = r.lrange(model_name_key, 0, -1)\\n                                else:\\n                                    rsp_string += f\"Server-endpoint: {endpoint} not found in Redis cache.\"\\n                                    continue\\n                        return jsonify(rsp_string)\\n                else:\\n                    rsp_string = \"Server-endpoint not found in Redis cache.\"\\n                    return jsonify(rsp_string)\\n            else:\\n                abort(400, \\'Mandatory parameters missing in the request.\\')\\n        else:\\n            abort(401,\"Elevated access privileges required.\")\\n',\n",
       "    'dependent_functions': ['is_admin_user',\n",
       "     'abort',\n",
       "     'get_mapping_keyname',\n",
       "     'abort',\n",
       "     'jsonify',\n",
       "     'delete_key',\n",
       "     'jsonify',\n",
       "     'jsonify',\n",
       "     'remove_values_list',\n",
       "     'remove_values_list',\n",
       "     'int']}}},\n",
       " {'name': 'UpdateModelMappings',\n",
       "  'summary': \"Class `UpdateModelMappings` handles POST requests to update a model's server endpoint in a Redis cache if the user is an admin. It verifies required parameters, checks GPU status for the new endpoint, and updates the Redis key if the old endpoint is found.\",\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/controllers/admins/mappings.py',\n",
       "  'type': 'class',\n",
       "  'code': 'class UpdateModelMappings(Resource):\\n    def __init__(self):\\n        self.parser = reqparse.RequestParser()\\n\\n    @jwt_or_api_token_required\\n    def post(self):\\n        rsp_string = \\'\\'\\n        if is_admin_user(request):\\n            request_data = request.get_json()\\n            model_name = request_data.get(\\'model_name\\')\\n            old_server_endpoint = request_data.get(\\'old_server_endpoint\\')\\n            new_server_endpoint = request_data.get(\\'new_server_endpoint\\')\\n\\n            if model_name and old_server_endpoint and new_server_endpoint:\\n                gpu_status = get_gpu_status(new_server_endpoint)\\n                if gpu_status != 200:\\n                    abort(400, f\\'{new_server_endpoint} GPU information is missing ! Please add the GPU information to DB\\')\\n                model_name_key = get_mapping_keyname(model_name)\\n                if r.exists(model_name_key):\\n                    endpoint_list = r.lrange(model_name_key, 0, -1)\\n                    if endpoint_list:\\n                        endpoint_count = endpoint_list.count(old_server_endpoint.encode())\\n                        if old_server_endpoint.encode() in endpoint_list:\\n                            for _ in range(int(endpoint_count)):\\n                                old_mapping_index = endpoint_list.index(old_server_endpoint.encode())\\n                                r.lset(model_name_key, old_mapping_index, new_server_endpoint)\\n                                rsp_string = f\"Server-endpoint successfully updated with newer value in Redis cache.\"\\n                                endpoint_list = r.lrange(model_name_key, 0, -1)\\n                        else:\\n                            rsp_string = f\"Endpoint: {old_server_endpoint} not found for Model-name: {model_name}\"\\n                        return jsonify(rsp_string)\\n                    else:\\n                        rsp_string = \"Model-name does not have any registered server endpoints in Redis cache.\"\\n                        return jsonify(rsp_string)\\n                else:\\n                    rsp_string = \"Model-name not found in Redis cache.\"\\n                    return jsonify(rsp_string)\\n            else:\\n                abort(400, \\'Mandatory parameters missing in request.\\')\\n        else:\\n            abort(401,\"Elevated access privileges required.\")\\n',\n",
       "  'class_funcs': {'__init__': {'function_name': '__init__',\n",
       "    'summary': '',\n",
       "    'code': '    def __init__(self):\\n        self.parser = reqparse.RequestParser()\\n',\n",
       "    'dependent_functions': []},\n",
       "   'post': {'function_name': 'post',\n",
       "    'summary': '',\n",
       "    'code': '    def post(self):\\n        rsp_string = \\'\\'\\n        if is_admin_user(request):\\n            request_data = request.get_json()\\n            model_name = request_data.get(\\'model_name\\')\\n            old_server_endpoint = request_data.get(\\'old_server_endpoint\\')\\n            new_server_endpoint = request_data.get(\\'new_server_endpoint\\')\\n\\n            if model_name and old_server_endpoint and new_server_endpoint:\\n                gpu_status = get_gpu_status(new_server_endpoint)\\n                if gpu_status != 200:\\n                    abort(400, f\\'{new_server_endpoint} GPU information is missing ! Please add the GPU information to DB\\')\\n                model_name_key = get_mapping_keyname(model_name)\\n                if r.exists(model_name_key):\\n                    endpoint_list = r.lrange(model_name_key, 0, -1)\\n                    if endpoint_list:\\n                        endpoint_count = endpoint_list.count(old_server_endpoint.encode())\\n                        if old_server_endpoint.encode() in endpoint_list:\\n                            for _ in range(int(endpoint_count)):\\n                                old_mapping_index = endpoint_list.index(old_server_endpoint.encode())\\n                                r.lset(model_name_key, old_mapping_index, new_server_endpoint)\\n                                rsp_string = f\"Server-endpoint successfully updated with newer value in Redis cache.\"\\n                                endpoint_list = r.lrange(model_name_key, 0, -1)\\n                        else:\\n                            rsp_string = f\"Endpoint: {old_server_endpoint} not found for Model-name: {model_name}\"\\n                        return jsonify(rsp_string)\\n                    else:\\n                        rsp_string = \"Model-name does not have any registered server endpoints in Redis cache.\"\\n                        return jsonify(rsp_string)\\n                else:\\n                    rsp_string = \"Model-name not found in Redis cache.\"\\n                    return jsonify(rsp_string)\\n            else:\\n                abort(400, \\'Mandatory parameters missing in request.\\')\\n        else:\\n            abort(401,\"Elevated access privileges required.\")\\n',\n",
       "    'dependent_functions': ['is_admin_user',\n",
       "     'abort',\n",
       "     'get_gpu_status',\n",
       "     'get_mapping_keyname',\n",
       "     'abort',\n",
       "     'abort',\n",
       "     'jsonify',\n",
       "     'jsonify',\n",
       "     'jsonify',\n",
       "     'range',\n",
       "     'int']}}},\n",
       " {'name': 'GetAllModelMappings',\n",
       "  'summary': 'Defines a Flask-RESTful resource `GetAllModelMappings` to fetch key-value mappings for admin users. Uses `reqparse` for request parsing and a decorator `jwt_or_api_token_required` for authentication. The `get` method checks for admin privileges, retrieves mappings, decodes them to UTF-8, and returns the data, or aborts with a 401 error if unauthorized.',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/controllers/admins/mappings.py',\n",
       "  'type': 'class',\n",
       "  'code': 'class GetAllModelMappings(Resource):\\n    def __init__(self):\\n        self.parser = reqparse.RequestParser()\\n\\n    @jwt_or_api_token_required\\n    def get(self):\\n        if is_admin_user(request):\\n            mappings_key_names = get_all_mappings_keys()\\n            data = {}\\n\\n            for key_name in mappings_key_names:\\n                str_key_name = key_name.decode(\\'utf-8\\')\\n                data[str_key_name] = []\\n                for val in get_values_list(key_name):\\n                    data[str_key_name].append(val.decode(\\'utf-8\\'))\\n\\n            return data\\n        else:\\n            abort(401,\"Elevated access privileges required.\")\\n',\n",
       "  'class_funcs': {'__init__': {'function_name': '__init__',\n",
       "    'summary': '',\n",
       "    'code': '    def __init__(self):\\n        self.parser = reqparse.RequestParser()\\n',\n",
       "    'dependent_functions': []},\n",
       "   'get': {'function_name': 'get',\n",
       "    'summary': '',\n",
       "    'code': '    def get(self):\\n        if is_admin_user(request):\\n            mappings_key_names = get_all_mappings_keys()\\n            data = {}\\n\\n            for key_name in mappings_key_names:\\n                str_key_name = key_name.decode(\\'utf-8\\')\\n                data[str_key_name] = []\\n                for val in get_values_list(key_name):\\n                    data[str_key_name].append(val.decode(\\'utf-8\\'))\\n\\n            return data\\n        else:\\n            abort(401,\"Elevated access privileges required.\")\\n',\n",
       "    'dependent_functions': ['is_admin_user',\n",
       "     'get_all_mappings_keys',\n",
       "     'abort',\n",
       "     'get_values_list']}}},\n",
       " {'name': 'GetAllServerMappings',\n",
       "  'summary': '\\nThe `GetAllServerMappings` class defines an API resource with a GET endpoint that requires JWT or API token authentication. It retrieves all server mappings if the user is an admin. It parses the mappings, compiles them into a nested dictionary organized by base URL and port, and includes the total number of servers. Unauthorized access returns a 401 error.\\n',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/controllers/admins/mappings.py',\n",
       "  'type': 'class',\n",
       "  'code': 'class GetAllServerMappings(Resource):\\n    def __init__(self):\\n        self.parser = reqparse.RequestParser()\\n\\n    @jwt_or_api_token_required\\n    def get(self):\\n        if is_admin_user(request):\\n            mappings_key_names = get_all_mappings_keys()\\n            data = {}\\n\\n            for key_name in mappings_key_names:\\n                for val in get_values_list(key_name):\\n                    url_parts = urlsplit(val.decode(\\'utf-8\\'))\\n                    base_addr = url_parts.scheme + \"://\" + url_parts.netloc\\n                    base_addr = base_addr.replace(\\'http://\\',\\'\\')\\n                    base_addr_split = base_addr.split(\\':\\')\\n                    if base_addr_split[0] not in data:\\n                        data[base_addr_split[0]] = {}\\n                    if base_addr_split[1] not in data[base_addr_split[0]]:\\n                        data[base_addr_split[0]][base_addr_split[1]] = []\\n                    if val.decode(\\'utf-8\\').__contains__(base_addr):\\n                        data[base_addr_split[0]][base_addr_split[1]].append(key_name.decode(\\'utf-8\\').replace(\\'mappings/\\',\\'\\'))\\n            data[\\'totalNoOfServers\\'] = len(data)\\n            return data\\n        else:\\n            abort(401,\"Elevated access privileges required.\")',\n",
       "  'class_funcs': {'__init__': {'function_name': '__init__',\n",
       "    'summary': '',\n",
       "    'code': '    def __init__(self):\\n        self.parser = reqparse.RequestParser()\\n',\n",
       "    'dependent_functions': []},\n",
       "   'get': {'function_name': 'get',\n",
       "    'summary': '',\n",
       "    'code': '    def get(self):\\n        if is_admin_user(request):\\n            mappings_key_names = get_all_mappings_keys()\\n            data = {}\\n\\n            for key_name in mappings_key_names:\\n                for val in get_values_list(key_name):\\n                    url_parts = urlsplit(val.decode(\\'utf-8\\'))\\n                    base_addr = url_parts.scheme + \"://\" + url_parts.netloc\\n                    base_addr = base_addr.replace(\\'http://\\',\\'\\')\\n                    base_addr_split = base_addr.split(\\':\\')\\n                    if base_addr_split[0] not in data:\\n                        data[base_addr_split[0]] = {}\\n                    if base_addr_split[1] not in data[base_addr_split[0]]:\\n                        data[base_addr_split[0]][base_addr_split[1]] = []\\n                    if val.decode(\\'utf-8\\').__contains__(base_addr):\\n                        data[base_addr_split[0]][base_addr_split[1]].append(key_name.decode(\\'utf-8\\').replace(\\'mappings/\\',\\'\\'))\\n            data[\\'totalNoOfServers\\'] = len(data)\\n            return data\\n        else:\\n            abort(401,\"Elevated access privileges required.\")',\n",
       "    'dependent_functions': ['is_admin_user',\n",
       "     'get_all_mappings_keys',\n",
       "     'len',\n",
       "     'abort',\n",
       "     'get_values_list',\n",
       "     'urlsplit']}}},\n",
       " {'name': 'tasks.py',\n",
       "  'summary': '\\nThis code sets up a Flask Blueprint and Flask-RESTful API for handling tasks-related routes in a Flask application. It begins by importing necessary modules: \\'tasks\\' from \\'asyncio\\', \\'Blueprint\\' from \\'flask\\', and \\'Api\\' from \\'flask_restful\\'. It also imports the \\'UpdateModelStatus\\' class from \\'app.controllers.tasks.tasks\\'. The \\'tasks_bp\\' Blueprint is created with the name \"tasks\", and an Api object \\'api\\' is linked to this Blueprint. Finally, the \\'UpdateModelStatus\\' resource is added to the API with the route \"/modelStatus\", making it accessible via this URL endpoint.\\n',\n",
       "  'type': 'file',\n",
       "  'code': 'from asyncio import tasks\\nfrom flask import Blueprint\\nfrom flask_restful import Api\\nfrom app.controllers.tasks.tasks import (\\n    UpdateModelStatus\\n)\\n\\ntasks_bp = Blueprint(\"tasks\", __name__)\\napi =  Api(tasks_bp)\\n\\napi.add_resource(UpdateModelStatus, \"/modelStatus\")',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/blueprints/tasks.py'},\n",
       " {'name': 'prometheus.py',\n",
       "  'summary': '\\nThe code initializes a Flask Blueprint named \"prometheus\" and sets up a Flask-RESTful API for it. It imports the `GetServerStatus` class from the `app.controllers.admins.prometheus` module. The `GetServerStatus` resource is then added to the API with the endpoint path \"/status\". This means that any HTTP requests to `/status` will be handled by the `GetServerStatus` resource.\\n',\n",
       "  'type': 'file',\n",
       "  'code': 'from flask import Blueprint\\nfrom flask_restful import Api\\nfrom app.controllers.admins.prometheus import (\\n    GetServerStatus\\n)\\n\\nprometheus_bp = Blueprint(\"prometheus\", __name__)\\napi =  Api(prometheus_bp)\\n\\napi.add_resource(GetServerStatus, \"/status\")',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/blueprints/prometheus.py'},\n",
       " {'name': 'model_mappings.py',\n",
       "  'summary': '\\nThe code defines a Flask blueprint named `model_mappings` and attaches a RESTful API to it using the `flask_restful` library. Several API resources are then added to this blueprint, each corresponding to an endpoint for managing model mappings. Specifically:\\n- `RegisterModelMappings` is mapped to the \"/register\" endpoint for registering new model mappings.\\n- `DeRegisterModelMappings` is mapped to the \"/de-register\" endpoint for deregistering existing model mappings.\\n- `UpdateModelMappings` is mapped to the \"/update\" endpoint for updating existing model mappings.\\n- `GetAllModelMappings` is mapped to the \"/get-all\" endpoint for retrieving all model mappings.\\n- `GetAllServerMappings` is mapped to the \"/get-by-server\" endpoint for retrieving all model mappings grouped by server.\\n\\nThe resources (`RegisterModelMappings`, `DeRegisterModelMappings`, `UpdateModelMappings`, `GetAllModelMappings`, and `GetAllServerMappings`) are intended to be imported from the `app.controllers.admins.mappings` module.\\n',\n",
       "  'type': 'file',\n",
       "  'code': 'from flask import Blueprint\\nfrom flask_restful import Api\\nfrom app.controllers.admins.mappings import (\\n    RegisterModelMappings,\\n    DeRegisterModelMappings,\\n    UpdateModelMappings,\\n    GetAllModelMappings,\\n    GetAllServerMappings\\n)\\n\\nmodel_mappings_bp = Blueprint(\"model_mappings\", __name__)\\napi =  Api(model_mappings_bp)\\n\\napi.add_resource(RegisterModelMappings, \"/register\")\\napi.add_resource(DeRegisterModelMappings, \"/de-register\")\\napi.add_resource(UpdateModelMappings, \"/update\")\\napi.add_resource(GetAllModelMappings, \"/get-all\")\\napi.add_resource(GetAllServerMappings, \"/get-by-server\")',\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/app/blueprints/model_mappings.py'},\n",
       " {'name': 'README.md',\n",
       "  'summary': '\\nThe code defines a Smart Orchestration Service for managing multiple Stable Diffusion (SD) models hosted on different servers. Below are the supported API endpoints:\\n\\n1. **Server-Mappings Endpoints:**\\n   - **Register Server (`POST /server-mappings/register`):** Registers a server hosting an SD model with parameters `model_name` (string, required), `server_endpoint` (List<string>, required), and optional `redundancy_count` (int).\\n   - **De-Register Server (`POST /server-mappings/de-register`):** De-registers a server with parameters `model_name` (string, required), `server_endpoint` (List<string>, required), and optionally `is_delete_key_required` (bool) and `deletion_count` (int).\\n   - **Update Server (`POST /server-mappings/update`):** Updates a server address with parameters `model_name` (string, required), `old_server_endpoint` (string, required), and `new_server_endpoint` (string, required).\\n   - **Get All Server Mappings (`GET /server-mappings/get-all`):** Retrieves all server mappings from the Redis cache.\\n   - **Get Server Mappings by Server (`GET /server-mappings/get-by-server`):** Retrieves server mappings organized by servers from the Redis cache.\\n   - **Redis Health Check (`GET /status/Redis-health-check`):** Checks the health status of the Redis cache.\\n\\n2. **Prometheus Monitoring Endpoints:**\\n   - **Get Node-Exporter Status (`GET /prometheus/status`):** Retrieves the status of all node-exporter/metrics endpoints, defaulting to port 8002 on each server.\\n\\nSecurity: All requests require a bearer token or an x-api-key and are restricted to Segmind admins.\\n',\n",
       "  'type': 'file',\n",
       "  'code': \"\\n# Smart Orchestraion Service\\n\\nAn orchestration service to handle multiple SD models hosted on different servers.\\n\\n\\n## Currently Supported Api Endpoints\\n## API Reference\\n\\n~~~\\nAll requests are required to have a bearer token or an x-api-key present in the http-headers and can be only used by admins of Segmind.\\n~~~\\n\\n## Server-Mappings Endpoints\\n#### Register Server Endpoint\\n\\n```\\n  POST /server-mappings/register\\n```\\n\\n| Parameter | Type     | Description                |\\n| :-------- | :------- | :------------------------- |\\n| `model_name` | `string` | **Required**. Name of the SD Model. |\\n| `server_endpoint` | `List<string>` | **Required**. Server addresses where model is hosted. |\\n| `redundancy_count` | `int` | **Optional**. Number of times endpoints needs to be added in model_name key |\\n\\n#### De-Register Server Endpoint\\n\\n```\\n  POST /server-mappings/de-register\\n```\\n\\n| Parameter | Type     | Description                |\\n| :-------- | :------- | :------------------------- |\\n| `model_name` | `string` | **Required**. Name of the SD Model. |\\n| `server_endpoint` | `List<string>` | **Required**. Server addresses needed to be de-registered. |\\n| `is_delete_key_required` | `bool` | **Optional**. Flag to delete whole key and it's content from Redis. |\\n| `deletion_count` | `int` | **Optional**. Number of times endpoints needs to be deleted from model_name key |\\n\\n\\n#### Update Server Endpoint\\n\\n```\\n  POST /server-mappings/update\\n```\\n\\n| Parameter | Type     | Description                |\\n| :-------- | :------- | :------------------------- |\\n| `model_name` | `string` | **Required**. Name of the SD Model |\\n| `old_server_endpoint` | `string` | **Required**.  Old Server address which needs to be updated. |\\n| `new_server_endpoint` | `string` | **Required**. New Server address. |\\n\\n#### Get All Server Mappings present in Redis-Cache\\n\\n```\\n  GET /server-mappings/get-all\\n```\\n\\n| Parameter | Type     | Description                |\\n| :-------- | :------- | :------------------------- |\\n| `None` | `None` |  |\\n\\n#### Get All server Mappings organized by servers present in Redis-Cache\\n\\n```\\n  GET /server-mappings/get-by-server\\n```\\n\\n| Parameter | Type     | Description                |\\n| :-------- | :------- | :------------------------- |\\n| `None` | `None` |  |\\n\\n\\n#### Health Check For Redis Cache\\n\\n```\\n  GET /status/Redis-health-check\\n```\\n\\n| Parameter | Type     | Description                |\\n| :-------- | :------- | :------------------------- |\\n| `None` | `None` |  |\\n\\n\\n## Prometheus Monitoring Endpoints\\n\\n#### Get Status for all node-exporter/metrics endpoints\\n#### Note: By default, this Endpoint only reports the status of node exporter running in port 8002 of each server.\\n```\\n  GET /prometheus/status\\n```\\n\\n| Parameter | Type     | Description                |\\n| :-------- | :------- | :------------------------- |\\n| `None` | `None` |  |\\n\",\n",
       "  'path': '/workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/README.md',\n",
       "  'dependent_functions': []}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = \"\"\n",
    "for element in summ:\n",
    "    name = element['name']\n",
    "    summary = element['summary']\n",
    "    type = element['type']\n",
    "    try :\n",
    "        dependent_functions = element['dependent_functions']\n",
    "    except Exception as e :\n",
    "        dependent_functions = \"null\"\n",
    "    path = element['path']\n",
    "    summary += f\"The name of the file is {name}. Its type is {type}. Its path is {path}. Its dependant functions are {dependent_functions}. Its summary is {summary}. \"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe code defines a Smart Orchestration Service for managing multiple Stable Diffusion (SD) models hosted on different servers. Below are the supported API endpoints:\\n\\n1. **Server-Mappings Endpoints:**\\n   - **Register Server (`POST /server-mappings/register`):** Registers a server hosting an SD model with parameters `model_name` (string, required), `server_endpoint` (List<string>, required), and optional `redundancy_count` (int).\\n   - **De-Register Server (`POST /server-mappings/de-register`):** De-registers a server with parameters `model_name` (string, required), `server_endpoint` (List<string>, required), and optionally `is_delete_key_required` (bool) and `deletion_count` (int).\\n   - **Update Server (`POST /server-mappings/update`):** Updates a server address with parameters `model_name` (string, required), `old_server_endpoint` (string, required), and `new_server_endpoint` (string, required).\\n   - **Get All Server Mappings (`GET /server-mappings/get-all`):** Retrieves all server mappings from the Redis cache.\\n   - **Get Server Mappings by Server (`GET /server-mappings/get-by-server`):** Retrieves server mappings organized by servers from the Redis cache.\\n   - **Redis Health Check (`GET /status/Redis-health-check`):** Checks the health status of the Redis cache.\\n\\n2. **Prometheus Monitoring Endpoints:**\\n   - **Get Node-Exporter Status (`GET /prometheus/status`):** Retrieves the status of all node-exporter/metrics endpoints, defaulting to port 8002 on each server.\\n\\nSecurity: All requests require a bearer token or an x-api-key and are restricted to Segmind admins.\\nThe name of the file is README.md. Its type is file. Its path is /workspace/groundcrew/smart-orchestration-service-master/smart-orchestration-service-master/README.md. Its dependant functions are []. Its summary is \\nThe code defines a Smart Orchestration Service for managing multiple Stable Diffusion (SD) models hosted on different servers. Below are the supported API endpoints:\\n\\n1. **Server-Mappings Endpoints:**\\n   - **Register Server (`POST /server-mappings/register`):** Registers a server hosting an SD model with parameters `model_name` (string, required), `server_endpoint` (List<string>, required), and optional `redundancy_count` (int).\\n   - **De-Register Server (`POST /server-mappings/de-register`):** De-registers a server with parameters `model_name` (string, required), `server_endpoint` (List<string>, required), and optionally `is_delete_key_required` (bool) and `deletion_count` (int).\\n   - **Update Server (`POST /server-mappings/update`):** Updates a server address with parameters `model_name` (string, required), `old_server_endpoint` (string, required), and `new_server_endpoint` (string, required).\\n   - **Get All Server Mappings (`GET /server-mappings/get-all`):** Retrieves all server mappings from the Redis cache.\\n   - **Get Server Mappings by Server (`GET /server-mappings/get-by-server`):** Retrieves server mappings organized by servers from the Redis cache.\\n   - **Redis Health Check (`GET /status/Redis-health-check`):** Checks the health status of the Redis cache.\\n\\n2. **Prometheus Monitoring Endpoints:**\\n   - **Get Node-Exporter Status (`GET /prometheus/status`):** Retrieves the status of all node-exporter/metrics endpoints, defaulting to port 8002 on each server.\\n\\nSecurity: All requests require a bearer token or an x-api-key and are restricted to Segmind admins.\\n. '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "791"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from create_summary import _get_token_length\n",
    "num = _get_token_length (summary)\n",
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=f\"The summaries of all the files are : {summary}\"\n",
    "system_prompt=\"\"\"You are a good code summarizer. Given summaries of all file in a repository , your task is to create an overall summary using all these summaries. This summary will be used for question answering about the code and code improvements , so build the summary accordingly.\n",
    "    Generate your summary between <summary>...</summary> tag.\"\"\"\n",
    "summary=llm(prompt,system_prompt)\n",
    "summary=summary.split('<summary>')[-1].split('</summary>')[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe repository contains a Smart Orchestration Service designed to manage multiple Stable Diffusion (SD) models hosted on different servers. This service provides several API endpoints for efficient server management and monitoring:\\n\\n1. **Server-Mappings Endpoints:**\\n   - **Register Server (`POST /server-mappings/register`):** Allows the registration of a new server hosting an SD model. Requires `model_name` and `server_endpoint` parameters, with an optional `redundancy_count`.\\n   - **De-Register Server (`POST /server-mappings/de-register`):** Enables de-registration of an existing server with `model_name` and `server_endpoint` parameters. It also supports optional `is_delete_key_required` and `deletion_count` parameters.\\n   - **Update Server (`POST /server-mappings/update`):** Facilitates updating the server address by specifying `model_name`, `old_server_endpoint`, and `new_server_endpoint`.\\n   - **Get All Server Mappings (`GET /server-mappings/get-all`):** Retrieves all server mappings currently stored in the Redis cache.\\n   - **Get Server Mappings by Server (`GET /server-mappings/get-by-server`):** Fetches server mappings grouped by servers from the Redis cache.\\n   - **Redis Health Check (`GET /status/Redis-health-check`):** Checks the health status of the Redis cache.\\n\\n2. **Prometheus Monitoring Endpoints:**\\n   - **Get Node-Exporter Status (`GET /prometheus/status`):** Provides the status of node-exporter/metrics endpoints, usually defaulted to port 8002 on each server.\\n\\nSecurity measures mandate that all requests must include either a bearer token or an x-api-key, and access is restricted to Segmind administrators.\\n\\nOverall, the code offers robust functionalities for server management and monitoring, ensuring efficient handling and health-checking of servers hosting Stable Diffusion models.\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
